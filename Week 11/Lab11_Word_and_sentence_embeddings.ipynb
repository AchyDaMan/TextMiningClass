{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Lab11: Exploring word and sentence embeddings:\n",
        "\n",
        "In this session, we learn about representation learning, which means extracting useful features from text using advanced deep neural models. This is important for creating effective NLP applications. Instead of manually designing features, we use deep learning to automatically grasp complex patterns in the text, going beyond what's just in the provided data.\n",
        "\n",
        "Note: Please solve the exercises and turn the IPYNB notebook in under Canvas->Week11 by no later than **end of today**.\n",
        "\n",
        "## PART A: Exploring Word Representations (Features) from Pre-trained Models\n",
        "## 1.1. Let's start with word vectors.\n",
        "\n",
        "`Word2Vec` and `GloVe` are both popular algorithms used for obtaining word embeddings, which represent words as dense vectors in a continuous vector space. These methods help capture the semantic meanings of words based on their contexts in a corpus.\n",
        "\n",
        "1. **Word2Vec**:\n",
        "\n",
        "   Word2Vec is a shallow, two-layer neural network model that is trained to reconstruct linguistic contexts of words. It has two training methods: Continuous Bag of Words (CBOW) and Skip-gram. CBOW predicts the target word from its context, while Skip-gram predicts the context words from the target word.\n",
        "\n",
        "2. **GloVe**:\n",
        "\n",
        "   GloVe, short for Global Vectors, is an unsupervised learning algorithm for obtaining vector representations for words. It works on the co-occurrence statistics of words in a corpus, considering the global corpus statistics. GloVe combines the advantages of global matrix factorization and local context window methods.\n",
        "\n",
        "Here's an example of how to compute word similarity using Word2Vec and GloVe in Python and then plot words in 2D.\n"
      ],
      "metadata": {
        "id": "UJzH-5UASWgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "\n",
        "# Download a pre-trained word2vec (trained on Google News data)\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Compute similarity between words\n",
        "def compute_similarity(model, word1, word2):\n",
        "    try:\n",
        "        return model.similarity(word1, word2)\n",
        "    except KeyError:\n",
        "        return 0  # return 0 if either of the words is not in the vocabulary\n",
        "\n",
        "# Examples of word similarity\n",
        "word1 = 'king'\n",
        "word2 = 'queen'\n",
        "word3 = 'tiger'\n",
        "\n",
        "w2v_similarity1 = compute_similarity(w2v_model, word1, word2)\n",
        "w2v_similarity2 = compute_similarity(w2v_model, word2, word3)\n",
        "w2v_similarity3 = compute_similarity(w2v_model, word1, word3)\n",
        "\n",
        "print(f\"Word2Vec similarity between {word1} and {word2}: {w2v_similarity1}\")\n",
        "print(f\"Word2Vec similarity between {word2} and {word3}: {w2v_similarity2}\")\n",
        "print(f\"Word2Vec similarity between {word1} and {word3}: {w2v_similarity3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFXF-gGXjU5l",
        "outputId": "31ea2c89-3ab9-483e-976c-7caf97d4cc2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Word2Vec similarity between king and queen: 0.6510956883430481\n",
            "Word2Vec similarity between queen and tiger: 0.08486607670783997\n",
            "Word2Vec similarity between king and tiger: 0.1430056393146515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the word `king` is more similar to `queen` than animals."
      ],
      "metadata": {
        "id": "GtnDAoKvw9F1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Finding Most Similar Words\n",
        "\n",
        "We find words that are most similar to the given word by identifying the nearest neighbors of a word in the N-dimensional space. We need a distance metric (Euclidean or Cosine) for this."
      ],
      "metadata": {
        "id": "WxNpCSA_ygjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = lambda word_list, target_word: sorted((word, sim) for word, sim in model.wv.most_similar(target_word) if word in word_list)\n",
        "\n",
        "word_to_check = 'king'\n",
        "k = 10\n",
        "\n",
        "most_similar_words = w2v_model.most_similar(word_to_check,topn=k)\n",
        "print (most_similar_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXLluZ8Yh5yj",
        "outputId": "4bfed0d1-849f-4851-f707-0192d5fe16b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('kings', 0.7138045430183411), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474), ('sultan', 0.5864824056625366), ('ruler', 0.5797567367553711), ('princes', 0.5646552443504333), ('Prince_Paras', 0.5432944297790527), ('throne', 0.5422105193138123)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also perform some consitional queries. E.g., `what word is most similar to \"king\" and \"woman\" but opposite of \"man\" (i.e., very dissimilar to \"man\")`."
      ],
      "metadata": {
        "id": "jJoU1snd3-jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sims = w2v_model.most_similar(positive=['king', 'woman'], negative=['man'])\n",
        "\n",
        "print (sims)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pHQ4HV54bw-",
        "outputId": "bd2577e8-220f-4d95-c684-cd7f513517ca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593831062317), ('monarchy', 0.5087411999702454)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract sentence vector from word vectors by averaging word embeddings\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "\n",
        "def extract_sentence_vector(sentence):\n",
        "    words = sentence.split()\n",
        "    word_vectors = [w2v_model[word] for word in words if word in w2v_model]\n",
        "    if not word_vectors:\n",
        "        return None  # Return None if no word vectors are found\n",
        "    sentence_vector = np.mean(word_vectors, axis=0)\n",
        "    return sentence_vector\n",
        "\n",
        "# Example of extracting sentence vector using mean of word vectors\n",
        "example_sentence1 = \"Natural Language Processing is a branch of Artificial Intelligence\".lower()\n",
        "example_sentence2 = \"The quick brown fox jumped over the laze dog\".lower()\n",
        "example_sentence3 = \"In the jungle the mighty jungle the lion sleeps tonight\".lower()\n",
        "\n",
        "sentence_vector1 = extract_sentence_vector(example_sentence1)\n",
        "sentence_vector2 = extract_sentence_vector(example_sentence2)\n",
        "sentence_vector3 = extract_sentence_vector(example_sentence3)\n",
        "\n",
        "def similarity (x1, x2):\n",
        "  # similarity is the opposite of distance\n",
        "  return 1 - cosine(x1, x2)\n",
        "\n",
        "sim1 = similarity(sentence_vector1, sentence_vector2)\n",
        "sim2 = similarity(sentence_vector2, sentence_vector3)\n",
        "sim3 = similarity(sentence_vector1, sentence_vector3)\n",
        "\n",
        "print (f\"Distance between sentence 1 and 2, score:  {sim1}\")\n",
        "print (f\"Similarity between sentence 2 and 3, score:  {sim2}\")\n",
        "print (f\"Similarity between sentence 1 and 3, score:  {sim3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl3x4GUk682i",
        "outputId": "8fb12dad-7b61-40f9-d73c-4e32401ec206"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance between sentence 1 and 2, score:  0.23709803819656372\n",
            "Similarity between sentence 2 and 3, score:  0.56390380859375\n",
            "Similarity between sentence 1 and 3, score:  0.265546590089798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Extracting Sentence Vectors from Word Vectors\n",
        "\n",
        "We have done this quite a few times now. Extracting a sentence vector from sentences is easy; we just take the mean (average) of the word vectors across all words in a sentence."
      ],
      "metadata": {
        "id": "Y0nQoedp6Z5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4. Zero-shot Text Classification using Sentence Vectors\n",
        "\n",
        "We have seen in Practicum 5, how it is possible to use word vectors such as GloVE to classify text. We extract sentence vectors (by averaging word vectors) and then treat the vectors as features. We extract training and test features-sets from the train/test splits. We then train a classifier (such as `LogisticRegression` based on features and labels and evaluate it.\n",
        "\n",
        "Here, let's conceptualize the task of classification as a problem of finding similarity between the input sentence and some generic sentences representing class labels. Based on the similarity, we then come up with the class labels.\n",
        "\n",
        "Since we do not use any training data in this setting, this can be reffered to as zero shot learning (i.e., no-training data is used here). Let's classify movie reviews here."
      ],
      "metadata": {
        "id": "D-mE0jEW5oeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generic_sentence1 = \"positive\"\n",
        "generic_sentence2 = \"negative\"\n",
        "\n",
        "pos_vec = extract_sentence_vector(generic_sentence1)\n",
        "neg_vec = extract_sentence_vector(generic_sentence2)\n",
        "\n",
        "\n",
        "def predict_sentiment_zero_shot(sentence):\n",
        "  vec = extract_sentence_vector(sentence)\n",
        "\n",
        "  sim1 = similarity(pos_vec, vec)\n",
        "  sim2 = similarity(neg_vec, vec)\n",
        "\n",
        "  if sim1 > sim2:\n",
        "    print (f\"Predicted sentiment: 'positive' , score: POS: {sim1} NEG: {sim2}\")\n",
        "  else:\n",
        "    print (f\"Predicted sentiment: 'negative' , score: POS: {sim2} NEG: {sim1}\")\n",
        "\n",
        "\n",
        "sentence1 = \"The movie was amazing . I liked the use of CGI\".lower()\n",
        "sentence2 = \"The movie was horrible . I did not like the use of CGI\".lower()\n",
        "sentence3 = \"The movie was somewhat good . However , I did not find the acting appealing and won't recommend\".lower()\n",
        "\n",
        "predict_sentiment_zero_shot(sentence1)\n",
        "predict_sentiment_zero_shot(sentence2)\n",
        "predict_sentiment_zero_shot(sentence3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA4KLfWr5n6J",
        "outputId": "9d54b9a0-e723-4ec4-ff84-c00f735aa962"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted sentiment: 'positive' , score: POS: 0.2235041707754135 NEG: 0.2126578837633133\n",
            "Predicted sentiment: 'negative' , score: POS: 0.2664741575717926 NEG: 0.20714746415615082\n",
            "Predicted sentiment: 'positive' , score: POS: 0.27916139364242554 NEG: 0.2693459689617157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, word vector based models do not give a lot of importance to context. Also we did not do any preprocessing (e.g., stop word removal and lemmatization), which would have removed some unnecessary words like `be` verbs and `prepositions` here."
      ],
      "metadata": {
        "id": "9lJFG8wXBlaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART B: Exploring Sentence Representations (Features) from Pre-trained Models\n",
        "\n",
        "Now, with the advent of deeper neural architecture such as transformers, it is possible to extract sentence representations directly using a pretrained transformer network. An example of a pre-trained transformer based model is BERT (stands for Bidirectional Encoder Representations from Transformers) .\n",
        "\n",
        "Sentence embeddings in BERT are representations of entire sentences generated by the BERT model. These embeddings capture the semantic meaning and contextual information of the input sentences. BERT employs a transformer architecture that processes words in the context of their surrounding words, enabling it to create contextualized word embeddings. By pooling or averaging these word embeddings, BERT can generate a fixed-size vector representation for the entire sentence. This embedding encodes various aspects of the sentence's meaning, including syntax, semantics, and contextual information, making it useful for various natural language processing tasks such as sentence similarity, classification, and translation.\n",
        "\n",
        "Let's try to repeat some of the above sections but with BERT based sentence embeddings this time.\n",
        "\n",
        "Ingredients:\n",
        "- the `sentence_transformers` library: A python library that is built on top of the `transformers` library by huggingface. We need to install this as this may not alreay be a part of the colab eco-system.  \n",
        "\n",
        "- the `bert-base-uncased` model: A BERT model variant, trained using large amount of web based text.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "02zTSRusE1OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I5ivrzLG-O1",
        "outputId": "673c604a-b302-4d28-aa89-55e1c15636a8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.4.99)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Computing sentence similarity using BERT based sentence vectors."
      ],
      "metadata": {
        "id": "TdQN8O-fHYjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load a pre-trained BERT model\n",
        "model = SentenceTransformer('bert-base-uncased')\n",
        "\n",
        "# Example sentences\n",
        "\n",
        "example_sentence1 = \"Natural Language Processing is a branch of Artificial Intelligence\".lower()\n",
        "example_sentence2 = \"The quick brown fox jumped over the laze dog\".lower()\n",
        "example_sentence3 = \"In the jungle the mighty jungle the lion sleeps tonight\".lower()\n",
        "\n",
        "all_sentences = [example_sentence1, example_sentence2, example_sentence3]\n",
        "sentence_embeddings = model.encode(all_sentences)\n",
        "\n",
        "# Compute similarity between the first two sentences\n",
        "similarity_score = similarity(sentence_embeddings[0],sentence_embeddings[1])\n",
        "print(f\"Similarity between the first two sentences: {similarity_score}\")\n",
        "\n",
        "# Compute similarity between the first and the third sentence\n",
        "similarity_score = similarity(sentence_embeddings[0],sentence_embeddings[2])\n",
        "print(f\"Similarity between the first and the third sentence: {similarity_score:.4f}\")\n",
        "\n",
        "# Compute similarity between the second and the third sentence\n",
        "similarity_score = similarity(sentence_embeddings[1],sentence_embeddings[2])\n",
        "print(f\"Similarity between the second and the third sentence: {similarity_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMxtqgZLGgFX",
        "outputId": "96bd7075-23a1-45b7-ea6d-c2d3352d219e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between the first two sentences: 0.4472642242908478\n",
            "Similarity between the first and the third sentence: 0.4397\n",
            "Similarity between the second and the third sentence: 0.6144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise E1: Repeat Zero-shot Text Classification using Sentence Vectors (Section 1.5)\n",
        "\n",
        "Repeat seciton 1.4. \"Zero-shot Text Classification using Sentence Vectors\" but this time use BERT sentence vectors instead of averaged word embedding.\n",
        "\n",
        "Change the generic sentences from `positive` and `negative` to sentential forms like `This is a positive sentence` and `This is a negative sentence`.\n",
        "\n",
        "Do you see any difference in predictions for the same examples we used in section 1.5?\n",
        "\n",
        "**Optional Exercise**: Load the `test.csv` file for IMDB movie review classification and compute the accuracy of the zero-shot classifier on the test data. Is it better or worse than the accuracy figures that we saw in our machine learning lab 7? Comment.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pdoth3X_JNcH"
      }
    }
  ]
}