{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArdRWAQyoEje"
      },
      "source": [
        "## Tutorial: Exploring Large Langauge Models\n",
        "\n",
        "**Note: This tutorial is graded. Please complete the exercises and turn it under Canvas->Files>-Week13**\n",
        "\n",
        "Also Note: IN colab, enable GPU before running this code.\n",
        "\n",
        "## Overview:\n",
        "Today, we'll delve into the fascinating world of large language models, specifically focusing on loading and utilizing instruction-tuned models. Our main language model for this session is Microsoft's `Phi2`, a remarkable language model boasting 2.7 billion parameters. Phi2 stands out for its exceptional reasoning and language understanding capabilities, setting the bar high among base models with less than 13 billion parameters.\n",
        "\n",
        "You can learn more about the `Phi2` model by visiting this link: https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/\n",
        "\n",
        "Phi-2 has been made part of the Huggingface Ecosystem (https://huggingface.co/microsoft/phi-2) and can be loaded and used through the `transformers` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgcvsHHxpaLC"
      },
      "source": [
        "## Loading Phi2 model\n",
        "\n",
        "Since Phi2 is already available as a part of Huggingface platform, we can load it using the transformers library in similar manner as we loaded BERT and other models in previous labs.\n",
        "\n",
        "We will also need the `accelerate` library for efficient model loading and data processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vWYia5J4rKf1",
        "outputId": "3f469b0e-ed0c-41c0-9825-709a9694a635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "Successfully installed transformers-4.39.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f046664e69fe47a189e1d98bc1cb6cfd",
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install accelerate\n",
        "%pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640,
          "referenced_widgets": [
            "ca09bf63e00b45fa9a5793b01dc0e20a",
            "18f804e9568e4ab08ef9a3213e930c71",
            "e797fa882c3a472fb3f40a10eba0f550",
            "cbc2cd6e1c704acbb520f9e947f0b5fd",
            "e3f9812ab2904f51ae675a86ce892fe7",
            "29aa783061354fde8aac82f1e15641ee",
            "1f01eb45cc3b4e1fbd88caaa1ba6d405",
            "0bbd79bf0ef4430f86ffd952c426d838",
            "90f5e5491aec40c38941f6ff13cb3cdc",
            "6d97459822994b50b56f9b56076fb8b4",
            "cf43ee6bf0724b699038c226a90c931b"
          ]
        },
        "id": "c4MFRfUQpocj",
        "outputId": "af7c5aeb-d0ac-4db0-dca2-34f313d0d1c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca09bf63e00b45fa9a5793b01dc0e20a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "PhiForCausalLM(\n",
              "  (model): PhiModel(\n",
              "    (embed_tokens): Embedding(51200, 2560)\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x PhiDecoderLayer(\n",
              "        (self_attn): PhiSdpaAttention(\n",
              "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "          (rotary_emb): PhiRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): PhiMLP(\n",
              "          (activation_fn): NewGELUActivation()\n",
              "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
              "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, PhiForCausalLM\n",
        "import torch\n",
        "\n",
        "# load model tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"microsoft/phi-2\",\n",
        "    trust_remote_code = True\n",
        ")\n",
        "\n",
        "# load model\n",
        "model = PhiForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-2\",\n",
        "    torch_dtype = \"auto\",\n",
        "    device_map = \"auto\",\n",
        "    trust_remote_code = True\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klcx449asDst"
      },
      "source": [
        "## Preparing the prompt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fzNZJenzsHff"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Write a short summary of the main idea and the key points of the following paragraph:\n",
        "\n",
        "Input: The Mount Rushmore National Memorial is a national memorial centered on a colossal\n",
        "sculpture carved into the granite face of Mount Rushmore\n",
        "in the Black Hills near Keystone, South Dakota, United States.\n",
        "Sculptor Gutzon Borglum designed the sculpture, called Shrine of Democracy, and oversaw\n",
        "the project's execution from 1927 to 1941 with the help of his son, Lincoln Borglum.\n",
        "The sculpture features the 60-foot-tall (18 m) heads of four\n",
        "United States presidents: George Washington, Thomas Jefferson,\n",
        "Theodore Roosevelt, and Abraham Lincoln, chosen to represent the nation's birth, growth,\n",
        "development and preservation, respectively.\n",
        "Mount Rushmore attracts more than two million visitors annually\n",
        "to the memorial park which covers 1,278 acres (2.00 sq mi; 5.17 km2).\n",
        "The mountain's elevation is 5,725 feet (1,745 m) above sea level.\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5o5SU3Qsu-J",
        "outputId": "9fb6ecd6-be64-48ac-9be9-2e6645502fb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Output: The paragraph is about the Mount Rushmore National Memorial, a national monument that honors four U.S. presidents with a massive sculpture carved into a mountain. The paragraph gives some background information on the sculptor, the design, and the location of the memorial, as well as some statistics on its popularity and size.\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False ,return_tensors=\"pt\")\n",
        "output_ids = model.generate(\n",
        "      token_ids.to(model.device),\n",
        "      max_new_tokens=100,\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids[0][token_ids.size(1) :])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHutq9-Dtbv2"
      },
      "source": [
        "## Exploring Chain of Throught (CoT) for in context learning:\n",
        "Chain of Thought (CoT) in large language models (LLMs) refers to a strategic approach in crafting text prompts to guide the model through a sequence of logical steps to accomplish a task.\n",
        "\n",
        "By chaining together these sequential steps, users can leverage the LLM's capabilities more effectively to generate desired outputs. We also avoid expensive fine-tuning of models to teach them how to perform certain tasks.\n",
        "\n",
        "Let's try to do Part of Speech (PoS) tagging with LLMs. We will try to PoS tag sentence with and without CoT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gGATbz8xu9D4"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Find part-of-speech tags for each token in the input sentence:\n",
        "\n",
        "The quick brown fox jumped over the lazy dog\"\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRnlfqXBvOkT",
        "outputId": "e7ca13b6-8db2-405f-9250-bc16c30277f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "```python\n",
            "import nltk\n",
            "\n",
            "# Define input sentence\n",
            "sentence = \"The quick brown fox jumped over the lazy dog\"\n",
            "\n",
            "# Tokenize sentence\n",
            "tokens = nltk.word_tokenize(sentence)\n",
            "\n",
            "# Find part-of-speech tags for each token\n",
            "pos_tags = nltk.pos_tag(tokens)\n",
            "\n",
            "# Print part-of-speech tags\n",
            "print(pos_tags\n"
          ]
        }
      ],
      "source": [
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False ,return_tensors=\"pt\")\n",
        "output_ids = model.generate(\n",
        "      token_ids.to(model.device),\n",
        "      max_new_tokens=100,\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids[0][token_ids.size(1) :])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFOUkQAOvVHS"
      },
      "source": [
        "As we can see, the output is a piece of broken python code and is not the expected output. Let's try to guide the model to do better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SXU6c3Q-vgVu"
      },
      "outputs": [],
      "source": [
        "cot_prompt = \"\"\"Find part-of-speech tags for each token in the input sentence:\n",
        "\n",
        "Input: \"The quick brown fox jumped over the lazy dog\"\n",
        "\n",
        "Example: Input: \"Marry had a little lamb\"\n",
        "Output: [(\"Marry\",\"NOUN\"), (\"had\", \"VERB\"), (\"a\", \"ARTICLE\"), (\"little\", \"ADJECTIVE\"), (\"lamb\", \"NOUN\")]\n",
        "\n",
        "Output:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6rebrVVxpBB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pwsUrlvwDMf",
        "outputId": "2cadcea5-913b-4694-9bb9-80795764d3a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(\"The\", \"DET\"), (\"quick\", \"ADJECTIVE\"), (\"brown\", \"ADJECTIVE\"), (\"fox\", \"NOUN\"), (\"jumped\", \"VERB\"), (\"over\", \"ADP\"), (\"the\", \"DET\"), (\"lazy\", \"ADJECTIVE\"), (\"dog\", \"NOUN\")]\n",
            "\n",
            "Solution:\n",
            "\n",
            "```python\n",
            "def get_pos_tags(sentence):\n",
            "    tokens = nltk.\n"
          ]
        }
      ],
      "source": [
        "token_ids = tokenizer.encode(cot_prompt, add_special_tokens=False ,return_tensors=\"pt\")\n",
        "output_ids = model.generate(\n",
        "      token_ids.to(model.device),\n",
        "      max_new_tokens=100,\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids[0][token_ids.size(1) :])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j6u6AtGwh3F"
      },
      "source": [
        "Better? Perhaps. But we need to tell the system not to output any python code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9xvZYASTxpsG"
      },
      "outputs": [],
      "source": [
        "\n",
        "modified_cot_prompt = \"\"\"Find part-of-speech tags for each token in the input sentence. Do not print any Python code:\n",
        "\n",
        "Input: \"The quick brown fox jumped over the lazy dog\"\n",
        "\n",
        "Example: Input: \"Marry had a little lamb\"\n",
        "Output: [(\"Marry\",\"NOUN\"), (\"had\", \"VERB\"), (\"a\", \"ARTICLE\"), (\"little\", \"ADJECTIVE\"), (\"lamb\", \"NOUN\")]\n",
        "\n",
        "Output:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPgP9QXwxzc7",
        "outputId": "6f1ceb7f-7f6c-4b60-e0ea-9e73cb2dc91a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(\"The\", \"DET\"), (\"quick\", \"ADJECTIVE\"), (\"brown\", \"ADJECTIVE\"), (\"fox\", \"NOUN\"), (\"jumped\", \"VERB\"), (\"over\", \"ADP\"), (\"the\", \"DET\"), (\"lazy\", \"ADJECTIVE\"), (\"dog\", \"NOUN\")]\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "token_ids = tokenizer.encode(cot_prompt, add_special_tokens=False ,return_tensors=\"pt\")\n",
        "output_ids = model.generate(\n",
        "      token_ids.to(model.device),\n",
        "      max_new_tokens=100,\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids[0][token_ids.size(1) :])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBUzeJlZyJPD"
      },
      "source": [
        "That's what we need. So, in-context learning helped improve model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_42q_XOkyQXk"
      },
      "source": [
        "## Exercise E1: Explore different prompting strategies for Named Entity Recognition Task\n",
        "\n",
        "1. As we have emphasized earlier, Named Entity Recognition aims at identifying proper nouns such as names of persons, organizations, geographical locations, word of art etc from the input text. NER serves as the backbone for many important information extraction related applications.\n",
        "\n",
        "2. In this exercise, explore various prompting strategies to identify and label named entities present in the input text. Start with a simple prompt and observe model's output.\n",
        "\n",
        "3. Apply CoT strategies similary to the PoS example above. Do you see any difference in results? Communicate your observations properly.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bbd79bf0ef4430f86ffd952c426d838": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18f804e9568e4ab08ef9a3213e930c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29aa783061354fde8aac82f1e15641ee",
            "placeholder": "​",
            "style": "IPY_MODEL_1f01eb45cc3b4e1fbd88caaa1ba6d405",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1f01eb45cc3b4e1fbd88caaa1ba6d405": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29aa783061354fde8aac82f1e15641ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d97459822994b50b56f9b56076fb8b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90f5e5491aec40c38941f6ff13cb3cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca09bf63e00b45fa9a5793b01dc0e20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18f804e9568e4ab08ef9a3213e930c71",
              "IPY_MODEL_e797fa882c3a472fb3f40a10eba0f550",
              "IPY_MODEL_cbc2cd6e1c704acbb520f9e947f0b5fd"
            ],
            "layout": "IPY_MODEL_e3f9812ab2904f51ae675a86ce892fe7"
          }
        },
        "cbc2cd6e1c704acbb520f9e947f0b5fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d97459822994b50b56f9b56076fb8b4",
            "placeholder": "​",
            "style": "IPY_MODEL_cf43ee6bf0724b699038c226a90c931b",
            "value": " 2/2 [00:25&lt;00:00, 10.96s/it]"
          }
        },
        "cf43ee6bf0724b699038c226a90c931b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3f9812ab2904f51ae675a86ce892fe7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e797fa882c3a472fb3f40a10eba0f550": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bbd79bf0ef4430f86ffd952c426d838",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90f5e5491aec40c38941f6ff13cb3cdc",
            "value": 2
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
