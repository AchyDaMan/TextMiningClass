{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heYP1DDCFmg0"
      },
      "source": [
        "## Lab8: Unsupervised Machine Leanring - Topic extraction example\n",
        "**Note:** This practicum is graded. Complete the exercises and turn in the assignment here under \"Lab 8: Keyword Extraction using Clustering and Topic Modeling Techniques\" (https://utexas.instructure.com/courses/1382133/assignments/6627284) by end of today (03/08)\n",
        "\n",
        "### Introduction:\n",
        "\n",
        "Topic extraction from text is a fundamental natural language processing (NLP) task that involves automatically identifying and categorizing the main themes or subjects present in a collection of textual documents. This unsupervised method is motivated by the need to efficiently organize and summarize large volumes of text data, making it more manageable and informative. By automatically uncovering latent topics within the text, topic extraction helps in various applications, including document clustering, recommendation systems, content summarization, and content understanding.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Consider a news website that publishes articles on various topics like politics, sports, technology, and entertainment. Using unsupervised topic extraction, we can automatically categorize each article into its respective topic without needing manual labels. This allows the website to:\n",
        "\n",
        "- Organize articles on its homepage by topics, making it easier for users to navigate and find articles of interest.\n",
        "- Recommend related articles to readers based on their past reading history, leveraging the identified topics.\n",
        "- Perform sentiment analysis on each topic to gauge public sentiment on current events or issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmnVJRa6GIqH"
      },
      "source": [
        "## 1. Using topic models such as LDA\n",
        "\n",
        "The Gensim library provides a widely used implementation of Latent Dirichlet Allocation (LDA) for topic modeling. LDA is a probabilistic generative model that assumes each document is a mixture of topics, and each word in a document is attributable to one of the document's topics. It iteratively uncovers these topics from the given text corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oiQv6FvFrOw",
        "outputId": "5748dcfa-7daa-455f-eb49-1867d2097564"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0: 0.020*\"routine\" + 0.020*\"vaccination\" + 0.020*\"essential\" + 0.020*\"achieving\" + 0.020*\"immunity\" + 0.020*\"herd\" + 0.020*\"spread\" + 0.020*\"artificial\" + 0.020*\"campaign\" + 0.020*\"infectious\"\n",
            "Topic 1: 0.018*\"crucial\" + 0.018*\"change\" + 0.018*\"access\" + 0.018*\"solar\" + 0.018*\"energy\" + 0.018*\"combating\" + 0.018*\"reducing\" + 0.018*\"renewable\" + 0.018*\"climate\" + 0.018*\"initiative\"\n",
            "Topic 2: 0.018*\"preventing\" + 0.018*\"disease\" + 0.018*\"regular\" + 0.018*\"diabetes\" + 0.018*\"lifestyle\" + 0.018*\"heart\" + 0.018*\"boast\" + 0.018*\"enhances\" + 0.018*\"component\" + 0.018*\"revolutionary\"\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Sample text data (you can replace this with your own text data)\n",
        "text_data = [\n",
        "    # Technology\n",
        "    \"The latest smartphone model boasts a revolutionary camera system that enhances low-light photography.\",\n",
        "    \"Artificial intelligence algorithms are reshaping industries by automating routine tasks and streamlining operations.\",\n",
        "    \"Quantum computing holds the promise of solving complex problems exponentially faster than classical computers.\",\n",
        "\n",
        "    # Environment\n",
        "    \"Deforestation in the Amazon rainforest continues to pose a significant threat to biodiversity and indigenous communities.\",\n",
        "    \"Renewable energy sources such as solar and wind power are crucial for reducing carbon emissions and combating climate change.\",\n",
        "    \"Plastic pollution in oceans is a pressing environmental issue, with millions of marine animals suffering from ingestion or entanglement.\",\n",
        "\n",
        "    # Health\n",
        "    \"Vaccination campaigns are essential for preventing the spread of infectious diseases and achieving herd immunity.\",\n",
        "    \"Mental health awareness initiatives aim to reduce stigma and promote access to support services for individuals struggling with psychological disorders.\",\n",
        "    \"Regular exercise and a balanced diet are key components of maintaining a healthy lifestyle and preventing chronic illnesses like heart disease and diabetes.\"\n",
        "]\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess(text):\n",
        "  # Remove punctuation and convert to lowercase\n",
        "  text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text.lower())\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "  # Remove stopwords\n",
        "  tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
        "  # Lemmatize words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "# Preprocess the text data\n",
        "processed_data = [preprocess(text) for text in text_data]\n",
        "\n",
        "# Create a Gensim dictionary from the processed data\n",
        "dictionary = corpora.Dictionary(processed_data)\n",
        "\n",
        "# Create a corpus\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_data]\n",
        "\n",
        "# Build the LDA model\n",
        "lda_model = LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print the topics and associated keywords\n",
        "for topic_id, topic_keywords in lda_model.print_topics():\n",
        "  print(f\"Topic {topic_id}: {topic_keywords}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj3jzkcKGMU6"
      },
      "source": [
        "## 2. By applying K-means Clustering on GloVe vectors\n",
        "\n",
        "- This method combines pre-trained word embeddings, such as GloVe, with K-means clustering to extract topics based on word similarity and clustering.\n",
        "- It represents words as vectors in a high-dimensional space and groups similar words into clusters, which can be interpreted as topics.\n",
        "\n",
        "**Example:** In the code snippet shared above, we used GloVe embeddings and K-means clustering to identify keywords and topics within a set of text documents. This method leverages the semantic similarity of words to form topic clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WPrejxxaZbW-"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Load GloVe word vectors (You can choose a different word vector model if you prefer)\n",
        "# We choose a 100d vector\n",
        "# I just realized that this is a much better way of getting glove vectors than manually downloading and using them\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blM-XcBbGWca",
        "outputId": "66392cb9-6890-4e92-efbb-4eb50601c635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 1: struggling, ocean, intelligence, like, system (total words: 63)\n",
            "Cluster 2: exponentially, renewable, model, quantum, photography (total words: 24)\n",
            "Cluster 0: chronic, suffering, stigma, diabetes, deforestation (total words: 18)\n"
          ]
        }
      ],
      "source": [
        "# Sample text data (you can replace this with your own text data)\n",
        "# Sample text data (you can replace this with your own text data)\n",
        "text_data = [\n",
        "    # Technology\n",
        "    \"The latest smartphone model boasts a revolutionary camera system that enhances low-light photography.\",\n",
        "    \"Artificial intelligence algorithms are reshaping industries by automating routine tasks and streamlining operations.\",\n",
        "    \"Quantum computing holds the promise of solving complex problems exponentially faster than classical computers.\",\n",
        "\n",
        "    # Environment\n",
        "    \"Deforestation in the Amazon rainforest continues to pose a significant threat to biodiversity and indigenous communities.\",\n",
        "    \"Renewable energy sources such as solar and wind power are crucial for reducing carbon emissions and combating climate change.\",\n",
        "    \"Plastic pollution in oceans is a pressing environmental issue, with millions of marine animals suffering from ingestion or entanglement.\",\n",
        "\n",
        "    # Health\n",
        "    \"Vaccination campaigns are essential for preventing the spread of infectious diseases and achieving herd immunity.\",\n",
        "    \"Mental health awareness initiatives aim to reduce stigma and promote access to support services for individuals struggling with psychological disorders.\",\n",
        "    \"Regular exercise and a balanced diet are key components of maintaining a healthy lifestyle and preventing chronic illnesses like heart disease and diabetes.\"\n",
        "]\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess(text):\n",
        "  # Remove punctuation and convert to lowercase\n",
        "  text = text.lower()\n",
        "  text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stopwords and lemmatize words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words(\"english\")]\n",
        "  return tokens\n",
        "\n",
        "# Preprocess the text data and store only unique words\n",
        "processed_data = [preprocess(text) for text in text_data]\n",
        "\n",
        "# Maintain a list of id_to_word\n",
        "token_list = []\n",
        "for doc in processed_data:\n",
        "  for token in doc:\n",
        "    token_list.append(token)\n",
        "\n",
        "token_list = list(set(token_list))\n",
        "\n",
        "# Create a list of word vectors for each token in the text\n",
        "word_vectors = []\n",
        "for token in token_list:\n",
        "  if token in glove_model:\n",
        "    word_vectors.append(glove_model[token])\n",
        "\n",
        "# Convert the list of word vectors to a numpy array because scikit-learn's kmeans accepts numpy array\n",
        "word_vectors = np.array(word_vectors)\n",
        "\n",
        "# Let's randomly choose K = 3 for the Kmeans algorithm\n",
        "# This means, we are assuming that three broad topics are covered in the data\n",
        "k_clusters = 3\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=k_clusters, random_state=0, n_init=10).fit(word_vectors)\n",
        "\n",
        "# Map words to their corresponding clusters\n",
        "word_clusters = {}\n",
        "for word, cluster_label in zip(token_list, kmeans.labels_):\n",
        "    if cluster_label not in word_clusters:\n",
        "        word_clusters[cluster_label] = []\n",
        "    word_clusters[cluster_label].append(word)\n",
        "\n",
        "# Print 5 words words in each cluster\n",
        "for cluster_label, cluster_words in word_clusters.items():\n",
        "    print(f\"Cluster {cluster_label}: {', '.join(cluster_words[:5])} (total words: {len(cluster_words)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KsG-ZiyhXyK"
      },
      "source": [
        "## Exercise E1. Analyze topics of a website data\n",
        "\n",
        "1. Pick up a website of your choice that has significant amount of text (e.g, wikipedia articles or blog pages). If you are a multilingual speaker, you can and should select a page that is in the other non-English langauge that you speak.\n",
        "\n",
        "2. Copy-paste the content of the page to a `txt` file and proceed with step 3. Alternatively, if you are fluent with web-scraping, you can scrape the website automatically and store the content in a file.\n",
        "\n",
        "3. Read the file, perform `sentence tokenization` using NLTK to obtain a list of sentences.\n",
        "\n",
        "4. Perform topic analysis following both LDA and Clustering based methods by reusing above code. Play with different parameters (number of topics / clusters, number of words per topic).\n",
        "\n",
        "5. Write down your observations and summary.\n",
        "\n",
        "**Optional:** Can you plot word clouds of topics based on the scroes returned by LDA / clustering methods? Do the word clouds look interesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from https://en.wikipedia.org/wiki/Force_carrier\n",
        "\n",
        "# part 2\n",
        "sentences = []\n",
        "with open(\"file.txt\") as f:\n",
        "    sentences = (f.readlines())\n",
        "\n",
        "x = 0\n",
        "for sen in sentences:\n",
        "    sentences[x] = sen.strip()\n",
        "    x += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0: 0.029*\"pollution\" + 0.029*\"entanglement\" + 0.029*\"environmental\" + 0.029*\"issue\" + 0.029*\"million\" + 0.029*\"computer\" + 0.029*\"ocean\" + 0.029*\"marine\" + 0.029*\"faster\" + 0.029*\"pressing\"\n",
            "Topic 1: 0.036*\"disorder\" + 0.036*\"aim\" + 0.036*\"struggling\" + 0.036*\"stigma\" + 0.036*\"mental\" + 0.036*\"support\" + 0.036*\"promote\" + 0.036*\"service\" + 0.036*\"health\" + 0.036*\"psychological\"\n",
            "Topic 2: 0.029*\"source\" + 0.029*\"energy\" + 0.029*\"change\" + 0.029*\"crucial\" + 0.029*\"emission\" + 0.029*\"solar\" + 0.029*\"power\" + 0.029*\"climate\" + 0.029*\"renewable\" + 0.029*\"wind\"\n",
            "Topic 3: 0.027*\"chronic\" + 0.027*\"key\" + 0.027*\"healthy\" + 0.027*\"lifestyle\" + 0.027*\"component\" + 0.027*\"regular\" + 0.027*\"exercise\" + 0.027*\"diabetes\" + 0.027*\"heart\" + 0.027*\"balanced\"\n",
            "Topic 4: 0.042*\"photography\" + 0.042*\"model\" + 0.042*\"smartphone\" + 0.042*\"revolutionary\" + 0.042*\"boast\" + 0.042*\"enhances\" + 0.042*\"camera\" + 0.042*\"lowlight\" + 0.042*\"latest\" + 0.042*\"system\"\n",
            "Topic 5: 0.042*\"disease\" + 0.042*\"preventing\" + 0.042*\"spread\" + 0.042*\"essential\" + 0.042*\"herd\" + 0.042*\"vaccination\" + 0.042*\"campaign\" + 0.042*\"infectious\" + 0.042*\"achieving\" + 0.042*\"immunity\"\n"
          ]
        }
      ],
      "source": [
        "# Define preprocessing function\n",
        "def preprocess(text):\n",
        "  # Remove punctuation and convert to lowercase\n",
        "  text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text.lower())\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "  # Remove stopwords\n",
        "  tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
        "  # Lemmatize words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "# Preprocess the text data\n",
        "processed_data = [preprocess(text) for text in text_data]\n",
        "\n",
        "# Create a Gensim dictionary from the processed data\n",
        "dictionary = corpora.Dictionary(processed_data)\n",
        "\n",
        "# Create a corpus\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_data]\n",
        "\n",
        "# Build the LDA model with 6 topics\n",
        "lda_model = LdaModel(corpus, num_topics=6, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print the topics and associated keywords\n",
        "for topic_id, topic_keywords in lda_model.print_topics():\n",
        "  print(f\"Topic {topic_id}: {topic_keywords}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It does not seem like changing the topics number after a certain point does anything. Might be the law of diminishing returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 3: struggling, intelligence, like, system, complex (total words: 30)\n",
            "Cluster 6: ocean, marine, climate, wind, amazon (total words: 8)\n",
            "Cluster 1: exponentially, pose, streamlining, combating, reshaping (total words: 15)\n",
            "Cluster 2: renewable, deforestation, energy, pollution, reduce (total words: 9)\n",
            "Cluster 0: chronic, suffering, stigma, diabetes, disorder (total words: 18)\n",
            "Cluster 5: model, quantum, photography, smartphone, lifestyle (total words: 12)\n",
            "Cluster 4: promote, million, support, health, awareness (total words: 13)\n"
          ]
        }
      ],
      "source": [
        "# Define preprocessing function\n",
        "def preprocess(text):\n",
        "  # Remove punctuation and convert to lowercase\n",
        "  text = text.lower()\n",
        "  text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stopwords and lemmatize words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words(\"english\")]\n",
        "  return tokens\n",
        "\n",
        "# Preprocess the text data and store only unique words\n",
        "processed_data = [preprocess(text) for text in text_data]\n",
        "\n",
        "# Maintain a list of id_to_word\n",
        "token_list = []\n",
        "for doc in processed_data:\n",
        "  for token in doc:\n",
        "    token_list.append(token)\n",
        "\n",
        "token_list = list(set(token_list))\n",
        "\n",
        "# Create a list of word vectors for each token in the text\n",
        "word_vectors = []\n",
        "for token in token_list:\n",
        "  if token in glove_model:\n",
        "    word_vectors.append(glove_model[token])\n",
        "\n",
        "# Convert the list of word vectors to a numpy array because scikit-learn's kmeans accepts numpy array\n",
        "word_vectors = np.array(word_vectors)\n",
        "\n",
        "# Let's randomly choose K = 7 for the Kmeans algorithm\n",
        "# This means, we are assuming that three broad topics are covered in the data\n",
        "k_clusters = 7\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=k_clusters, random_state=0, n_init=10).fit(word_vectors)\n",
        "\n",
        "# Map words to their corresponding clusters\n",
        "word_clusters = {}\n",
        "for word, cluster_label in zip(token_list, kmeans.labels_):\n",
        "    if cluster_label not in word_clusters:\n",
        "        word_clusters[cluster_label] = []\n",
        "    word_clusters[cluster_label].append(word)\n",
        "\n",
        "# Print 5 words words in each cluster\n",
        "for cluster_label, cluster_words in word_clusters.items():\n",
        "    print(f\"Cluster {cluster_label}: {', '.join(cluster_words[:5])} (total words: {len(cluster_words)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there is quite a lot of weirdness in this group of clusters. It seems to be working well though, getting the different clusters together to mark all the words in. I think it is good."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
