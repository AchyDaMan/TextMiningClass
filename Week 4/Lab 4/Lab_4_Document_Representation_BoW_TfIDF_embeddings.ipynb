{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab4: Lexical Analysis - Document Representation\n",
        "\n",
        "Note: This lab session is graded. Complete all exercises and submit it under **Canvas->Lab4** (https://utexas.instructure.com/courses/1382133/assignments/6619548) by no later than **02/08/2023, 11:59PM**. Please attempt all exercises.\n",
        "\n",
        "We will be using NLTK and Gensim for various preprocessing and lexical analysis steps:\n",
        "\n",
        "\n",
        "References:\n",
        "1. https://www.nltk.org/howto.html\n",
        "2. https://radimrehurek.com/gensim/auto_examples/index.html"
      ],
      "metadata": {
        "id": "iYmt3yulr3p3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Vector based search 1.0: Representing a Text Corpus through  N-hot vectorization and performing basic search by vector distance computation\n",
        "\n",
        "We are interested in featurizing (or vectorizing) a given text corpus so that we can search and retrieve sentences in the corpus that are most relevant to the given query."
      ],
      "metadata": {
        "id": "YVYnMTIosgfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample corpus of sentences\n",
        "# Sample corpus of sentences related to AI, ML, and NLP\n",
        "# Sample corpus with sentences containing punctuations, symbols, and abbreviations\n",
        "corpus = [\n",
        "    \"AI, ML, and NLP are fields of study in computer science.\",\n",
        "    \"Data preprocessing is crucial for effective NLP.\",\n",
        "    \"The model achieved 95% accuracy on the test set!\",\n",
        "    \"NLP tasks include sentiment analysis, named entity recognition, and more.\",\n",
        "    \"DL frameworks (e.g., TensorFlow, PyTorch) are widely used in AI research.\",\n",
        "    \"The conference is scheduled for Sep. 15-17, 2023.\",\n",
        "    \"BERT's pre-trained embeddings capture rich contextual information.\",\n",
        "    \"The algorithm outperformed SVM, k-NN, and Naive Bayes.\",\n",
        "    \"Neural networks can process sequences (e.g., sentences) efficiently.\",\n",
        "    \"I'm excited about AI advancements in healthcare.\",\n",
        "    \"RNNs are good at processing sequential data.\",\n",
        "    \"Unsupervised learning finds patterns without labeled data, right?\",\n",
        "    \"GPU acceleration is crucial for training deep learning models.\",\n",
        "    \"The paper introduced a novel attention mechanism.\",\n",
        "    \"Text classification: spam detection, topic categorization, sentiment analysis.\",\n",
        "    \"AI is revolutionizing industries across the globe.\",\n",
        "    \"ML algorithms can adapt to new data and improve over time.\",\n",
        "    \"The company uses NLP for customer feedback analysis.\",\n",
        "    \"DL techniques have enabled breakthroughs in image generation.\",\n",
        "    \"AI, ML, NLP - these fields overlap in many ways.\",\n",
        "    \"The workshop will cover AI ethics, explainable ML, and more.\"\n",
        "]\n",
        "\n",
        "# Split words with space : Basic Tokenization\n",
        "processed_corpus = [c.split() for c in corpus]\n",
        "\n",
        "# Define the N for N-grams\n",
        "N = 1  # Change this to the desired value for N-grams\n",
        "\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False,binary = True, analyzer=lambda x:x)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(processed_corpus)\n",
        "n_hot_matrix = vectorizer.transform(processed_corpus)\n",
        "\n",
        "print ('PC',processed_corpus)\n",
        "# Get the feature names (N-grams)\n",
        "n_gram_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the length of vocabulary (unique words)\n",
        "vocabulary_dict = vectorizer.vocabulary_\n",
        "print(f\"Vocabulary length {len(vocabulary_dict)}\")\n",
        "\n",
        "# Print the vocabulary items\n",
        "print(f\"Vocabulary items {sorted(vocabulary_dict.items(),key=lambda x:x[0])}\")\n",
        "\n",
        "# Convert the matrix to an array and print the result for the first 5 instances\n",
        "print(\"N-Hot Vectorization:\")\n",
        "for vec in n_hot_matrix.toarray().tolist()[:5]:\n",
        "  print (vec)\n",
        "\n",
        "print(\"Find top similar sentences w.r.t a given query\")\n",
        "query = \"algorithm\"\n",
        "query = [[query.lower()]]\n",
        "print (\"Query\",query)\n",
        "\n",
        "query_vector = vectorizer.transform(query).toarray()\n",
        "print (\"Query vector\", query_vector)\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "distances = pairwise_distances(query_vector, n_hot_matrix.toarray(), metric='euclidean')[0]\n",
        "\n",
        "#sort indices in ascending order\n",
        "sorted_indices = np.argsort(distances)\n",
        "\n",
        "# print top 5 most similar queries\n",
        "print (f\"Printing top 5 most similar text for query {query}\")\n",
        "for q in sorted_indices[:5]:\n",
        "  print(corpus[q], \"Score: \", distances[q])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9QrRBj1sf89",
        "outputId": "9dbce431-fb30-4291-813c-78020f046dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PC [['AI,', 'ML,', 'and', 'NLP', 'are', 'fields', 'of', 'study', 'in', 'computer', 'science.'], ['Data', 'preprocessing', 'is', 'crucial', 'for', 'effective', 'NLP.'], ['The', 'model', 'achieved', '95%', 'accuracy', 'on', 'the', 'test', 'set!'], ['NLP', 'tasks', 'include', 'sentiment', 'analysis,', 'named', 'entity', 'recognition,', 'and', 'more.'], ['DL', 'frameworks', '(e.g.,', 'TensorFlow,', 'PyTorch)', 'are', 'widely', 'used', 'in', 'AI', 'research.'], ['The', 'conference', 'is', 'scheduled', 'for', 'Sep.', '15-17,', '2023.'], [\"BERT's\", 'pre-trained', 'embeddings', 'capture', 'rich', 'contextual', 'information.'], ['The', 'algorithm', 'outperformed', 'SVM,', 'k-NN,', 'and', 'Naive', 'Bayes.'], ['Neural', 'networks', 'can', 'process', 'sequences', '(e.g.,', 'sentences)', 'efficiently.'], [\"I'm\", 'excited', 'about', 'AI', 'advancements', 'in', 'healthcare.'], ['RNNs', 'are', 'good', 'at', 'processing', 'sequential', 'data.'], ['Unsupervised', 'learning', 'finds', 'patterns', 'without', 'labeled', 'data,', 'right?'], ['GPU', 'acceleration', 'is', 'crucial', 'for', 'training', 'deep', 'learning', 'models.'], ['The', 'paper', 'introduced', 'a', 'novel', 'attention', 'mechanism.'], ['Text', 'classification:', 'spam', 'detection,', 'topic', 'categorization,', 'sentiment', 'analysis.'], ['AI', 'is', 'revolutionizing', 'industries', 'across', 'the', 'globe.'], ['ML', 'algorithms', 'can', 'adapt', 'to', 'new', 'data', 'and', 'improve', 'over', 'time.'], ['The', 'company', 'uses', 'NLP', 'for', 'customer', 'feedback', 'analysis.'], ['DL', 'techniques', 'have', 'enabled', 'breakthroughs', 'in', 'image', 'generation.'], ['AI,', 'ML,', 'NLP', '-', 'these', 'fields', 'overlap', 'in', 'many', 'ways.'], ['The', 'workshop', 'will', 'cover', 'AI', 'ethics,', 'explainable', 'ML,', 'and', 'more.']]\n",
            "Vocabulary length 139\n",
            "Vocabulary items [('(e.g.,', 0), ('-', 1), ('15-17,', 2), ('2023.', 3), ('95%', 4), ('AI', 5), ('AI,', 6), (\"BERT's\", 7), ('Bayes.', 8), ('DL', 9), ('Data', 10), ('GPU', 11), (\"I'm\", 12), ('ML', 13), ('ML,', 14), ('NLP', 15), ('NLP.', 16), ('Naive', 17), ('Neural', 18), ('PyTorch)', 19), ('RNNs', 20), ('SVM,', 21), ('Sep.', 22), ('TensorFlow,', 23), ('Text', 24), ('The', 25), ('Unsupervised', 26), ('a', 27), ('about', 28), ('acceleration', 29), ('accuracy', 30), ('achieved', 31), ('across', 32), ('adapt', 33), ('advancements', 34), ('algorithm', 35), ('algorithms', 36), ('analysis,', 37), ('analysis.', 38), ('and', 39), ('are', 40), ('at', 41), ('attention', 42), ('breakthroughs', 43), ('can', 44), ('capture', 45), ('categorization,', 46), ('classification:', 47), ('company', 48), ('computer', 49), ('conference', 50), ('contextual', 51), ('cover', 52), ('crucial', 53), ('customer', 54), ('data', 55), ('data,', 56), ('data.', 57), ('deep', 58), ('detection,', 59), ('effective', 60), ('efficiently.', 61), ('embeddings', 62), ('enabled', 63), ('entity', 64), ('ethics,', 65), ('excited', 66), ('explainable', 67), ('feedback', 68), ('fields', 69), ('finds', 70), ('for', 71), ('frameworks', 72), ('generation.', 73), ('globe.', 74), ('good', 75), ('have', 76), ('healthcare.', 77), ('image', 78), ('improve', 79), ('in', 80), ('include', 81), ('industries', 82), ('information.', 83), ('introduced', 84), ('is', 85), ('k-NN,', 86), ('labeled', 87), ('learning', 88), ('many', 89), ('mechanism.', 90), ('model', 91), ('models.', 92), ('more.', 93), ('named', 94), ('networks', 95), ('new', 96), ('novel', 97), ('of', 98), ('on', 99), ('outperformed', 100), ('over', 101), ('overlap', 102), ('paper', 103), ('patterns', 104), ('pre-trained', 105), ('preprocessing', 106), ('process', 107), ('processing', 108), ('recognition,', 109), ('research.', 110), ('revolutionizing', 111), ('rich', 112), ('right?', 113), ('scheduled', 114), ('science.', 115), ('sentences)', 116), ('sentiment', 117), ('sequences', 118), ('sequential', 119), ('set!', 120), ('spam', 121), ('study', 122), ('tasks', 123), ('techniques', 124), ('test', 125), ('the', 126), ('these', 127), ('time.', 128), ('to', 129), ('topic', 130), ('training', 131), ('used', 132), ('uses', 133), ('ways.', 134), ('widely', 135), ('will', 136), ('without', 137), ('workshop', 138)]\n",
            "N-Hot Vectorization:\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "Find top similar sentences w.r.t a given query\n",
            "Query [['algorithm']]\n",
            "Query vector [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Printing top 5 most similar text for query [['algorithm']]\n",
            "The algorithm outperformed SVM, k-NN, and Naive Bayes. Score:  2.6457513110645907\n",
            "RNNs are good at processing sequential data. Score:  2.8284271247461903\n",
            "Data preprocessing is crucial for effective NLP. Score:  2.8284271247461903\n",
            "BERT's pre-trained embeddings capture rich contextual information. Score:  2.8284271247461903\n",
            "I'm excited about AI advancements in healthcare. Score:  2.8284271247461903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Vector based search 2.0: Recucing vocabulary size through *tokenization*\n",
        "\n",
        "In certain cases, the inclusion of punctuation and various markers can needlessly inflate the vocabulary size and lead to suboptimal matching results. A more effective approach to feature engineering involves refining the vocabulary by isolating it from punctuation marks. This separation of punctuations is seamlessly integrated within the tokenization process, where text is divided based on word delimiters such as spaces. This strategy not only streamlines vocabulary construction but also enhances the quality of the overall search mechanism."
      ],
      "metadata": {
        "id": "n6Tsuwd8xri7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = [\n",
        "    \"AI, ML, and NLP are fields of study in computer science.\",\n",
        "    \"Data preprocessing is crucial for effective NLP.\",\n",
        "    \"The model achieved 95% accuracy on the test set!\",\n",
        "    \"NLP tasks include sentiment analysis, named entity recognition, and more.\",\n",
        "    \"DL frameworks (e.g., TensorFlow, PyTorch) are widely used in AI research.\",\n",
        "    \"The conference is scheduled for Sep. 15-17, 2023.\",\n",
        "    \"BERT's pre-trained embeddings capture rich contextual information.\",\n",
        "    \"The algorithm outperformed SVM, k-NN, and Naive Bayes.\",\n",
        "    \"Neural networks can process sequences (e.g., sentences) efficiently.\",\n",
        "    \"I'm excited about AI advancements in healthcare.\",\n",
        "    \"RNNs are good at processing sequential data.\",\n",
        "    \"Unsupervised learning finds patterns without labeled data, right?\",\n",
        "    \"GPU acceleration is crucial for training deep learning models.\",\n",
        "    \"The paper introduced a novel attention mechanism.\",\n",
        "    \"Text classification: spam detection, topic categorization, sentiment analysis.\",\n",
        "    \"AI is revolutionizing industries across the globe.\",\n",
        "    \"ML algorithms can adapt to new data and improve over time.\",\n",
        "    \"The company uses NLP for customer feedback analysis.\",\n",
        "    \"DL techniques have enabled breakthroughs in image generation.\",\n",
        "    \"AI, ML, NLP - these fields overlap in many ways.\",\n",
        "    \"The workshop will cover AI ethics, explainable ML, and more.\"\n",
        "]\n",
        "\n",
        "processed_corpus = [word_tokenize(sentence) for sentence in corpus]\n",
        "\n",
        "print (\"Printing a few tokenized sentences\")\n",
        "for tokenized_sentence in processed_corpus[:5]:\n",
        "  print (tokenized_sentence)\n",
        "\n",
        "# Define the N for N-grams\n",
        "N = 1  # Change this to the desired value for N-grams\n",
        "\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False,binary = True, analyzer=lambda x:x)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(processed_corpus)\n",
        "n_hot_matrix = vectorizer.transform(processed_corpus)\n",
        "\n",
        "# Print the length of vocabulary (unique words)\n",
        "vocabulary_dict = vectorizer.vocabulary_\n",
        "print(f\"Vocabulary length {len(vocabulary_dict)}\")\n",
        "\n",
        "# Print the vocabulary items\n",
        "print(f\"Vocabulary items {sorted(vocabulary_dict.items(),key=lambda x:x[0])}\")\n",
        "\n",
        "# Convert the matrix to an array and print the result for the first 5 instances\n",
        "print(\"N-Hot Vectorization:\")\n",
        "for vec in n_hot_matrix.toarray().tolist()[:5]:\n",
        "  print (vec)\n",
        "\n",
        "print(\"Find top similar sentences w.r.t a given query\")\n",
        "query = \"algorithm\"\n",
        "query = [[query.lower()]]\n",
        "print (\"Query\",query)\n",
        "\n",
        "query_vector = vectorizer.transform(query).toarray()\n",
        "print (\"Query vector\", query_vector)\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "distances = pairwise_distances(query_vector, n_hot_matrix.toarray(), metric='euclidean')[0]\n",
        "\n",
        "#sort indices in ascending order\n",
        "sorted_indices = np.argsort(distances)\n",
        "\n",
        "# print top 5 most similar queries\n",
        "print (f\"Printing top 5 most similar text for query {query}\")\n",
        "for q in sorted_indices[:5]:\n",
        "  print(corpus[q], \"Score: \", distances[q])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqzuBHotxq24",
        "outputId": "f3029044-6965-49c8-ead2-9bb23eeb7fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing a few tokenized sentences\n",
            "['AI', ',', 'ML', ',', 'and', 'NLP', 'are', 'fields', 'of', 'study', 'in', 'computer', 'science', '.']\n",
            "['Data', 'preprocessing', 'is', 'crucial', 'for', 'effective', 'NLP', '.']\n",
            "['The', 'model', 'achieved', '95', '%', 'accuracy', 'on', 'the', 'test', 'set', '!']\n",
            "['NLP', 'tasks', 'include', 'sentiment', 'analysis', ',', 'named', 'entity', 'recognition', ',', 'and', 'more', '.']\n",
            "['DL', 'frameworks', '(', 'e.g.', ',', 'TensorFlow', ',', 'PyTorch', ')', 'are', 'widely', 'used', 'in', 'AI', 'research', '.']\n",
            "Vocabulary length 143\n",
            "Vocabulary items [('!', 0), ('%', 1), (\"'m\", 2), (\"'s\", 3), ('(', 4), (')', 5), (',', 6), ('-', 7), ('.', 8), ('15-17', 9), ('2023', 10), ('95', 11), (':', 12), ('?', 13), ('AI', 14), ('BERT', 15), ('Bayes', 16), ('DL', 17), ('Data', 18), ('GPU', 19), ('I', 20), ('ML', 21), ('NLP', 22), ('Naive', 23), ('Neural', 24), ('PyTorch', 25), ('RNNs', 26), ('SVM', 27), ('Sep.', 28), ('TensorFlow', 29), ('Text', 30), ('The', 31), ('Unsupervised', 32), ('a', 33), ('about', 34), ('acceleration', 35), ('accuracy', 36), ('achieved', 37), ('across', 38), ('adapt', 39), ('advancements', 40), ('algorithm', 41), ('algorithms', 42), ('analysis', 43), ('and', 44), ('are', 45), ('at', 46), ('attention', 47), ('breakthroughs', 48), ('can', 49), ('capture', 50), ('categorization', 51), ('classification', 52), ('company', 53), ('computer', 54), ('conference', 55), ('contextual', 56), ('cover', 57), ('crucial', 58), ('customer', 59), ('data', 60), ('deep', 61), ('detection', 62), ('e.g.', 63), ('effective', 64), ('efficiently', 65), ('embeddings', 66), ('enabled', 67), ('entity', 68), ('ethics', 69), ('excited', 70), ('explainable', 71), ('feedback', 72), ('fields', 73), ('finds', 74), ('for', 75), ('frameworks', 76), ('generation', 77), ('globe', 78), ('good', 79), ('have', 80), ('healthcare', 81), ('image', 82), ('improve', 83), ('in', 84), ('include', 85), ('industries', 86), ('information', 87), ('introduced', 88), ('is', 89), ('k-NN', 90), ('labeled', 91), ('learning', 92), ('many', 93), ('mechanism', 94), ('model', 95), ('models', 96), ('more', 97), ('named', 98), ('networks', 99), ('new', 100), ('novel', 101), ('of', 102), ('on', 103), ('outperformed', 104), ('over', 105), ('overlap', 106), ('paper', 107), ('patterns', 108), ('pre-trained', 109), ('preprocessing', 110), ('process', 111), ('processing', 112), ('recognition', 113), ('research', 114), ('revolutionizing', 115), ('rich', 116), ('right', 117), ('scheduled', 118), ('science', 119), ('sentences', 120), ('sentiment', 121), ('sequences', 122), ('sequential', 123), ('set', 124), ('spam', 125), ('study', 126), ('tasks', 127), ('techniques', 128), ('test', 129), ('the', 130), ('these', 131), ('time', 132), ('to', 133), ('topic', 134), ('training', 135), ('used', 136), ('uses', 137), ('ways', 138), ('widely', 139), ('will', 140), ('without', 141), ('workshop', 142)]\n",
            "N-Hot Vectorization:\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "Find top similar sentences w.r.t a given query\n",
            "Query [['algorithm']]\n",
            "Query vector [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Printing top 5 most similar text for query [['algorithm']]\n",
            "RNNs are good at processing sequential data. Score:  3.0\n",
            "Data preprocessing is crucial for effective NLP. Score:  3.0\n",
            "The algorithm outperformed SVM, k-NN, and Naive Bayes. Score:  3.0\n",
            "AI is revolutionizing industries across the globe. Score:  3.0\n",
            "The paper introduced a novel attention mechanism. Score:  3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Vocabulary size increased slightly which is an aberration for a smaller corpus like this. For larger corpora though, vocabulary size will reduce considerably as we apply punctuation based tokenization. Many words with augmentated punctuation marks are responsible for increasing vocabulary size."
      ],
      "metadata": {
        "id": "D4kho0KEzdMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Vector based search 3.0: Sometimes lowercasing helps reduce vocabulary size\n",
        "\n",
        "Unless we really need true-cases (i.e., capitalization), we can convert everything to a single consistent casing -- lower casing is generally picked. *\\[Quesiton: Can you think about an task/applicaiton where lower caseing is not a good idea?\\]*\n",
        "\n",
        "Lower casing can further reduce the vocabulary size."
      ],
      "metadata": {
        "id": "9J7ORGwl0AnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = [\n",
        "    \"AI, ML, and NLP are fields of study in computer science.\",\n",
        "    \"Data preprocessing is crucial for effective NLP.\",\n",
        "    \"The model achieved 95% accuracy on the test set!\",\n",
        "    \"NLP tasks include sentiment analysis, named entity recognition, and more.\",\n",
        "    \"DL frameworks (e.g., TensorFlow, PyTorch) are widely used in AI research.\",\n",
        "    \"The conference is scheduled for Sep. 15-17, 2023.\",\n",
        "    \"BERT's pre-trained embeddings capture rich contextual information.\",\n",
        "    \"The algorithm outperformed SVM, k-NN, and Naive Bayes.\",\n",
        "    \"Neural networks can process sequences (e.g., sentences) efficiently.\",\n",
        "    \"I'm excited about AI advancements in healthcare.\",\n",
        "    \"RNNs are good at processing sequential data.\",\n",
        "    \"Unsupervised learning finds patterns without labeled data, right?\",\n",
        "    \"GPU acceleration is crucial for training deep learning models.\",\n",
        "    \"The paper introduced a novel attention mechanism.\",\n",
        "    \"Text classification: spam detection, topic categorization, sentiment analysis.\",\n",
        "    \"AI is revolutionizing industries across the globe.\",\n",
        "    \"ML algorithms can adapt to new data and improve over time.\",\n",
        "    \"The company uses NLP for customer feedback analysis.\",\n",
        "    \"DL techniques have enabled breakthroughs in image generation.\",\n",
        "    \"AI, ML, NLP - these fields overlap in many ways.\",\n",
        "    \"The workshop will cover AI ethics, explainable ML, and more.\"\n",
        "]\n",
        "\n",
        "# tokenize using NLTK punct tokenizer for English\n",
        "processed_corpus = [word_tokenize(sentence) for sentence in corpus]\n",
        "\n",
        "# Lower case the processed corpus\n",
        "processed_corpus_lc = []\n",
        "for tokenized_sent in processed_corpus:\n",
        "  processed_corpus_lc.append([word.lower() for word in tokenized_sent])\n",
        "\n",
        "print (\"Printing a few tokenized sentences\")\n",
        "for tokenized_sentence in processed_corpus_lc[:5]:\n",
        "  print (tokenized_sentence)\n",
        "\n",
        "# Define the N for N-grams\n",
        "N = 1  # Change this to the desired value for N-grams\n",
        "\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False,binary = True, analyzer=lambda x:x)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(processed_corpus_lc)\n",
        "n_hot_matrix = vectorizer.transform(processed_corpus_lc)\n",
        "\n",
        "# Print the length of vocabulary (unique words)\n",
        "vocabulary_dict = vectorizer.vocabulary_\n",
        "print(f\"Vocabulary length {len(vocabulary_dict)}\")\n",
        "\n",
        "# Print the vocabulary items\n",
        "print(f\"Vocabulary items {sorted(vocabulary_dict.items(),key=lambda x:x[0])}\")\n",
        "\n",
        "# Convert the matrix to an array and print the result for the first 5 instances\n",
        "print(\"N-Hot Vectorization:\")\n",
        "for vec in n_hot_matrix.toarray().tolist()[:5]:\n",
        "  print (vec)\n",
        "\n",
        "print(\"Find top similar sentences w.r.t a given query\")\n",
        "query = \"algorithm\"\n",
        "query = [[query.lower()]]\n",
        "print (\"Query\",query)\n",
        "\n",
        "query_vector = vectorizer.transform(query).toarray()\n",
        "print (\"Query vector\", query_vector)\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "distances = pairwise_distances(query_vector, n_hot_matrix.toarray(), metric='euclidean')[0]\n",
        "\n",
        "#sort indices in ascending order\n",
        "sorted_indices = np.argsort(distances)\n",
        "\n",
        "# print top 5 most similar queries\n",
        "print (f\"Printing top 5 most similar text for query {query}\")\n",
        "for q in sorted_indices[:5]:\n",
        "  print(corpus[q], \"Score: \", distances[q])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mnlQVaLxc8i",
        "outputId": "1bbfa6cd-59ad-4734-cc77-90820fca0f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing a few tokenized sentences\n",
            "['ai', ',', 'ml', ',', 'and', 'nlp', 'are', 'fields', 'of', 'study', 'in', 'computer', 'science', '.']\n",
            "['data', 'preprocessing', 'is', 'crucial', 'for', 'effective', 'nlp', '.']\n",
            "['the', 'model', 'achieved', '95', '%', 'accuracy', 'on', 'the', 'test', 'set', '!']\n",
            "['nlp', 'tasks', 'include', 'sentiment', 'analysis', ',', 'named', 'entity', 'recognition', ',', 'and', 'more', '.']\n",
            "['dl', 'frameworks', '(', 'e.g.', ',', 'tensorflow', ',', 'pytorch', ')', 'are', 'widely', 'used', 'in', 'ai', 'research', '.']\n",
            "Vocabulary length 141\n",
            "Vocabulary items [('!', 0), ('%', 1), (\"'m\", 2), (\"'s\", 3), ('(', 4), (')', 5), (',', 6), ('-', 7), ('.', 8), ('15-17', 9), ('2023', 10), ('95', 11), (':', 12), ('?', 13), ('a', 14), ('about', 15), ('acceleration', 16), ('accuracy', 17), ('achieved', 18), ('across', 19), ('adapt', 20), ('advancements', 21), ('ai', 22), ('algorithm', 23), ('algorithms', 24), ('analysis', 25), ('and', 26), ('are', 27), ('at', 28), ('attention', 29), ('bayes', 30), ('bert', 31), ('breakthroughs', 32), ('can', 33), ('capture', 34), ('categorization', 35), ('classification', 36), ('company', 37), ('computer', 38), ('conference', 39), ('contextual', 40), ('cover', 41), ('crucial', 42), ('customer', 43), ('data', 44), ('deep', 45), ('detection', 46), ('dl', 47), ('e.g.', 48), ('effective', 49), ('efficiently', 50), ('embeddings', 51), ('enabled', 52), ('entity', 53), ('ethics', 54), ('excited', 55), ('explainable', 56), ('feedback', 57), ('fields', 58), ('finds', 59), ('for', 60), ('frameworks', 61), ('generation', 62), ('globe', 63), ('good', 64), ('gpu', 65), ('have', 66), ('healthcare', 67), ('i', 68), ('image', 69), ('improve', 70), ('in', 71), ('include', 72), ('industries', 73), ('information', 74), ('introduced', 75), ('is', 76), ('k-nn', 77), ('labeled', 78), ('learning', 79), ('many', 80), ('mechanism', 81), ('ml', 82), ('model', 83), ('models', 84), ('more', 85), ('naive', 86), ('named', 87), ('networks', 88), ('neural', 89), ('new', 90), ('nlp', 91), ('novel', 92), ('of', 93), ('on', 94), ('outperformed', 95), ('over', 96), ('overlap', 97), ('paper', 98), ('patterns', 99), ('pre-trained', 100), ('preprocessing', 101), ('process', 102), ('processing', 103), ('pytorch', 104), ('recognition', 105), ('research', 106), ('revolutionizing', 107), ('rich', 108), ('right', 109), ('rnns', 110), ('scheduled', 111), ('science', 112), ('sentences', 113), ('sentiment', 114), ('sep.', 115), ('sequences', 116), ('sequential', 117), ('set', 118), ('spam', 119), ('study', 120), ('svm', 121), ('tasks', 122), ('techniques', 123), ('tensorflow', 124), ('test', 125), ('text', 126), ('the', 127), ('these', 128), ('time', 129), ('to', 130), ('topic', 131), ('training', 132), ('unsupervised', 133), ('used', 134), ('uses', 135), ('ways', 136), ('widely', 137), ('will', 138), ('without', 139), ('workshop', 140)]\n",
            "N-Hot Vectorization:\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
            "Find top similar sentences w.r.t a given query\n",
            "Query [['algorithm']]\n",
            "Query vector [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Printing top 5 most similar text for query [['algorithm']]\n",
            "RNNs are good at processing sequential data. Score:  3.0\n",
            "Data preprocessing is crucial for effective NLP. Score:  3.0\n",
            "The algorithm outperformed SVM, k-NN, and Naive Bayes. Score:  3.0\n",
            "AI is revolutionizing industries across the globe. Score:  3.0\n",
            "The paper introduced a novel attention mechanism. Score:  3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Vector based search 4.0: Reducing vocabulary size further through stopword removal\n",
        "\n",
        "For certain applicaiton (like information retrieval) we do not need stopwords (e.g., articles, prepositions, punctuations). We can remove them to further reduce vocabulary size."
      ],
      "metadata": {
        "id": "KGzlCEhV26O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = [\n",
        "    \"AI, ML, and NLP are fields of study in computer science.\",\n",
        "    \"Data preprocessing is crucial for effective NLP.\",\n",
        "    \"The model achieved 95% accuracy on the test set!\",\n",
        "    \"NLP tasks include sentiment analysis, named entity recognition, and more.\",\n",
        "    \"DL frameworks (e.g., TensorFlow, PyTorch) are widely used in AI research.\",\n",
        "    \"The conference is scheduled for Sep. 15-17, 2023.\",\n",
        "    \"BERT's pre-trained embeddings capture rich contextual information.\",\n",
        "    \"The algorithm outperformed SVM, k-NN, and Naive Bayes.\",\n",
        "    \"Neural networks can process sequences (e.g., sentences) efficiently.\",\n",
        "    \"I'm excited about AI advancements in healthcare.\",\n",
        "    \"RNNs are good at processing sequential data.\",\n",
        "    \"Unsupervised learning finds patterns without labeled data, right?\",\n",
        "    \"GPU acceleration is crucial for training deep learning models.\",\n",
        "    \"The paper introduced a novel attention mechanism.\",\n",
        "    \"Text classification: spam detection, topic categorization, sentiment analysis.\",\n",
        "    \"AI is revolutionizing industries across the globe.\",\n",
        "    \"ML algorithms can adapt to new data and improve over time.\",\n",
        "    \"The company uses NLP for customer feedback analysis.\",\n",
        "    \"DL techniques have enabled breakthroughs in image generation.\",\n",
        "    \"AI, ML, NLP - these fields overlap in many ways.\",\n",
        "    \"The workshop will cover AI ethics, explainable ML, and more.\"\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# tokenize using NLTK punct tokenizer for English\n",
        "processed_corpus = [word_tokenize(sentence) for sentence in corpus]\n",
        "\n",
        "# Lower case the processed corpus\n",
        "processed_corpus_lc_filtered = []\n",
        "for tokenized_sent in processed_corpus:\n",
        "  processed_corpus_lc_filtered.append([word.lower() for word in tokenized_sent if word not in stop_words])\n",
        "\n",
        "print (\"Printing a few tokenized sentences\")\n",
        "for tokenized_sentence in processed_corpus_lc_filtered[:5]:\n",
        "  print (tokenized_sentence)\n",
        "\n",
        "# Define the N for N-grams\n",
        "N = 1  # Change this to the desired value for N-grams\n",
        "\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False,binary = True, analyzer=lambda x:x)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(processed_corpus_lc_filtered)\n",
        "n_hot_matrix = vectorizer.transform(processed_corpus_lc_filtered)\n",
        "\n",
        "# Print the length of vocabulary (unique words)\n",
        "vocabulary_dict = vectorizer.vocabulary_\n",
        "print(f\"Vocabulary length {len(vocabulary_dict)}\")\n",
        "\n",
        "# Print the vocabulary items\n",
        "print(f\"Vocabulary items {sorted(vocabulary_dict.items(),key=lambda x:x[0])}\")\n",
        "\n",
        "# Convert the matrix to an array and print the result for the first 5 instances\n",
        "print(\"N-Hot Vectorization:\")\n",
        "for vec in n_hot_matrix.toarray().tolist()[:5]:\n",
        "  print (vec)\n",
        "\n",
        "print(\"Find top similar sentences w.r.t a given query\")\n",
        "query = \"algorithm\"\n",
        "query = [[query.lower()]]\n",
        "print (\"Query\",query)\n",
        "\n",
        "query_vector = vectorizer.transform(query).toarray()\n",
        "print (\"Query vector\", query_vector)\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "distances = pairwise_distances(query_vector, n_hot_matrix.toarray(), metric='euclidean')[0]\n",
        "\n",
        "#sort indices in ascending order\n",
        "sorted_indices = np.argsort(distances)\n",
        "\n",
        "# print top 5 most similar queries\n",
        "print (f\"Printing top 5 most similar text for query {query}\")\n",
        "for q in sorted_indices[:5]:\n",
        "  print(corpus[q], \"Score: \", distances[q])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ7cecKp24lo",
        "outputId": "3621d33b-765f-4421-c480-89146610395d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing a few tokenized sentences\n",
            "['ai', ',', 'ml', ',', 'nlp', 'fields', 'study', 'computer', 'science', '.']\n",
            "['data', 'preprocessing', 'crucial', 'effective', 'nlp', '.']\n",
            "['the', 'model', 'achieved', '95', '%', 'accuracy', 'test', 'set', '!']\n",
            "['nlp', 'tasks', 'include', 'sentiment', 'analysis', ',', 'named', 'entity', 'recognition', ',', '.']\n",
            "['dl', 'frameworks', '(', 'e.g.', ',', 'tensorflow', ',', 'pytorch', ')', 'widely', 'used', 'ai', 'research', '.']\n",
            "Vocabulary length 124\n",
            "Vocabulary items [('!', 0), ('%', 1), (\"'m\", 2), (\"'s\", 3), ('(', 4), (')', 5), (',', 6), ('-', 7), ('.', 8), ('15-17', 9), ('2023', 10), ('95', 11), (':', 12), ('?', 13), ('acceleration', 14), ('accuracy', 15), ('achieved', 16), ('across', 17), ('adapt', 18), ('advancements', 19), ('ai', 20), ('algorithm', 21), ('algorithms', 22), ('analysis', 23), ('attention', 24), ('bayes', 25), ('bert', 26), ('breakthroughs', 27), ('capture', 28), ('categorization', 29), ('classification', 30), ('company', 31), ('computer', 32), ('conference', 33), ('contextual', 34), ('cover', 35), ('crucial', 36), ('customer', 37), ('data', 38), ('deep', 39), ('detection', 40), ('dl', 41), ('e.g.', 42), ('effective', 43), ('efficiently', 44), ('embeddings', 45), ('enabled', 46), ('entity', 47), ('ethics', 48), ('excited', 49), ('explainable', 50), ('feedback', 51), ('fields', 52), ('finds', 53), ('frameworks', 54), ('generation', 55), ('globe', 56), ('good', 57), ('gpu', 58), ('healthcare', 59), ('i', 60), ('image', 61), ('improve', 62), ('include', 63), ('industries', 64), ('information', 65), ('introduced', 66), ('k-nn', 67), ('labeled', 68), ('learning', 69), ('many', 70), ('mechanism', 71), ('ml', 72), ('model', 73), ('models', 74), ('naive', 75), ('named', 76), ('networks', 77), ('neural', 78), ('new', 79), ('nlp', 80), ('novel', 81), ('outperformed', 82), ('overlap', 83), ('paper', 84), ('patterns', 85), ('pre-trained', 86), ('preprocessing', 87), ('process', 88), ('processing', 89), ('pytorch', 90), ('recognition', 91), ('research', 92), ('revolutionizing', 93), ('rich', 94), ('right', 95), ('rnns', 96), ('scheduled', 97), ('science', 98), ('sentences', 99), ('sentiment', 100), ('sep.', 101), ('sequences', 102), ('sequential', 103), ('set', 104), ('spam', 105), ('study', 106), ('svm', 107), ('tasks', 108), ('techniques', 109), ('tensorflow', 110), ('test', 111), ('text', 112), ('the', 113), ('time', 114), ('topic', 115), ('training', 116), ('unsupervised', 117), ('used', 118), ('uses', 119), ('ways', 120), ('widely', 121), ('without', 122), ('workshop', 123)]\n",
            "N-Hot Vectorization:\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
            "Find top similar sentences w.r.t a given query\n",
            "Query [['algorithm']]\n",
            "Query vector [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Printing top 5 most similar text for query [['algorithm']]\n",
            "RNNs are good at processing sequential data. Score:  2.6457513110645907\n",
            "Data preprocessing is crucial for effective NLP. Score:  2.6457513110645907\n",
            "AI is revolutionizing industries across the globe. Score:  2.6457513110645907\n",
            "DL techniques have enabled breakthroughs in image generation. Score:  2.8284271247461903\n",
            "The algorithm outperformed SVM, k-NN, and Naive Bayes. Score:  2.8284271247461903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Vector based search 5.0: Reducing vocabulary further through stemming\n",
        "- Stemming invloves extracting the base form of a word (which may not be in a linguistically valid form).\n",
        "\n",
        "- Stemming is useful for shallow information retrieval tasks where word meanings are not under consideration.\n",
        "\n",
        "- Since we are getting rid of inflections (suffixes and prefixes) through stemming, this will result in significant reduction of vocabulary size and result in a better match."
      ],
      "metadata": {
        "id": "9dOkXXnU1cTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "corpus = [\n",
        "    \"AI, ML, and NLP are fields of study in computer science.\",\n",
        "    \"Data preprocessing is crucial for effective NLP.\",\n",
        "    \"The model achieved 95% accuracy on the test set!\",\n",
        "    \"NLP tasks include sentiment analysis, named entity recognition, and more.\",\n",
        "    \"DL frameworks (e.g., TensorFlow, PyTorch) are widely used in AI research.\",\n",
        "    \"The conference is scheduled for Sep. 15-17, 2023.\",\n",
        "    \"BERT's pre-trained embeddings capture rich contextual information.\",\n",
        "    \"The algorithm outperformed SVM, k-NN, and Naive Bayes.\",\n",
        "    \"Neural networks can process sequences (e.g., sentences) efficiently.\",\n",
        "    \"I'm excited about AI advancements in healthcare.\",\n",
        "    \"RNNs are good at processing sequential data.\",\n",
        "    \"Unsupervised learning finds patterns without labeled data, right?\",\n",
        "    \"GPU acceleration is crucial for training deep learning models.\",\n",
        "    \"The paper introduced a novel attention mechanism.\",\n",
        "    \"Text classification: spam detection, topic categorization, sentiment analysis.\",\n",
        "    \"AI is revolutionizing industries across the globe.\",\n",
        "    \"ML algorithms can adapt to new data and improve over time.\",\n",
        "    \"The company uses NLP for customer feedback analysis.\",\n",
        "    \"DL techniques have enabled breakthroughs in image generation.\",\n",
        "    \"AI, ML, NLP - these fields overlap in many ways.\",\n",
        "    \"The workshop will cover AI ethics, explainable ML, and more.\"\n",
        "]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# tokenize using NLTK punct tokenizer for English\n",
        "processed_corpus = [word_tokenize(sentence) for sentence in corpus]\n",
        "\n",
        "# Lower case the processed corpus\n",
        "processed_corpus_lc_filtered = []\n",
        "for tokenized_sent in processed_corpus:\n",
        "  processed_corpus_lc_filtered.append([word.lower() for word in tokenized_sent if word not in stop_words])\n",
        "\n",
        "# Stem words using Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "processed_corpus_stemmed = []\n",
        "for tokenized_sent in processed_corpus_lc_filtered:\n",
        "  processed_corpus_stemmed.append([stemmer.stem(word) for word in tokenized_sent])\n",
        "\n",
        "print (\"Printing a few tokenized sentences\")\n",
        "for tokenized_sentence in processed_corpus_stemmed[:5]:\n",
        "  print (tokenized_sentence)\n",
        "\n",
        "# Define the N for N-grams\n",
        "N = 1  # Change this to the desired value for N-grams\n",
        "\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False,binary = True, analyzer=lambda x:x)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(processed_corpus_stemmed)\n",
        "n_hot_matrix = vectorizer.transform(processed_corpus_stemmed)\n",
        "\n",
        "# Print the length of vocabulary (unique words)\n",
        "vocabulary_dict = vectorizer.vocabulary_\n",
        "print(f\"Vocabulary length {len(vocabulary_dict)}\")\n",
        "\n",
        "# Print the vocabulary items\n",
        "print(f\"Vocabulary items {sorted(vocabulary_dict.items(),key=lambda x:x[0])}\")\n",
        "\n",
        "# Convert the matrix to an array and print the result for the first 5 instances\n",
        "print(\"N-Hot Vectorization:\")\n",
        "for vec in n_hot_matrix.toarray().tolist()[:5]:\n",
        "  print (vec)\n",
        "\n",
        "print(\"Find top similar sentences w.r.t a given query\")\n",
        "query = \"algorithm\"\n",
        "query = [[query.lower()]]\n",
        "print (\"Query\",query)\n",
        "\n",
        "query_vector = vectorizer.transform(query).toarray()\n",
        "print (\"Query vector\", query_vector)\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "distances = pairwise_distances(query_vector, n_hot_matrix.toarray(), metric='euclidean')[0]\n",
        "\n",
        "#sort indices in ascending order\n",
        "sorted_indices = np.argsort(distances)\n",
        "\n",
        "# print top 5 most similar queries\n",
        "print (f\"Printing top 5 most similar text for query {query}\")\n",
        "for q in sorted_indices[:5]:\n",
        "  print(corpus[q], \"Score: \", distances[q])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Am0Kk3J1bI7",
        "outputId": "dfe48e8a-c5f8-4eeb-e2cd-b40f16ade179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing a few tokenized sentences\n",
            "['ai', ',', 'ml', ',', 'nlp', 'field', 'studi', 'comput', 'scienc', '.']\n",
            "['data', 'preprocess', 'crucial', 'effect', 'nlp', '.']\n",
            "['the', 'model', 'achiev', '95', '%', 'accuraci', 'test', 'set', '!']\n",
            "['nlp', 'task', 'includ', 'sentiment', 'analysi', ',', 'name', 'entiti', 'recognit', ',', '.']\n",
            "['dl', 'framework', '(', 'e.g.', ',', 'tensorflow', ',', 'pytorch', ')', 'wide', 'use', 'ai', 'research', '.']\n",
            "Vocabulary length 120\n",
            "Vocabulary items [('!', 0), ('%', 1), (\"'m\", 2), (\"'s\", 3), ('(', 4), (')', 5), (',', 6), ('-', 7), ('.', 8), ('15-17', 9), ('2023', 10), ('95', 11), (':', 12), ('?', 13), ('acceler', 14), ('accuraci', 15), ('achiev', 16), ('across', 17), ('adapt', 18), ('advanc', 19), ('ai', 20), ('algorithm', 21), ('analysi', 22), ('attent', 23), ('bay', 24), ('bert', 25), ('breakthrough', 26), ('captur', 27), ('categor', 28), ('classif', 29), ('compani', 30), ('comput', 31), ('confer', 32), ('contextu', 33), ('cover', 34), ('crucial', 35), ('custom', 36), ('data', 37), ('deep', 38), ('detect', 39), ('dl', 40), ('e.g.', 41), ('effect', 42), ('effici', 43), ('embed', 44), ('enabl', 45), ('entiti', 46), ('ethic', 47), ('excit', 48), ('explain', 49), ('feedback', 50), ('field', 51), ('find', 52), ('framework', 53), ('gener', 54), ('globe', 55), ('good', 56), ('gpu', 57), ('healthcar', 58), ('i', 59), ('imag', 60), ('improv', 61), ('includ', 62), ('industri', 63), ('inform', 64), ('introduc', 65), ('k-nn', 66), ('label', 67), ('learn', 68), ('mani', 69), ('mechan', 70), ('ml', 71), ('model', 72), ('naiv', 73), ('name', 74), ('network', 75), ('neural', 76), ('new', 77), ('nlp', 78), ('novel', 79), ('outperform', 80), ('overlap', 81), ('paper', 82), ('pattern', 83), ('pre-train', 84), ('preprocess', 85), ('process', 86), ('pytorch', 87), ('recognit', 88), ('research', 89), ('revolution', 90), ('rich', 91), ('right', 92), ('rnn', 93), ('schedul', 94), ('scienc', 95), ('sentenc', 96), ('sentiment', 97), ('sep.', 98), ('sequenc', 99), ('sequenti', 100), ('set', 101), ('spam', 102), ('studi', 103), ('svm', 104), ('task', 105), ('techniqu', 106), ('tensorflow', 107), ('test', 108), ('text', 109), ('the', 110), ('time', 111), ('topic', 112), ('train', 113), ('unsupervis', 114), ('use', 115), ('way', 116), ('wide', 117), ('without', 118), ('workshop', 119)]\n",
            "N-Hot Vectorization:\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "Find top similar sentences w.r.t a given query\n",
            "Query [['algorithm']]\n",
            "Query vector [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Printing top 5 most similar text for query [['algorithm']]\n",
            "RNNs are good at processing sequential data. Score:  2.6457513110645907\n",
            "Data preprocessing is crucial for effective NLP. Score:  2.6457513110645907\n",
            "ML algorithms can adapt to new data and improve over time. Score:  2.6457513110645907\n",
            "AI is revolutionizing industries across the globe. Score:  2.6457513110645907\n",
            "DL techniques have enabled breakthroughs in image generation. Score:  2.8284271247461903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Vector based search 6.0: Exploring Lemmatization\n",
        "\n",
        "- Lemmatization is different from stemming in a way that it helps extract the root form of a word and the root form is linguistically valid. (e.g., \"went\" => \"go\").\n",
        "\n",
        "- In many applicaions (e.g., text-classification), stemming is not a good idea as it loses text semantics. We can resort to lemmatization in such cases."
      ],
      "metadata": {
        "id": "xoW1satdjwaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "corpus = [\n",
        "    \"AI, ML, and NLP are fields of study in computer science.\",\n",
        "    \"Data preprocessing is crucial for effective NLP.\",\n",
        "    \"The model achieved 95% accuracy on the test set!\",\n",
        "    \"NLP tasks include sentiment analysis, named entity recognition, and more.\",\n",
        "    \"DL frameworks (e.g., TensorFlow, PyTorch) are widely used in AI research.\",\n",
        "    \"The conference is scheduled for Sep. 15-17, 2023.\",\n",
        "    \"BERT's pre-trained embeddings capture rich contextual information.\",\n",
        "    \"The algorithm outperformed SVM, k-NN, and Naive Bayes.\",\n",
        "    \"Neural networks can process sequences (e.g., sentences) efficiently.\",\n",
        "    \"I'm excited about AI advancements in healthcare.\",\n",
        "    \"RNNs are good at processing sequential data.\",\n",
        "    \"Unsupervised learning finds patterns without labeled data, right?\",\n",
        "    \"GPU acceleration is crucial for training deep learning models.\",\n",
        "    \"The paper introduced a novel attention mechanism.\",\n",
        "    \"Text classification: spam detection, topic categorization, sentiment analysis.\",\n",
        "    \"AI is revolutionizing industries across the globe.\",\n",
        "    \"ML algorithms can adapt to new data and improve over time.\",\n",
        "    \"The company uses NLP for customer feedback analysis.\",\n",
        "    \"DL techniques have enabled breakthroughs in image generation.\",\n",
        "    \"AI, ML, NLP - these fields overlap in many ways.\",\n",
        "    \"The workshop will cover AI ethics, explainable ML, and more.\"\n",
        "]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# tokenize using NLTK punct tokenizer for English\n",
        "processed_corpus = [word_tokenize(sentence) for sentence in corpus]\n",
        "\n",
        "# Lower case the processed corpus\n",
        "processed_corpus_lc_filtered = []\n",
        "for tokenized_sent in processed_corpus:\n",
        "  processed_corpus_lc_filtered.append([word.lower() for word in tokenized_sent if word not in stop_words])\n",
        "\n",
        "\n",
        "processed_corpus_lemmatized = []\n",
        "for tokenized_sent in processed_corpus_lc_filtered:\n",
        "  processed_corpus_lemmatized.append([lemmatizer.lemmatize(word) for word in tokenized_sent])\n",
        "\n",
        "print (\"Printing a few tokenized sentences\")\n",
        "for tokenized_sentence in processed_corpus_lemmatized[:5]:\n",
        "  print (tokenized_sentence)\n",
        "\n",
        "# Define the N for N-grams\n",
        "N = 1  # Change this to the desired value for N-grams\n",
        "\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False,binary = True, analyzer=lambda x:x)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(processed_corpus_lemmatized)\n",
        "n_hot_matrix = vectorizer.transform(processed_corpus_lemmatized)\n",
        "\n",
        "# Print the length of vocabulary (unique words)\n",
        "vocabulary_dict = vectorizer.vocabulary_\n",
        "print(f\"Vocabulary length {len(vocabulary_dict)}\")\n",
        "\n",
        "# Print the vocabulary items\n",
        "print(f\"Vocabulary items {sorted(vocabulary_dict.items(),key=lambda x:x[0])}\")\n",
        "\n",
        "# Convert the matrix to an array and print the result for the first 5 instances\n",
        "print(\"N-Hot Vectorization:\")\n",
        "for vec in n_hot_matrix.toarray().tolist()[:5]:\n",
        "  print (vec)\n",
        "\n",
        "print(\"Find top similar sentences w.r.t a given query\")\n",
        "query = \"algorithm\"\n",
        "query = [[query.lower()]]\n",
        "print (\"Query\",query)\n",
        "\n",
        "query_vector = vectorizer.transform(query).toarray()\n",
        "print (\"Query vector\", query_vector)\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "distances = pairwise_distances(query_vector, n_hot_matrix.toarray(), metric='euclidean')[0]\n",
        "\n",
        "#sort indices in ascending order\n",
        "sorted_indices = np.argsort(distances)\n",
        "\n",
        "# print top 5 most similar queries\n",
        "print (f\"Printing top 5 most similar text for query {query}\")\n",
        "for q in sorted_indices[:5]:\n",
        "  print(corpus[q], \"Score: \", distances[q])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiijA3QNjvW7",
        "outputId": "8d69a9f0-fba8-4677-d749-d9f7854a8cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing a few tokenized sentences\n",
            "['ai', ',', 'ml', ',', 'nlp', 'field', 'study', 'computer', 'science', '.']\n",
            "['data', 'preprocessing', 'crucial', 'effective', 'nlp', '.']\n",
            "['the', 'model', 'achieved', '95', '%', 'accuracy', 'test', 'set', '!']\n",
            "['nlp', 'task', 'include', 'sentiment', 'analysis', ',', 'named', 'entity', 'recognition', ',', '.']\n",
            "['dl', 'framework', '(', 'e.g.', ',', 'tensorflow', ',', 'pytorch', ')', 'widely', 'used', 'ai', 'research', '.']\n",
            "Vocabulary length 122\n",
            "Vocabulary items [('!', 0), ('%', 1), (\"'m\", 2), (\"'s\", 3), ('(', 4), (')', 5), (',', 6), ('-', 7), ('.', 8), ('15-17', 9), ('2023', 10), ('95', 11), (':', 12), ('?', 13), ('acceleration', 14), ('accuracy', 15), ('achieved', 16), ('across', 17), ('adapt', 18), ('advancement', 19), ('ai', 20), ('algorithm', 21), ('analysis', 22), ('attention', 23), ('bayes', 24), ('bert', 25), ('breakthrough', 26), ('capture', 27), ('categorization', 28), ('classification', 29), ('company', 30), ('computer', 31), ('conference', 32), ('contextual', 33), ('cover', 34), ('crucial', 35), ('customer', 36), ('data', 37), ('deep', 38), ('detection', 39), ('dl', 40), ('e.g.', 41), ('effective', 42), ('efficiently', 43), ('embeddings', 44), ('enabled', 45), ('entity', 46), ('ethic', 47), ('excited', 48), ('explainable', 49), ('feedback', 50), ('field', 51), ('find', 52), ('framework', 53), ('generation', 54), ('globe', 55), ('good', 56), ('gpu', 57), ('healthcare', 58), ('i', 59), ('image', 60), ('improve', 61), ('include', 62), ('industry', 63), ('information', 64), ('introduced', 65), ('k-nn', 66), ('labeled', 67), ('learning', 68), ('many', 69), ('mechanism', 70), ('ml', 71), ('model', 72), ('naive', 73), ('named', 74), ('network', 75), ('neural', 76), ('new', 77), ('nlp', 78), ('novel', 79), ('outperformed', 80), ('overlap', 81), ('paper', 82), ('pattern', 83), ('pre-trained', 84), ('preprocessing', 85), ('process', 86), ('processing', 87), ('pytorch', 88), ('recognition', 89), ('research', 90), ('revolutionizing', 91), ('rich', 92), ('right', 93), ('rnns', 94), ('scheduled', 95), ('science', 96), ('sentence', 97), ('sentiment', 98), ('sep.', 99), ('sequence', 100), ('sequential', 101), ('set', 102), ('spam', 103), ('study', 104), ('svm', 105), ('task', 106), ('technique', 107), ('tensorflow', 108), ('test', 109), ('text', 110), ('the', 111), ('time', 112), ('topic', 113), ('training', 114), ('unsupervised', 115), ('us', 116), ('used', 117), ('way', 118), ('widely', 119), ('without', 120), ('workshop', 121)]\n",
            "N-Hot Vectorization:\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "Find top similar sentences w.r.t a given query\n",
            "Query [['algorithm']]\n",
            "Query vector [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Printing top 5 most similar text for query [['algorithm']]\n",
            "RNNs are good at processing sequential data. Score:  2.6457513110645907\n",
            "Data preprocessing is crucial for effective NLP. Score:  2.6457513110645907\n",
            "ML algorithms can adapt to new data and improve over time. Score:  2.6457513110645907\n",
            "AI is revolutionizing industries across the globe. Score:  2.6457513110645907\n",
            "DL techniques have enabled breakthroughs in image generation. Score:  2.8284271247461903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Vector based search 7.0: Extracting more meaningful vectors using existing word embeddings: Performing semantic search\n",
        "\n",
        "- In all the above implementations, we always convert words into a number and sentences into an N-hot representation.\n",
        "\n",
        "- This does not effectively capture relationships between words and phrases.\n",
        "\n",
        "- In principle, words should be \"known by the company they keep\". For example, the word \"cat\" should be related to \"dog\" more than \"Wednesday\".\n",
        "\n",
        "- We thus vectorize corpus and queries using word embeddings, i.e., representations that capture the semantic association between words\n",
        "\n",
        "- Vectorization using word embeddings allow us to perform semantic search\n",
        "\n",
        "- We will use glove embeddings (http://nlp.stanford.edu/data/glove.6B.zip) as our source of pre-trained word embeddings/"
      ],
      "metadata": {
        "id": "Uc1dU6qdiyLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a one time download\n",
        "!wget -c http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "# do some necessary conversions\n",
        "!python -m gensim.scripts.glove2word2vec --input  glove.6B.50d.txt --output glove.6B.50d.vec\n",
        "!python -m gensim.scripts.glove2word2vec --input  glove.6B.200d.txt --output glove.6B.200d.vec\n",
        "!rm glove*.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhga_MKysZ9P",
        "outputId": "02c72e7b-22dc-41d5-e3ba-6a87fe4c8763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-28 19:34:42--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-08-28 19:34:42--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-08-28 19:34:42--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: glove.6B.zip\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2023-08-28 19:37:22 (5.16 MB/s) - glove.6B.zip saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "2023-08-28 19:37:51,309 - glove2word2vec - INFO - running /usr/local/lib/python3.10/dist-packages/gensim/scripts/glove2word2vec.py --input glove.6B.50d.txt --output glove.6B.50d.vec\n",
            "/usr/local/lib/python3.10/dist-packages/gensim/scripts/glove2word2vec.py:125: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  num_lines, num_dims = glove2word2vec(args.input, args.output)\n",
            "2023-08-28 19:37:51,311 - keyedvectors - INFO - loading projection weights from glove.6B.50d.txt\n",
            "2023-08-28 19:38:08,063 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (400000, 50) matrix of type float32 from glove.6B.50d.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2023-08-28T19:38:08.061803', 'gensim': '4.3.1', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'load_word2vec_format'}\n",
            "2023-08-28 19:38:08,063 - glove2word2vec - INFO - converting 400000 vectors from glove.6B.50d.txt to glove.6B.50d.vec\n",
            "2023-08-28 19:38:08,391 - keyedvectors - INFO - storing 400000x50 projection weights into glove.6B.50d.vec\n",
            "2023-08-28 19:38:21,096 - glove2word2vec - INFO - Converted model with 400000 vectors and 50 dimensions\n",
            "2023-08-28 19:38:22,364 - glove2word2vec - INFO - running /usr/local/lib/python3.10/dist-packages/gensim/scripts/glove2word2vec.py --input glove.6B.200d.txt --output glove.6B.200d.vec\n",
            "/usr/local/lib/python3.10/dist-packages/gensim/scripts/glove2word2vec.py:125: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  num_lines, num_dims = glove2word2vec(args.input, args.output)\n",
            "2023-08-28 19:38:22,364 - keyedvectors - INFO - loading projection weights from glove.6B.200d.txt\n",
            "2023-08-28 19:39:23,913 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (400000, 200) matrix of type float32 from glove.6B.200d.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2023-08-28T19:39:23.911040', 'gensim': '4.3.1', 'python': '3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]', 'platform': 'Linux-5.15.109+-x86_64-with-glibc2.35', 'event': 'load_word2vec_format'}\n",
            "2023-08-28 19:39:23,913 - glove2word2vec - INFO - converting 400000 vectors from glove.6B.200d.txt to glove.6B.200d.vec\n",
            "2023-08-28 19:39:24,306 - keyedvectors - INFO - storing 400000x200 projection weights into glove.6B.200d.vec\n",
            "2023-08-28 19:40:15,342 - glove2word2vec - INFO - Converted model with 400000 vectors and 200 dimensions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# Load GloVe word vectors\n",
        "glove_path = 'glove.6B.50d.vec'  # Change this to your actual path\n",
        "word_vectors = KeyedVectors.load_word2vec_format(glove_path, binary=False)\n",
        "\n",
        "corpus = [\n",
        "    \"AI, ML, and NLP are fields of study in computer science.\",\n",
        "    \"Data preprocessing is crucial for effective NLP.\",\n",
        "    \"The model achieved 95% accuracy on the test set!\",\n",
        "    \"NLP tasks include sentiment analysis, named entity recognition, and more.\",\n",
        "    \"DL frameworks (e.g., TensorFlow, PyTorch) are widely used in AI research.\",\n",
        "    \"The conference is scheduled for Sep. 15-17, 2023.\",\n",
        "    \"BERT's pre-trained embeddings capture rich contextual information.\",\n",
        "    \"The algorithm outperformed SVM, k-NN, and Naive Bayes.\",\n",
        "    \"Neural networks can process sequences (e.g., sentences) efficiently.\",\n",
        "    \"I'm excited about AI advancements in healthcare.\",\n",
        "    \"RNNs are good at processing sequential data.\",\n",
        "    \"Unsupervised learning finds patterns without labeled data, right?\",\n",
        "    \"GPU acceleration is crucial for training deep learning models.\",\n",
        "    \"The paper introduced a novel attention mechanism.\",\n",
        "    \"Text classification: spam detection, topic categorization, sentiment analysis.\",\n",
        "    \"AI is revolutionizing industries across the globe.\",\n",
        "    \"ML algorithms can adapt to new data and improve over time.\",\n",
        "    \"The company uses NLP for customer feedback analysis.\",\n",
        "    \"DL techniques have enabled breakthroughs in image generation.\",\n",
        "    \"AI, ML, NLP - these fields overlap in many ways.\",\n",
        "    \"The workshop will cover AI ethics, explainable ML, and more.\"\n",
        "]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# tokenize using NLTK punct tokenizer for English\n",
        "processed_corpus = [word_tokenize(sentence) for sentence in corpus]\n",
        "\n",
        "# Lower case the processed corpus\n",
        "processed_corpus_lc_filtered = []\n",
        "for tokenized_sent in processed_corpus:\n",
        "  processed_corpus_lc_filtered.append([word.lower() for word in tokenized_sent if word not in stop_words])\n",
        "\n",
        "\n",
        "processed_corpus_lemmatized = []\n",
        "for tokenized_sent in processed_corpus_lc_filtered:\n",
        "  processed_corpus_lemmatized.append([lemmatizer.lemmatize(word) for word in tokenized_sent])\n",
        "\n",
        "def vectorize_corpus(corpus, word_vectors):\n",
        "    vectorized_corpus = []\n",
        "    for sentence in corpus:\n",
        "        vectorized_sentence = np.mean([word_vectors.get_vector(token) for token in sentence if token in word_vectors.key_to_index], axis=0)\n",
        "        vectorized_corpus.append(vectorized_sentence)\n",
        "    return vectorized_corpus\n",
        "\n",
        "def compute_distance_scores(query_vector, vector_set):\n",
        "    distances = euclidean_distances([query_vector], vector_set)[0]\n",
        "    sorted_indices = np.argsort(distances)\n",
        "    sorted_distances = [distances[i] for i in sorted_indices]\n",
        "    return sorted_distances, sorted_indices\n",
        "\n",
        "vectorized_corpus = vectorize_corpus(processed_corpus_lemmatized, word_vectors)\n",
        "\n",
        "print(\"Find top similar sentences w.r.t a given query\")\n",
        "query = \"algorithm\"\n",
        "query = [query.lower()]\n",
        "print (\"Query\",query)\n",
        "query_vector = np.mean([word_vectors.get_vector(token) for token in query if token in word_vectors.key_to_index], axis=0)\n",
        "\n",
        "print(\"Query vector\", query_vector)\n",
        "\n",
        "sorted_distances, sorted_indices = compute_distance_scores(query_vector, vectorized_corpus)\n",
        "\n",
        "for i, q in enumerate(sorted_indices[:5]):\n",
        "  print(corpus[q], \"Score: \", sorted_distances[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "Yoc61BjimDJO",
        "outputId": "d82881eb-a44e-4925-dd11-9edac94bd58a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-05ba85b72681>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load GloVe word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mglove_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove.6B.50d.vec'\u001b[0m  \u001b[0;31m# Change this to your actual path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m corpus = [\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.50d.vec'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise E1: Error analysis and mitigation for versions 1-5 of the search\n",
        "\n",
        "- Investigate why the algorithm omitted certain sentences from the top-five list, despite a precise match between the query and the words within those sentences.\n",
        "\n",
        "- Could you suggest a potential remedy to address this problem? (Hint: normalizing the distance function using the sentence's token count in the corpus could be a viable solution.)\n",
        "\n",
        "- Implement the solution for ONLY \"Vector based search 5.0\"\n",
        "\n",
        "## Exercise E2: Compare `glove.6B.50d.vec` and `glove.6B.200d.vec`\n",
        "\n",
        "- Reimplement section 7 but this time with `glove.6B.200d.vec`. This is a richer word embedding representation with each word represented as a 200d vector (as opposed to 50d in the previous case).\n",
        "\n",
        "- Try out different queries and see if you are getting better results.\n",
        "\n",
        "- Explain your observations in a markdown block."
      ],
      "metadata": {
        "id": "o-HYH_wxqA0u"
      }
    }
  ]
}