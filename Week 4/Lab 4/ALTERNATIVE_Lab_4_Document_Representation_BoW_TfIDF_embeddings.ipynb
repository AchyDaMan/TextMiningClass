{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4: Document Representation and Similariy Measurement\n",
        "\n",
        "Note: This lab session is graded. Complete all exercises and submit it under **Canvas->Lab4** (https://utexas.instructure.com/courses/1382133/assignments/6619548) by no later than **02/08/2023, 11:59PM**. Please attempt all exercises.\n",
        "\n",
        "For extracting representations from text, we will be using the following libraries:\n",
        "\n",
        "1. SpaCy for text pre-processing (tokenization, lemmatization, stopword removal)\n",
        "2. Scikit learn's vectorizer's\n",
        "3. Gensim's word vector's\n",
        "\n",
        "\n",
        "References:\n",
        "1. [ SpaCY ]  https://spacy.io/usage/processing-pipelines\n",
        "2. [ Gensim ] https://radimrehurek.com/gensim/auto_examples/index.html\n",
        "3. [ Scikit ] https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ],
      "metadata": {
        "id": "iYmt3yulr3p3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Representing Documents with Bag-of-words approach\n",
        "\n",
        "We are interested in featurizing (or vectorizing) a given text corpus so that we can compute document similarity or perform similariy based search.\n",
        "\n",
        "For example: for the given query, `query  = \"Context is captured better through deep learning based text encoders\"`, we want to identify, which documents in the text corpus are most similary to the query, and possibly rank them in the order of similarity."
      ],
      "metadata": {
        "id": "YVYnMTIosgfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    # Technology\n",
        "    \"The latest smartphone model boasts a revolutionary camera system that enhances low-light photography.\",\n",
        "    \"Artificial intelligence algorithms are reshaping industries by automating routine tasks and streamlining operations.\",\n",
        "    \"Quantum computing holds the promise of solving complex problems exponentially faster than classical computers.\",\n",
        "\n",
        "    # Environment\n",
        "    \"Deforestation in the Amazon rainforest continues to pose a significant threat to biodiversity and indigenous communities.\",\n",
        "    \"Renewable energy sources such as solar and wind power are crucial for reducing carbon emissions and combating climate change.\",\n",
        "    \"Plastic pollution in oceans is a pressing environmental issue, with millions of marine animals suffering from ingestion or entanglement.\",\n",
        "\n",
        "    # Health\n",
        "    \"Vaccination campaigns are essential for preventing the spread of infectious diseases and achieving herd immunity.\",\n",
        "    \"Mental health awareness initiatives aim to reduce stigma and promote access to support services for individuals struggling with psychological disorders.\",\n",
        "    \"Regular exercise and a balanced diet are key components of maintaining a healthy lifestyle and preventing chronic illnesses like heart disease and diabetes.\"\n",
        "]\n",
        "\n",
        "query = \"the latest advancements in artificial intelligence for image recognition\""
      ],
      "metadata": {
        "id": "p9QrRBj1sf89"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Apply Pre-processing Techniques\n",
        "\n",
        "Understanding the benefits of employing text pre-processing techniques is crucial in mitigating data sparsity concerns and reducing the necessity for an extensive vocabulary. We will proceed by directly implementing pre-processing on both the corpus and the query. However, for those who are curious, we offer the option to disable pre-processing to evaluate whether its application was indeed advantageous.\n",
        "\n",
        "We will apply the following basic preprocessing steps:\n",
        "\n",
        "1. Lowercasing\n",
        "2. Tokenization\n",
        "2. Lemmatization\n",
        "3. Stopword removal\n"
      ],
      "metadata": {
        "id": "9WG4XzNmMQqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# get a list of stopwords from NLTK\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "# Load SpaCy English language model\n",
        "# this is apipeline capable of applying morphological, lexical and syntax analysis on text\n",
        "\n",
        "nlp_pipeline = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pre_process_a_single_sentence(sentence):\n",
        "  # Lower case text\n",
        "  sentence = sentence.lower()\n",
        "\n",
        "  processed_sentence = []\n",
        "\n",
        "  # Tokenize, and lemmatize the text\n",
        "  doc = nlp_pipeline(sentence)\n",
        "\n",
        "  for token in doc:\n",
        "    # here token is an object that contains various information about each token\n",
        "    # information such as lemma, pos, parse labels are available\n",
        "\n",
        "    # we will check here if tokens are present in stopwords\n",
        "    # if not, we will retain their lemma\n",
        "    if token not in stops:\n",
        "      lemmatized_token = token.lemma_\n",
        "      processed_sentence.append(lemmatized_token)\n",
        "  processed_sentence = \" \".join (processed_sentence)\n",
        "  return processed_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkZQDWVXMJ0y",
        "outputId": "4cf4b146-5e3d-42eb-bb82-27bb4f7d11fb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, apply the preprocessing steps on the corpus as well as the queries"
      ],
      "metadata": {
        "id": "LzXJ5BXWb5DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_processed_corpus = [pre_process_a_single_sentence(sentence) for sentence in corpus]\n",
        "\n",
        "pre_processed_query = pre_process_a_single_sentence(query)\n",
        "\n",
        "# sanity check: print 5 processed documents\n",
        "\n",
        "print (pre_processed_corpus[:5])\n",
        "print (pre_processed_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Z6ZD9scNIv",
        "outputId": "592d24b7-d5df-4387-c3a7-fdd3bb79a841"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the late smartphone model boast a revolutionary camera system that enhance low - light photography .', 'artificial intelligence algorithm be reshape industry by automate routine task and streamline operation .', 'quantum computing hold the promise of solve complex problem exponentially fast than classical computer .', 'deforestation in the amazon rainforest continue to pose a significant threat to biodiversity and indigenous community .', 'renewable energy source such as solar and wind power be crucial for reduce carbon emission and combat climate change .']\n",
            "the late advancement in artificial intelligence for image recognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Extract Bag-of-words (BoW) representations / vectors based on the corpus\n",
        "\n",
        "We construct the vocabulary using `fit()` method applied on the corpus and then used the vocabulary to construct a BoW presence / absence vector."
      ],
      "metadata": {
        "id": "LMqaD_BVc9EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define the N for N-grams\n",
        "N = 1  # Change this to the desired value for N-grams\n",
        "\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False, binary = True)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(pre_processed_corpus)\n",
        "\n",
        "# Check a few items in the vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "# check the vocabulary size\n",
        "print (len(vocab))\n",
        "\n",
        "# sanity check: check the list of vocabulary\n",
        "print (vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUSRCTdrdsr_",
        "outputId": "056d3707-03cd-406f-d346-38a6a81b6e42"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n",
            "['access' 'achieve' 'aim' 'algorithm' 'amazon' 'and' 'animal' 'artificial'\n",
            " 'as' 'automate' 'awareness' 'balanced' 'be' 'biodiversity' 'boast' 'by'\n",
            " 'camera' 'campaign' 'carbon' 'change' 'chronic' 'classical' 'climate'\n",
            " 'combat' 'community' 'complex' 'component' 'computer' 'computing'\n",
            " 'continue' 'crucial' 'deforestation' 'diabete' 'diet' 'disease'\n",
            " 'disorder' 'emission' 'energy' 'enhance' 'entanglement' 'environmental'\n",
            " 'essential' 'exercise' 'exponentially' 'fast' 'for' 'from' 'health'\n",
            " 'healthy' 'heart' 'herd' 'hold' 'illness' 'immunity' 'in' 'indigenous'\n",
            " 'individual' 'industry' 'infectious' 'ingestion' 'initiative'\n",
            " 'intelligence' 'issue' 'key' 'late' 'lifestyle' 'light' 'like' 'low'\n",
            " 'maintain' 'marine' 'mental' 'million' 'model' 'ocean' 'of' 'operation'\n",
            " 'or' 'photography' 'plastic' 'pollution' 'pose' 'power' 'press' 'prevent'\n",
            " 'problem' 'promise' 'promote' 'psychological' 'quantum' 'rainforest'\n",
            " 'reduce' 'regular' 'renewable' 'reshape' 'revolutionary' 'routine'\n",
            " 'service' 'significant' 'smartphone' 'solar' 'solve' 'source' 'spread'\n",
            " 'stigma' 'streamline' 'struggle' 'such' 'suffer' 'support' 'system'\n",
            " 'task' 'than' 'that' 'the' 'threat' 'to' 'vaccination' 'wind' 'with']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the vocabulary is formed, we now transform ANY text using `transform` function."
      ],
      "metadata": {
        "id": "HvGtRbhVefdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_transformed_corpus = []\n",
        "\n",
        "for sentence in pre_processed_corpus:\n",
        "  transformed_vector = vectorizer.transform([sentence])\n",
        "  bow_transformed_corpus.append(transformed_vector.toarray()[0])\n",
        "\n",
        "bow_transformed_query = vectorizer.transform([pre_processed_query]).toarray()[0]\n",
        "\n",
        "# sanity check : print a few items from the bow_transformed_corpus and bow_transformed_query\n",
        "print (\"Transformed Corpus Samples\", bow_transformed_corpus[:5])\n",
        "print (\"Transformed Query\", bow_transformed_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es-i8jXeeuUN",
        "outputId": "935a96c3-d883-4818-ff82-2f55757e9953"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed Corpus Samples [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "       1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0]), array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
            "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0]), array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
            "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0])]\n",
            "Transformed Query [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can match the query vector with the corpus vectors in a pairwise manner to see which ones are most similar to the query vector."
      ],
      "metadata": {
        "id": "Z1-fAGTZgYbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define a similarity function.\n",
        "# we take 1 / euclidean_distance between two vectors as the similarity\n",
        "\n",
        "import numpy as np\n",
        "def euclidean_distance_based_similarity (vector1, vector2):\n",
        "    \"\"\"\n",
        "    Compute the Euclidean distance between two vectors.\n",
        "\n",
        "    Parameters:\n",
        "        vector1 (array-like): First vector.\n",
        "        vector2 (array-like): Second vector.\n",
        "\n",
        "    Returns:\n",
        "        float: Euclidean distance between the two vectors.\n",
        "    \"\"\"\n",
        "    return 1 / (np.linalg.norm(np.array(vector1) - np.array(vector2)))\n",
        "\n",
        "\n",
        "similarity_scores = {}\n",
        "\n",
        "for i, document_vector in enumerate(bow_transformed_corpus):\n",
        "  sim = euclidean_distance_based_similarity(document_vector, bow_transformed_query)\n",
        "  similarity_scores[i] = sim\n",
        "\n",
        "ranked_documents = sorted(similarity_scores.items(),key = lambda x: x[1] ,reverse = True)\n",
        "\n",
        "# Let's print the top 5 documents based on ranked score\n",
        "\n",
        "print (f\"Query: {query}\")\n",
        "for document_idx, score in ranked_documents[:5]:\n",
        "  print (f\"Document: {corpus[document_idx]}, Score: {score}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fhIJrKVXgCfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9700c94-cb8e-4777-f966-0fd0cd7fe7c6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: the latest advancements in artificial intelligence for image recognition\n",
            "Document: The latest smartphone model boasts a revolutionary camera system that enhances low-light photography., Score: 0.2581988897471611\n",
            "Document: Artificial intelligence algorithms are reshaping industries by automating routine tasks and streamlining operations., Score: 0.2581988897471611\n",
            "Document: Deforestation in the Amazon rainforest continues to pose a significant threat to biodiversity and indigenous communities., Score: 0.25\n",
            "Document: Vaccination campaigns are essential for preventing the spread of infectious diseases and achieving herd immunity., Score: 0.24253562503633297\n",
            "Document: Quantum computing holds the promise of solving complex problems exponentially faster than classical computers., Score: 0.23570226039551587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise E1. Repeat the above steps using BoW-count method:\n",
        "\n",
        "Hint: Use `binary = False`\n",
        "\n",
        "`vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False, binary = False)`\n",
        "\n",
        "### Exercise E2: Repeat the above steps using TF-IDF method:\n",
        "\n",
        "Hint: Use `TfidfVectorizer` instead of CountVectorizer\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\n",
        "Try different queries and explain your observations"
      ],
      "metadata": {
        "id": "_g2RefzjkDO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "APEr1oX3mqZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Extract Word Vectors based on GloVe and compute similarity between query and documents\n",
        "\n",
        "- In all the above implementations, we always convert words into a number and sentences into an N-hot representation.\n",
        "\n",
        "- This does not effectively capture relationships between words and phrases.\n",
        "\n",
        "- In principle, words should be \"known by the company they keep\". For example, the word \"cat\" should be related to \"dog\" more than \"Wednesday\".\n",
        "\n",
        "- We thus vectorize corpus and queries using word embeddings, i.e., representations that capture the semantic association between words\n",
        "\n",
        "- Vectorization using word embeddings allow us to perform semantic search\n",
        "\n",
        "- We will use glove embeddings (http://nlp.stanford.edu/data/glove.6B.zip) as our source of pre-trained word embeddings/\n"
      ],
      "metadata": {
        "id": "RP7RRnSdi7bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# Function to generate average word vectors for a sentence\n",
        "def average_word_embeddings(sentence):\n",
        "    words = sentence.split()\n",
        "    embeddings = []\n",
        "    for word in words:\n",
        "        if word in word_vectors:\n",
        "            embeddings.append(word_vectors[word])\n",
        "    if len(embeddings) > 0:\n",
        "        # is word vector exists for the word\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word_vectors.vector_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "A4FkkF3pjXrJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector_transformed_corpus = []\n",
        "\n",
        "for sentence in pre_processed_corpus:\n",
        "  transformed_vector = average_word_embeddings(sentence)\n",
        "  word_vector_transformed_corpus.append(transformed_vector)\n",
        "\n",
        "word_vector_transformed_query = average_word_embeddings(pre_processed_query)\n",
        "\n",
        "# sanity check : print a few items from the bow_transformed_corpus and bow_transformed_query\n",
        "print (\"Word Vector Transformed Corpus Samples\", word_vector_transformed_corpus[:5])\n",
        "print (\"Word Vector Transformed Query\", word_vector_transformed_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4XH-7tZjtIa",
        "outputId": "3e14f00e-5427-4235-ccef-65c4e176e828"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Vector Transformed Corpus Samples [array([ 0.1861708 ,  0.23128964,  0.1962533 ,  0.07322185,  0.09015149,\n",
            "       -0.01049213, -0.29616618, -0.6643501 ,  0.0113958 ,  0.25757927,\n",
            "        0.2509965 , -0.06853063, -0.17922047,  0.22037098, -0.10514338,\n",
            "        0.11703007, -0.368467  ,  0.1734872 , -0.31144908, -0.42928684,\n",
            "        0.09016962,  0.04219813, -0.21167749, -0.18209696,  0.02590456,\n",
            "       -1.3121846 , -0.4729219 ,  0.05249795,  0.16609696,  0.12275426,\n",
            "        2.8859878 , -0.0463383 , -0.27610356, -0.39248237,  0.10850608,\n",
            "        0.02044093, -0.02673562,  0.31013373, -0.21586457, -0.22675025,\n",
            "        0.13021228,  0.16225356, -0.03704819,  0.01487388,  0.04129475,\n",
            "       -0.01695319,  0.19756524, -0.21693762,  0.06474212, -0.01510743],\n",
            "      dtype=float32), array([ 6.65031433e-01, -5.51267922e-01, -7.34305009e-02,  1.08917437e-01,\n",
            "       -9.01242867e-02,  1.31470293e-01, -5.51946349e-02, -3.14387828e-01,\n",
            "        4.34026450e-01, -1.80350065e-01,  1.47496939e-01,  2.39261106e-01,\n",
            "       -3.54352951e-01,  2.21403003e-01, -3.89457084e-02,  1.47164658e-01,\n",
            "        8.39814451e-03,  2.06601452e-02,  2.78091401e-01, -3.30505580e-01,\n",
            "        3.66178542e-01, -1.66800961e-01, -5.20542562e-02, -2.13722214e-01,\n",
            "        5.24764173e-02, -9.82092202e-01, -3.60993356e-01, -1.92898944e-01,\n",
            "        2.84022152e-01, -2.47391500e-02,  2.54966164e+00, -1.02038108e-01,\n",
            "       -3.87977153e-01, -5.35810828e-01, -9.07605067e-02,  3.81620646e-01,\n",
            "       -6.41754344e-02,  4.71663922e-01, -5.65256737e-02,  7.00580771e-04,\n",
            "       -1.87875628e-01, -1.01243861e-01,  1.00440852e-01,  7.89626986e-02,\n",
            "       -2.16670688e-02, -1.18815266e-01,  3.01968098e-01,  2.44032159e-01,\n",
            "       -3.95721849e-03,  2.83943564e-01], dtype=float32), array([ 3.9172047e-01,  1.2900580e-01,  2.0420581e-01,  1.5327910e-01,\n",
            "        2.4062508e-01,  1.1633820e-01,  2.8576596e-02, -5.0333136e-01,\n",
            "       -4.0251337e-02,  3.1612000e-01,  1.6361172e-01,  4.7533662e-04,\n",
            "       -2.9986697e-01,  4.3444864e-02,  2.3199336e-01,  2.8060266e-01,\n",
            "        8.5461006e-02,  2.9715979e-01, -6.1993320e-03, -4.6446151e-01,\n",
            "        1.8231973e-01, -1.5975301e-01, -1.2052727e-01,  3.1586342e-02,\n",
            "        3.5787657e-01, -1.0669286e+00, -4.1094869e-01, -2.6214767e-01,\n",
            "        2.5093552e-01,  1.2832053e-01,  2.9472032e+00, -1.1105145e-01,\n",
            "       -1.5928563e-01, -5.7094949e-01,  1.2716478e-02, -3.4318723e-02,\n",
            "       -3.0151600e-01,  3.1469259e-01,  9.1245919e-02,  1.3608252e-01,\n",
            "       -2.0185390e-01, -6.6788331e-02, -8.4780850e-02,  5.6183565e-01,\n",
            "       -1.6099812e-01,  1.5295687e-01,  4.4407013e-01,  1.2917320e-01,\n",
            "       -3.2199258e-01, -5.8066852e-02], dtype=float32), array([ 0.41160825,  0.03198936, -0.17880146,  0.17501259,  0.02401954,\n",
            "        0.0539993 , -0.43018597, -0.3107636 ,  0.51613015,  0.02991601,\n",
            "        0.21159112, -0.091382  ,  0.19406164, -0.37581322,  0.12083244,\n",
            "        0.212719  ,  0.47153506, -0.18733534, -0.24070883, -0.19600448,\n",
            "       -0.17699158, -0.11887617, -0.05364224, -0.1244106 ,  0.12862587,\n",
            "       -1.0799372 , -0.15318091, -0.13824254,  0.17848527,  0.18671258,\n",
            "        2.9894586 , -0.03816568, -0.2927905 , -0.6356198 , -0.16903345,\n",
            "       -0.04134628, -0.34912944, -0.2583366 , -0.052456  , -0.07177824,\n",
            "       -0.39163336, -0.04872801,  0.19073546, -0.05724087,  0.05282818,\n",
            "        0.17250147, -0.16975883,  0.03626582,  0.01699288, -0.3162806 ],\n",
            "      dtype=float32), array([ 3.3166641e-01,  2.9425693e-01,  3.5540617e-01, -1.2295691e-02,\n",
            "       -1.8778530e-01,  2.7292940e-01, -3.9368786e-02, -5.3398043e-01,\n",
            "        2.4718373e-01,  3.5760814e-01,  3.6202496e-01,  1.8906558e-01,\n",
            "        2.5939506e-02,  1.9519775e-01, -2.0751655e-02,  3.7216592e-01,\n",
            "        4.0485111e-01,  1.7786548e-01, -2.5209668e-01, -7.3235881e-01,\n",
            "        2.4850032e-01, -3.5174209e-01,  2.1395782e-01, -4.1440827e-01,\n",
            "        5.2659888e-02, -1.1435591e+00, -6.7158647e-02, -1.3675161e-02,\n",
            "        3.6030388e-01,  4.5823631e-01,  3.1421747e+00,  1.9451322e-01,\n",
            "       -4.4032055e-01, -7.5102878e-01,  1.2578830e-02, -3.6561742e-02,\n",
            "       -1.1146768e-01,  1.6073941e-01,  8.7763593e-02,  1.0231966e-01,\n",
            "       -2.9401755e-01,  4.8506558e-02,  1.9520642e-01, -5.9763748e-02,\n",
            "       -4.4319008e-02,  2.0756450e-01, -9.5205963e-02,  1.9220749e-01,\n",
            "       -2.7397531e-03,  5.0286252e-02], dtype=float32)]\n",
            "Word Vector Transformed Query [ 0.12992312  0.34614065 -0.20598     0.07797755  0.181838    0.14452912\n",
            " -0.3227381  -0.3301632   0.17259555  0.03385688  0.35603043 -0.09890389\n",
            " -0.16480544 -0.01330334 -0.08531885  0.11958823 -0.14276844 -0.09645544\n",
            " -0.2185589   0.02335288  0.08201378  0.16925266 -0.04636272 -0.473748\n",
            " -0.04970878 -1.3831545  -0.52707046 -0.1593099  -0.07167777  0.08884601\n",
            "  2.9674666  -0.01558955 -0.37647232 -0.8620853  -0.08335832  0.25001976\n",
            " -0.06147078  0.30945146 -0.21170244 -0.26154977 -0.05120032 -0.06026081\n",
            " -0.16217512  0.01054322 -0.13702378 -0.03056667  0.02288499  0.16727\n",
            " -0.12842447 -0.3544489 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's use the vectors for computing similarity between queries and documents to see which ones are most similar to the queries.\n",
        "\n",
        "For aimilarity measurement, we will use the same `euclidean_distance_based_similarity()` function."
      ],
      "metadata": {
        "id": "wIbjPNZ2kme5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector_based_similarity_scores = {}\n",
        "\n",
        "for i, vector in enumerate(word_vector_transformed_corpus):\n",
        "  sim = euclidean_distance_based_similarity(vector, word_vector_transformed_query)\n",
        "  word_vector_based_similarity_scores[i] = sim\n",
        "\n",
        "ranked_documents = sorted(word_vector_based_similarity_scores.items(),key = lambda x: x[1] ,reverse = True)\n",
        "\n",
        "# Let's print the top 5 documents based on ranked score\n",
        "\n",
        "print (f\"Query: {query}\")\n",
        "for document_idx, score in ranked_documents[:5]:\n",
        "  print (f\"Document: {corpus[document_idx]}, Score: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR6VT2mdkvtx",
        "outputId": "255bd1b8-f5f2-4250-9c67-99a0030d2825"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: the latest advancements in artificial intelligence for image recognition\n",
            "Document: The latest smartphone model boasts a revolutionary camera system that enhances low-light photography., Score: 0.7373770039611364\n",
            "Document: Deforestation in the Amazon rainforest continues to pose a significant threat to biodiversity and indigenous communities., Score: 0.6037419499710674\n",
            "Document: Mental health awareness initiatives aim to reduce stigma and promote access to support services for individuals struggling with psychological disorders., Score: 0.6014855932843852\n",
            "Document: Vaccination campaigns are essential for preventing the spread of infectious diseases and achieving herd immunity., Score: 0.577208812792024\n",
            "Document: Regular exercise and a balanced diet are key components of maintaining a healthy lifestyle and preventing chronic illnesses like heart disease and diabetes., Score: 0.5741223590009982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise E3: Repeat section 1.3 with `glove-wiki-gigaword-50`\n",
        "\n",
        "Hint: Use, `word_vectors = api.load(\"glove-wiki-gigaword-300\")`\n",
        "\n",
        "Do you see any improved results? What could be the reason behind getting better results? Comment.\n",
        "\n",
        "Also feel free to try other queries and share your observations."
      ],
      "metadata": {
        "id": "jPmeMDh5mr7n"
      }
    }
  ]
}