{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYmt3yulr3p3"
      },
      "source": [
        "# Lab 4: Document Representation and Similariy Measurement\n",
        "\n",
        "Note: This lab session is graded. Complete all exercises and submit it under **Canvas->Lab4** (https://utexas.instructure.com/courses/1382133/assignments/6619548) by no later than **02/08/2023, 11:59PM**. Please attempt all exercises.\n",
        "\n",
        "For extracting representations from text, we will be using the following libraries:\n",
        "\n",
        "1. SpaCy for text pre-processing (tokenization, lemmatization, stopword removal)\n",
        "2. Scikit learn's vectorizer's\n",
        "3. Gensim's word vector's\n",
        "\n",
        "\n",
        "References:\n",
        "1. [ SpaCY ]  https://spacy.io/usage/processing-pipelines\n",
        "2. [ Gensim ] https://radimrehurek.com/gensim/auto_examples/index.html\n",
        "3. [ Scikit ] https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVYnMTIosgfI"
      },
      "source": [
        "## 1. Representing Documents with Bag-of-words approach\n",
        "\n",
        "We are interested in featurizing (or vectorizing) a given text corpus so that we can compute document similarity or perform similariy based search.\n",
        "\n",
        "For example: for the given query, `query  = \"Context is captured better through deep learning based text encoders\"`, we want to identify, which documents in the text corpus are most similary to the query, and possibly rank them in the order of similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p9QrRBj1sf89"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    # Technology\n",
        "    \"The latest smartphone model boasts a revolutionary camera system that enhances low-light photography.\",\n",
        "    \"Artificial intelligence algorithms are reshaping industries by automating routine tasks and streamlining operations.\",\n",
        "    \"Quantum computing holds the promise of solving complex problems exponentially faster than classical computers.\",\n",
        "\n",
        "    # Environment\n",
        "    \"Deforestation in the Amazon rainforest continues to pose a significant threat to biodiversity and indigenous communities.\",\n",
        "    \"Renewable energy sources such as solar and wind power are crucial for reducing carbon emissions and combating climate change.\",\n",
        "    \"Plastic pollution in oceans is a pressing environmental issue, with millions of marine animals suffering from ingestion or entanglement.\",\n",
        "\n",
        "    # Health\n",
        "    \"Vaccination campaigns are essential for preventing the spread of infectious diseases and achieving herd immunity.\",\n",
        "    \"Mental health awareness initiatives aim to reduce stigma and promote access to support services for individuals struggling with psychological disorders.\",\n",
        "    \"Regular exercise and a balanced diet are key components of maintaining a healthy lifestyle and preventing chronic illnesses like heart disease and diabetes.\"\n",
        "]\n",
        "\n",
        "query = \"the latest advancements in artificial intelligence for image recognition\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WG4XzNmMQqQ"
      },
      "source": [
        "## 1.1. Apply Pre-processing Techniques\n",
        "\n",
        "Understanding the benefits of employing text pre-processing techniques is crucial in mitigating data sparsity concerns and reducing the necessity for an extensive vocabulary. We will proceed by directly implementing pre-processing on both the corpus and the query. However, for those who are curious, we offer the option to disable pre-processing to evaluate whether its application was indeed advantageous.\n",
        "\n",
        "We will apply the following basic preprocessing steps:\n",
        "\n",
        "1. Lowercasing\n",
        "2. Tokenization\n",
        "2. Lemmatization\n",
        "3. Stopword removal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkZQDWVXMJ0y",
        "outputId": "4cf4b146-5e3d-42eb-bb82-27bb4f7d11fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# get a list of stopwords from NLTK\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "# Load SpaCy English language model\n",
        "# this is a pipeline capable of applying morphological, lexical and syntax analysis on text\n",
        "\n",
        "nlp_pipeline = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pre_process_a_single_sentence(sentence):\n",
        "  # Lower case text\n",
        "  sentence = sentence.lower()\n",
        "\n",
        "  processed_sentence = []\n",
        "\n",
        "  # Tokenize, and lemmatize the text\n",
        "  doc = nlp_pipeline(sentence)\n",
        "\n",
        "  for token in doc:\n",
        "    # here token is an object that contains various information about each token\n",
        "    # information such as lemma, pos, parse labels are available\n",
        "\n",
        "    # we will check here if tokens are present in stopwords\n",
        "    # if not, we will retain their lemma\n",
        "    if token not in stops:\n",
        "      lemmatized_token = token.lemma_\n",
        "      processed_sentence.append(lemmatized_token)\n",
        "  processed_sentence = \" \".join (processed_sentence)\n",
        "  return processed_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzXJ5BXWb5DN"
      },
      "source": [
        "Now, apply the preprocessing steps on the corpus as well as the queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Z6ZD9scNIv",
        "outputId": "592d24b7-d5df-4387-c3a7-fdd3bb79a841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the late smartphone model boast a revolutionary camera system that enhance low - light photography .', 'artificial intelligence algorithm be reshape industry by automate routine task and streamline operation .', 'quantum computing hold the promise of solve complex problem exponentially fast than classical computer .', 'deforestation in the amazon rainforest continue to pose a significant threat to biodiversity and indigenous community .', 'renewable energy source such as solar and wind power be crucial for reduce carbon emission and combat climate change .']\n",
            "the late advancement in artificial intelligence for image recognition\n"
          ]
        }
      ],
      "source": [
        "pre_processed_corpus = [pre_process_a_single_sentence(sentence) for sentence in corpus]\n",
        "\n",
        "pre_processed_query = pre_process_a_single_sentence(query)\n",
        "\n",
        "# sanity check: print 5 processed documents\n",
        "\n",
        "print (pre_processed_corpus[:5])\n",
        "print (pre_processed_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMqaD_BVc9EO"
      },
      "source": [
        "## 1.2. Extract Bag-of-words (BoW) representations / vectors based on the corpus\n",
        "\n",
        "We construct the vocabulary using `fit()` method applied on the corpus and then used the vocabulary to construct a BoW presence / absence vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUSRCTdrdsr_",
        "outputId": "056d3707-03cd-406f-d346-38a6a81b6e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "120\n",
            "['access' 'achieve' 'aim' 'algorithm' 'amazon' 'and' 'animal' 'artificial'\n",
            " 'as' 'automate' 'awareness' 'balanced' 'be' 'biodiversity' 'boast' 'by'\n",
            " 'camera' 'campaign' 'carbon' 'change' 'chronic' 'classical' 'climate'\n",
            " 'combat' 'community' 'complex' 'component' 'computer' 'computing'\n",
            " 'continue' 'crucial' 'deforestation' 'diabete' 'diet' 'disease'\n",
            " 'disorder' 'emission' 'energy' 'enhance' 'entanglement' 'environmental'\n",
            " 'essential' 'exercise' 'exponentially' 'fast' 'for' 'from' 'health'\n",
            " 'healthy' 'heart' 'herd' 'hold' 'illness' 'immunity' 'in' 'indigenous'\n",
            " 'individual' 'industry' 'infectious' 'ingestion' 'initiative'\n",
            " 'intelligence' 'issue' 'key' 'late' 'lifestyle' 'light' 'like' 'low'\n",
            " 'maintain' 'marine' 'mental' 'million' 'model' 'ocean' 'of' 'operation'\n",
            " 'or' 'photography' 'plastic' 'pollution' 'pose' 'power' 'press' 'prevent'\n",
            " 'problem' 'promise' 'promote' 'psychological' 'quantum' 'rainforest'\n",
            " 'reduce' 'regular' 'renewable' 'reshape' 'revolutionary' 'routine'\n",
            " 'service' 'significant' 'smartphone' 'solar' 'solve' 'source' 'spread'\n",
            " 'stigma' 'streamline' 'struggle' 'such' 'suffer' 'support' 'system'\n",
            " 'task' 'than' 'that' 'the' 'threat' 'to' 'vaccination' 'wind' 'with']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define the N for N-grams\n",
        "N = 1  # Change this to the desired value for N-grams\n",
        "\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False, binary = True)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(pre_processed_corpus)\n",
        "\n",
        "# Check a few items in the vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "# check the vocabulary size\n",
        "print (len(vocab))\n",
        "\n",
        "# sanity check: check the list of vocabulary\n",
        "print (vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGtRbhVefdO"
      },
      "source": [
        "Once the vocabulary is formed, we now transform ANY text using `transform` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es-i8jXeeuUN",
        "outputId": "935a96c3-d883-4818-ff82-2f55757e9953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed Corpus Samples [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "       1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0]), array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
            "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0]), array([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
            "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0])]\n",
            "Transformed Query [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "bow_transformed_corpus = []\n",
        "\n",
        "for sentence in pre_processed_corpus:\n",
        "  transformed_vector = vectorizer.transform([sentence])\n",
        "  bow_transformed_corpus.append(transformed_vector.toarray()[0])\n",
        "\n",
        "bow_transformed_query = vectorizer.transform([pre_processed_query]).toarray()[0]\n",
        "\n",
        "# sanity check : print a few items from the bow_transformed_corpus and bow_transformed_query\n",
        "print (\"Transformed Corpus Samples\", bow_transformed_corpus[:5])\n",
        "print (\"Transformed Query\", bow_transformed_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1-fAGTZgYbK"
      },
      "source": [
        "Now we can match the query vector with the corpus vectors in a pairwise manner to see which ones are most similar to the query vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhIJrKVXgCfv",
        "outputId": "d9700c94-cb8e-4777-f966-0fd0cd7fe7c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: the latest advancements in artificial intelligence for image recognition\n",
            "Document: The latest smartphone model boasts a revolutionary camera system that enhances low-light photography., Score: 0.2581988897471611\n",
            "Document: Artificial intelligence algorithms are reshaping industries by automating routine tasks and streamlining operations., Score: 0.2581988897471611\n",
            "Document: Deforestation in the Amazon rainforest continues to pose a significant threat to biodiversity and indigenous communities., Score: 0.25\n",
            "Document: Vaccination campaigns are essential for preventing the spread of infectious diseases and achieving herd immunity., Score: 0.24253562503633297\n",
            "Document: Quantum computing holds the promise of solving complex problems exponentially faster than classical computers., Score: 0.23570226039551587\n"
          ]
        }
      ],
      "source": [
        "# let's define a similarity function.\n",
        "# we take 1 / euclidean_distance between two vectors as the similarity\n",
        "\n",
        "import numpy as np\n",
        "def euclidean_distance_based_similarity (vector1, vector2):\n",
        "    \"\"\"\n",
        "    Compute the Euclidean distance between two vectors.\n",
        "\n",
        "    Parameters:\n",
        "        vector1 (array-like): First vector.\n",
        "        vector2 (array-like): Second vector.\n",
        "\n",
        "    Returns:\n",
        "        float: Euclidean distance between the two vectors.\n",
        "    \"\"\"\n",
        "    return 1 / (np.linalg.norm(np.array(vector1) - np.array(vector2)))\n",
        "\n",
        "\n",
        "similarity_scores = {}\n",
        "\n",
        "for i, document_vector in enumerate(bow_transformed_corpus):\n",
        "  sim = euclidean_distance_based_similarity(document_vector, bow_transformed_query)\n",
        "  similarity_scores[i] = sim\n",
        "\n",
        "ranked_documents = sorted(similarity_scores.items(),key = lambda x: x[1] ,reverse = True)\n",
        "\n",
        "# Let's print the top 5 documents based on ranked score\n",
        "\n",
        "print (f\"Query: {query}\")\n",
        "for document_idx, score in ranked_documents[:5]:\n",
        "  print (f\"Document: {corpus[document_idx]}, Score: {score}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g2RefzjkDO0"
      },
      "source": [
        "### Exercise E1. Repeat the above steps using BoW-count method:\n",
        "\n",
        "Hint: Use `binary = False`\n",
        "\n",
        "`vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False, binary = False)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "APEr1oX3mqZf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed Corpus Samples [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "       1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0]), array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
            "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 1, 1, 2, 0, 0, 0]), array([0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
            "       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0])]\n",
            "Transformed Query [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Exercise E1\n",
        "bow_c_vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False, binary = False)\n",
        "bow_c_vectorizer.fit(pre_processed_corpus)\n",
        "\n",
        "bow_c_vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "# now transform the vector and the query\n",
        "bow_c_transformed_corpus = []\n",
        "\n",
        "for sentence in pre_processed_corpus:\n",
        "  bow_c_transformed_vector = bow_c_vectorizer.transform([sentence])\n",
        "  bow_c_transformed_corpus.append(bow_c_transformed_vector.toarray()[0])\n",
        "\n",
        "bow_c_transformed_query = bow_c_vectorizer.transform([pre_processed_query]).toarray()[0]\n",
        "\n",
        "#  print items to make sure it worked\n",
        "print (\"Transformed Corpus Samples\", bow_c_transformed_corpus[:5])\n",
        "print (\"Transformed Query\", bow_c_transformed_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There's a 2 in there! it worked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: the latest advancements in artificial intelligence for image recognition\n",
            "Document: The latest smartphone model boasts a revolutionary camera system that enhances low-light photography., Score: 0.2581988897471611\n",
            "Document: Artificial intelligence algorithms are reshaping industries by automating routine tasks and streamlining operations., Score: 0.2581988897471611\n",
            "Document: Vaccination campaigns are essential for preventing the spread of infectious diseases and achieving herd immunity., Score: 0.24253562503633297\n",
            "Document: Quantum computing holds the promise of solving complex problems exponentially faster than classical computers., Score: 0.23570226039551587\n",
            "Document: Deforestation in the Amazon rainforest continues to pose a significant threat to biodiversity and indigenous communities., Score: 0.22941573387056174\n"
          ]
        }
      ],
      "source": [
        "# part 2 of E1 - get similarity (C+P from above)\n",
        "# we take 1 / euclidean_distance between two vectors as the similarity\n",
        "c_similarity_scores = {}\n",
        "\n",
        "for i, document_vector in enumerate(bow_c_transformed_corpus):\n",
        "  s = euclidean_distance_based_similarity(document_vector, bow_c_transformed_query)\n",
        "  c_similarity_scores[i] = s\n",
        "\n",
        "c_ranked_documents = sorted(c_similarity_scores.items(),key = lambda x: x[1], reverse = True)\n",
        "\n",
        "# Let's print the top 5 documents based on ranked score\n",
        "\n",
        "print (f\"Query: {query}\")\n",
        "for document_idx, score in c_ranked_documents[:5]:\n",
        "  print (f\"Document: {corpus[document_idx]}, Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise E2: Repeat the above steps using TF-IDF method:\n",
        "\n",
        "Hint: Use `TfidfVectorizer` instead of CountVectorizer\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\n",
        "Try different queries and explain your observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed Corpus Samples [array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.28374061,\n",
            "       0.        , 0.28374061, 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.28374061, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.28374061,\n",
            "       0.        , 0.28374061, 0.        , 0.28374061, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.28374061, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.28374061, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.28374061, 0.        , 0.        , 0.        , 0.28374061,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.28374061, 0.        , 0.        , 0.28374061, 0.18410655,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.        , 0.        , 0.        , 0.29354014, 0.        ,\n",
            "       0.15261469, 0.        , 0.29354014, 0.        , 0.29354014,\n",
            "       0.        , 0.        , 0.16995536, 0.        , 0.        ,\n",
            "       0.29354014, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.29354014, 0.        , 0.        ,\n",
            "       0.        , 0.29354014, 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.29354014, 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.29354014,\n",
            "       0.        , 0.29354014, 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.29354014, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.29354014, 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.27905077, 0.        , 0.        , 0.        ,\n",
            "       0.27905077, 0.        , 0.27905077, 0.27905077, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.27905077, 0.27905077,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.27905077, 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.18106353, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.27905077, 0.27905077, 0.        , 0.        , 0.27905077,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.27905077, 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.27905077, 0.        , 0.18106353,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]), array([0.        , 0.        , 0.        , 0.        , 0.28726534,\n",
            "       0.14935235, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.28726534, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.28726534,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.28726534,\n",
            "       0.        , 0.28726534, 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.24262888,\n",
            "       0.28726534, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.28726534, 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.28726534, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.28726534, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.18639359,\n",
            "       0.28726534, 0.24262888, 0.        , 0.        , 0.        ]), array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.1305575 , 0.        , 0.        , 0.25111519, 0.        ,\n",
            "       0.        , 0.        , 0.14539195, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.25111519, 0.25111519,\n",
            "       0.        , 0.        , 0.25111519, 0.25111519, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.25111519, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.25111519, 0.25111519, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.18441125, 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.25111519, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.21209589, 0.        , 0.25111519, 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.25111519, 0.        , 0.25111519, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.25111519, 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
            "       0.        , 0.        , 0.        , 0.25111519, 0.        ])]\n",
            "Transformed Query [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.46256225 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.33969145 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.39068744 0.         0.         0.         0.         0.\n",
            " 0.         0.46256225 0.         0.         0.46256225 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.30013589 0.         0.         0.         0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "# Exercise E2 - Use TF-IDF Method\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# mostly the same code as before\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(N, N), lowercase = False, binary = True)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_vectorizer.fit(pre_processed_corpus)\n",
        "\n",
        "# Check a few items in the vocabulary\n",
        "vocab = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# now transform the vector and the query\n",
        "tfidf_transformed_corpus = []\n",
        "\n",
        "for sentence in pre_processed_corpus:\n",
        "  tfidf_transformed_vector = tfidf_vectorizer.transform([sentence])\n",
        "  tfidf_transformed_corpus.append(tfidf_transformed_vector.toarray()[0])\n",
        "\n",
        "tfidf_transformed_query = tfidf_vectorizer.transform([pre_processed_query]).toarray()[0]\n",
        "\n",
        "#  print items to make sure it worked\n",
        "print (\"Transformed Corpus Samples\", tfidf_transformed_corpus[:5])\n",
        "print (\"Transformed Query\", tfidf_transformed_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: the latest advancements in artificial intelligence for image recognition\n",
            "Document: Artificial intelligence algorithms are reshaping industries by automating routine tasks and streamlining operations., Score: 0.8284922651973035\n",
            "Document: The latest smartphone model boasts a revolutionary camera system that enhances low-light photography., Score: 0.7839844906920026\n",
            "Document: Deforestation in the Amazon rainforest continues to pose a significant threat to biodiversity and indigenous communities., Score: 0.7672970118364889\n",
            "Document: Vaccination campaigns are essential for preventing the spread of infectious diseases and achieving herd immunity., Score: 0.7587451350178317\n",
            "Document: Plastic pollution in oceans is a pressing environmental issue, with millions of marine animals suffering from ingestion or entanglement., Score: 0.7380234489141645\n"
          ]
        }
      ],
      "source": [
        "# part 2 of E2 - get similarity (C+P from above)\n",
        "# we take 1 / euclidean_distance between two vectors as the similarity\n",
        "t_similarity_scores = {}\n",
        "\n",
        "for i, document_vector in enumerate(tfidf_transformed_corpus):\n",
        "  t_similarity_scores[i] = euclidean_distance_based_similarity(document_vector, tfidf_transformed_query)\n",
        "\n",
        "t_ranked_documents = sorted(t_similarity_scores.items(),key = lambda x: x[1], reverse = True)\n",
        "\n",
        "# Let's print the top 5 documents based on ranked score\n",
        "\n",
        "print (f\"Query: {query}\")\n",
        "for document_idx, score in t_ranked_documents[:5]:\n",
        "  print (f\"Document: {corpus[document_idx]}, Score: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scores given here are MUCH higher than in regular Count Vectorization. That's cool!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP7RRnSdi7bu"
      },
      "source": [
        "## 1.3. Extract Word Vectors based on GloVe and compute similarity between query and documents\n",
        "\n",
        "- In all the above implementations, we always convert words into a number and sentences into an N-hot representation.\n",
        "\n",
        "- This does not effectively capture relationships between words and phrases.\n",
        "\n",
        "- In principle, words should be \"known by the company they keep\". For example, the word \"cat\" should be related to \"dog\" more than \"Wednesday\".\n",
        "\n",
        "- We thus vectorize corpus and queries using word embeddings, i.e., representations that capture the semantic association between words\n",
        "\n",
        "- Vectorization using word embeddings allow us to perform semantic search\n",
        "\n",
        "- We will use glove embeddings (http://nlp.stanford.edu/data/glove.6B.zip) as our source of pre-trained word embeddings/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/python/3.10.13/lib/python3.10/site-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gensim) (6.4.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A4FkkF3pjXrJ"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# Function to generate average word vectors for a sentence\n",
        "def average_word_embeddings(sentence):\n",
        "    words = sentence.split()\n",
        "    embeddings = []\n",
        "    for word in words:\n",
        "        if word in word_vectors:\n",
        "            embeddings.append(word_vectors[word])\n",
        "    if len(embeddings) > 0:\n",
        "        # is word vector exists for the word\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word_vectors.vector_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4XH-7tZjtIa",
        "outputId": "3e14f00e-5427-4235-ccef-65c4e176e828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Vector Transformed Corpus Samples [array([ 0.1861708 ,  0.23128964,  0.1962533 ,  0.07322185,  0.09015149,\n",
            "       -0.01049213, -0.29616618, -0.6643501 ,  0.0113958 ,  0.25757927,\n",
            "        0.2509965 , -0.06853063, -0.17922047,  0.22037098, -0.10514338,\n",
            "        0.11703007, -0.368467  ,  0.1734872 , -0.31144908, -0.42928684,\n",
            "        0.09016962,  0.04219813, -0.21167749, -0.18209696,  0.02590456,\n",
            "       -1.3121846 , -0.4729219 ,  0.05249795,  0.16609696,  0.12275426,\n",
            "        2.8859878 , -0.0463383 , -0.27610356, -0.39248237,  0.10850608,\n",
            "        0.02044093, -0.02673562,  0.31013373, -0.21586457, -0.22675025,\n",
            "        0.13021228,  0.16225356, -0.03704819,  0.01487388,  0.04129475,\n",
            "       -0.01695319,  0.19756524, -0.21693762,  0.06474212, -0.01510743],\n",
            "      dtype=float32), array([ 6.65031433e-01, -5.51267922e-01, -7.34305009e-02,  1.08917437e-01,\n",
            "       -9.01242867e-02,  1.31470293e-01, -5.51946349e-02, -3.14387828e-01,\n",
            "        4.34026450e-01, -1.80350065e-01,  1.47496939e-01,  2.39261106e-01,\n",
            "       -3.54352951e-01,  2.21403003e-01, -3.89457084e-02,  1.47164658e-01,\n",
            "        8.39814451e-03,  2.06601452e-02,  2.78091401e-01, -3.30505580e-01,\n",
            "        3.66178542e-01, -1.66800961e-01, -5.20542562e-02, -2.13722214e-01,\n",
            "        5.24764173e-02, -9.82092202e-01, -3.60993356e-01, -1.92898944e-01,\n",
            "        2.84022152e-01, -2.47391500e-02,  2.54966164e+00, -1.02038108e-01,\n",
            "       -3.87977153e-01, -5.35810828e-01, -9.07605067e-02,  3.81620646e-01,\n",
            "       -6.41754344e-02,  4.71663922e-01, -5.65256737e-02,  7.00580771e-04,\n",
            "       -1.87875628e-01, -1.01243861e-01,  1.00440852e-01,  7.89626986e-02,\n",
            "       -2.16670688e-02, -1.18815266e-01,  3.01968098e-01,  2.44032159e-01,\n",
            "       -3.95721849e-03,  2.83943564e-01], dtype=float32), array([ 3.9172047e-01,  1.2900580e-01,  2.0420581e-01,  1.5327910e-01,\n",
            "        2.4062508e-01,  1.1633820e-01,  2.8576596e-02, -5.0333136e-01,\n",
            "       -4.0251337e-02,  3.1612000e-01,  1.6361172e-01,  4.7533662e-04,\n",
            "       -2.9986697e-01,  4.3444864e-02,  2.3199336e-01,  2.8060266e-01,\n",
            "        8.5461006e-02,  2.9715979e-01, -6.1993320e-03, -4.6446151e-01,\n",
            "        1.8231973e-01, -1.5975301e-01, -1.2052727e-01,  3.1586342e-02,\n",
            "        3.5787657e-01, -1.0669286e+00, -4.1094869e-01, -2.6214767e-01,\n",
            "        2.5093552e-01,  1.2832053e-01,  2.9472032e+00, -1.1105145e-01,\n",
            "       -1.5928563e-01, -5.7094949e-01,  1.2716478e-02, -3.4318723e-02,\n",
            "       -3.0151600e-01,  3.1469259e-01,  9.1245919e-02,  1.3608252e-01,\n",
            "       -2.0185390e-01, -6.6788331e-02, -8.4780850e-02,  5.6183565e-01,\n",
            "       -1.6099812e-01,  1.5295687e-01,  4.4407013e-01,  1.2917320e-01,\n",
            "       -3.2199258e-01, -5.8066852e-02], dtype=float32), array([ 0.41160825,  0.03198936, -0.17880146,  0.17501259,  0.02401954,\n",
            "        0.0539993 , -0.43018597, -0.3107636 ,  0.51613015,  0.02991601,\n",
            "        0.21159112, -0.091382  ,  0.19406164, -0.37581322,  0.12083244,\n",
            "        0.212719  ,  0.47153506, -0.18733534, -0.24070883, -0.19600448,\n",
            "       -0.17699158, -0.11887617, -0.05364224, -0.1244106 ,  0.12862587,\n",
            "       -1.0799372 , -0.15318091, -0.13824254,  0.17848527,  0.18671258,\n",
            "        2.9894586 , -0.03816568, -0.2927905 , -0.6356198 , -0.16903345,\n",
            "       -0.04134628, -0.34912944, -0.2583366 , -0.052456  , -0.07177824,\n",
            "       -0.39163336, -0.04872801,  0.19073546, -0.05724087,  0.05282818,\n",
            "        0.17250147, -0.16975883,  0.03626582,  0.01699288, -0.3162806 ],\n",
            "      dtype=float32), array([ 3.3166641e-01,  2.9425693e-01,  3.5540617e-01, -1.2295691e-02,\n",
            "       -1.8778530e-01,  2.7292940e-01, -3.9368786e-02, -5.3398043e-01,\n",
            "        2.4718373e-01,  3.5760814e-01,  3.6202496e-01,  1.8906558e-01,\n",
            "        2.5939506e-02,  1.9519775e-01, -2.0751655e-02,  3.7216592e-01,\n",
            "        4.0485111e-01,  1.7786548e-01, -2.5209668e-01, -7.3235881e-01,\n",
            "        2.4850032e-01, -3.5174209e-01,  2.1395782e-01, -4.1440827e-01,\n",
            "        5.2659888e-02, -1.1435591e+00, -6.7158647e-02, -1.3675161e-02,\n",
            "        3.6030388e-01,  4.5823631e-01,  3.1421747e+00,  1.9451322e-01,\n",
            "       -4.4032055e-01, -7.5102878e-01,  1.2578830e-02, -3.6561742e-02,\n",
            "       -1.1146768e-01,  1.6073941e-01,  8.7763593e-02,  1.0231966e-01,\n",
            "       -2.9401755e-01,  4.8506558e-02,  1.9520642e-01, -5.9763748e-02,\n",
            "       -4.4319008e-02,  2.0756450e-01, -9.5205963e-02,  1.9220749e-01,\n",
            "       -2.7397531e-03,  5.0286252e-02], dtype=float32)]\n",
            "Word Vector Transformed Query [ 0.12992312  0.34614065 -0.20598     0.07797755  0.181838    0.14452912\n",
            " -0.3227381  -0.3301632   0.17259555  0.03385688  0.35603043 -0.09890389\n",
            " -0.16480544 -0.01330334 -0.08531885  0.11958823 -0.14276844 -0.09645544\n",
            " -0.2185589   0.02335288  0.08201378  0.16925266 -0.04636272 -0.473748\n",
            " -0.04970878 -1.3831545  -0.52707046 -0.1593099  -0.07167777  0.08884601\n",
            "  2.9674666  -0.01558955 -0.37647232 -0.8620853  -0.08335832  0.25001976\n",
            " -0.06147078  0.30945146 -0.21170244 -0.26154977 -0.05120032 -0.06026081\n",
            " -0.16217512  0.01054322 -0.13702378 -0.03056667  0.02288499  0.16727\n",
            " -0.12842447 -0.3544489 ]\n"
          ]
        }
      ],
      "source": [
        "word_vector_transformed_corpus = []\n",
        "\n",
        "for sentence in pre_processed_corpus:\n",
        "  transformed_vector = average_word_embeddings(sentence)\n",
        "  word_vector_transformed_corpus.append(transformed_vector)\n",
        "\n",
        "word_vector_transformed_query = average_word_embeddings(pre_processed_query)\n",
        "\n",
        "# sanity check : print a few items from the bow_transformed_corpus and bow_transformed_query\n",
        "print (\"Word Vector Transformed Corpus Samples\", word_vector_transformed_corpus[:5])\n",
        "print (\"Word Vector Transformed Query\", word_vector_transformed_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIbjPNZ2kme5"
      },
      "source": [
        "Now, let's use the vectors for computing similarity between queries and documents to see which ones are most similar to the queries.\n",
        "\n",
        "For aimilarity measurement, we will use the same `euclidean_distance_based_similarity()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR6VT2mdkvtx",
        "outputId": "255bd1b8-f5f2-4250-9c67-99a0030d2825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: the latest advancements in artificial intelligence for image recognition\n",
            "Document: The latest smartphone model boasts a revolutionary camera system that enhances low-light photography., Score: 0.7373770039611364\n",
            "Document: Deforestation in the Amazon rainforest continues to pose a significant threat to biodiversity and indigenous communities., Score: 0.6037419499710674\n",
            "Document: Mental health awareness initiatives aim to reduce stigma and promote access to support services for individuals struggling with psychological disorders., Score: 0.6014855932843852\n",
            "Document: Vaccination campaigns are essential for preventing the spread of infectious diseases and achieving herd immunity., Score: 0.577208812792024\n",
            "Document: Regular exercise and a balanced diet are key components of maintaining a healthy lifestyle and preventing chronic illnesses like heart disease and diabetes., Score: 0.5741223590009982\n"
          ]
        }
      ],
      "source": [
        "word_vector_based_similarity_scores = {}\n",
        "\n",
        "for i, vector in enumerate(word_vector_transformed_corpus):\n",
        "  sim = euclidean_distance_based_similarity(vector, word_vector_transformed_query)\n",
        "  word_vector_based_similarity_scores[i] = sim\n",
        "\n",
        "ranked_documents = sorted(word_vector_based_similarity_scores.items(),key = lambda x: x[1] ,reverse = True)\n",
        "\n",
        "# Let's print the top 5 documents based on ranked score\n",
        "\n",
        "print (f\"Query: {query}\")\n",
        "for document_idx, score in ranked_documents[:5]:\n",
        "  print (f\"Document: {corpus[document_idx]}, Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPmeMDh5mr7n"
      },
      "source": [
        "### Exercise E3: Repeat section 1.3 with `glove-wiki-gigaword-50`\n",
        "\n",
        "Hint: Use, `word_vectors = api.load(\"glove-wiki-gigaword-300\")`\n",
        "\n",
        "Do you see any improved results? What could be the reason behind getting better results? Comment.\n",
        "\n",
        "Also feel free to try other queries and share your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Exercise E3\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m n_word_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglove-wiki-gigaword-300\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function to generate average word vectors for a sentence\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage_word_embeddings\u001b[39m(sentence):\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/gensim/downloader.py:503\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, BASE_DIR)\n\u001b[1;32m    502\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28m__import__\u001b[39m(name)\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/gensim-data/glove-wiki-gigaword-300/__init__.py:8\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n\u001b[1;32m      7\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove-wiki-gigaword-300\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove-wiki-gigaword-300.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2069\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2065\u001b[0m         _word2vec_read_binary(\n\u001b[1;32m   2066\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[1;32m   2067\u001b[0m         )\n\u001b[1;32m   2068\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2069\u001b[0m         \u001b[43m_word2vec_read_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(kv):\n\u001b[1;32m   2071\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2073\u001b[0m         kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(kv),\n\u001b[1;32m   2074\u001b[0m     )\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1971\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_word2vec_read_text\u001b[39m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding):\n\u001b[1;32m   1970\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(vocab_size):\n\u001b[0;32m-> 1971\u001b[0m         line \u001b[38;5;241m=\u001b[39m \u001b[43mfin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1972\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1973\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/gzip.py:399\u001b[0m, in \u001b[0;36mGzipFile.readline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadline\u001b[39m(\u001b[38;5;28mself\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_not_closed()\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/gzip.py:496\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    494\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 496\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mprepend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Exercise E3\n",
        "n_word_vectors = api.load(\"glove-wiki-gigaword-300\")\n",
        "\n",
        "# Function to generate average word vectors for a sentence\n",
        "def average_word_embeddings(sentence):\n",
        "    words = sentence.split()\n",
        "    embeddings = []\n",
        "    for word in words:\n",
        "        if word in n_word_vectors:\n",
        "            embeddings.append(n_word_vectors[word])\n",
        "    if len(embeddings) > 0:\n",
        "        # is word vector exists for the word\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(n_word_vectors.vector_size)\n",
        "    \n",
        "word_vector_transformed_corpus = []\n",
        "for sentence in pre_processed_corpus:\n",
        "  transformed_vector = average_word_embeddings(sentence)\n",
        "  word_vector_transformed_corpus.append(transformed_vector)\n",
        "\n",
        "word_vector_transformed_query = average_word_embeddings(pre_processed_query)\n",
        "\n",
        "# sanity check : print a few items from the bow_transformed_corpus and bow_transformed_query\n",
        "print (\"Word Vector Transformed Corpus Samples\", word_vector_transformed_corpus[:5])\n",
        "print (\"Word Vector Transformed Query\", word_vector_transformed_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: the latest advancements in artificial intelligence for image recognition\n",
            "Document: The latest smartphone model boasts a revolutionary camera system that enhances low-light photography., Score: 0.4489006370416574\n",
            "Document: Artificial intelligence algorithms are reshaping industries by automating routine tasks and streamlining operations., Score: 0.4023113287992021\n",
            "Document: Mental health awareness initiatives aim to reduce stigma and promote access to support services for individuals struggling with psychological disorders., Score: 0.40202061581758824\n",
            "Document: Plastic pollution in oceans is a pressing environmental issue, with millions of marine animals suffering from ingestion or entanglement., Score: 0.3953461497640631\n",
            "Document: Quantum computing holds the promise of solving complex problems exponentially faster than classical computers., Score: 0.39100700592005466\n"
          ]
        }
      ],
      "source": [
        "word_vector_based_similarity_scores = {}\n",
        "\n",
        "for i, vector in enumerate(word_vector_transformed_corpus):\n",
        "  sim = euclidean_distance_based_similarity(vector, word_vector_transformed_query)\n",
        "  word_vector_based_similarity_scores[i] = sim\n",
        "\n",
        "ranked_documents = sorted(word_vector_based_similarity_scores.items(),key = lambda x: x[1] ,reverse = True)\n",
        "\n",
        "# Let's print the top 5 documents based on ranked score\n",
        "\n",
        "print (f\"Query: {query}\")\n",
        "for document_idx, score in ranked_documents[:5]:\n",
        "  print (f\"Document: {corpus[document_idx]}, Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scores are... lower? idk if this is right, but the scores are significantly worse than the scores before, like nearly .3 worse than before. It could be the extra information mucking things up, or it could just be that I'm interpreting the scores in the wrong way, idk which."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
