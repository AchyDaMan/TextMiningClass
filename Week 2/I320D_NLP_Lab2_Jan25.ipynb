{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab2: Loading and Navigating through Text Corpora and PDF files, Analyzing patterns using Regular Expressions, NLTK and SpaCY library introduction\n",
        "\n",
        "Note: this is a lab session **graded**. Complete all exercises and upload to Canvas under **Lab 2: PDF Files, Datasets, RegEx and more** (https://utexas.instructure.com/courses/1382133/assignments/6619547?module_item_id=13585840) by no later than **01/25/2024, 11:59 PM** (Labs are supposed to be completed inside class).\n",
        "\n",
        "\n",
        "**Also Note:** The first take home assignment - \"Assignment 1: Regular Expressions based Pattern Extraction on PDF data\" will be posted tonight."
      ],
      "metadata": {
        "id": "fS7SPvSpoyFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Processing PDF files and extracting Textual Data\n",
        "\n",
        "By now, we are familiar with the basics of reading and writing simple text files (with a `.txt` extension). However, before delving deeper into advanced text processing techniques, it's crucial to understand how to automatically read and process various file formats. In this context, let's explore PDF file processing, complementing our knowledge of processing web pages using BeautifulSoup and related libraries (previously discussed in Lab1). PDFs and web pages stand out as significant sources of data, collectively presenting vast amounts of text data for text mining endeavors.\n",
        "\n",
        "Before you execute the example code below, please download the sample PDF file from **https://www.fdic.gov/news/events/affordable/hcachecklist.pdf** and place it under **Files**. You can, of course, first open the file locally and examine the content.\n",
        "\n",
        "For reading PDF content, we will need to use a specialized library *PyPDF2* which may not be installed in your environment.\n",
        "\n",
        "Let's launch the installation command fist"
      ],
      "metadata": {
        "id": "IqVjwQv0rA4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz8K8ZDMuZxR",
        "outputId": "f1c72c93-17c3-41ab-94bf-15961c838f34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/232.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Sample PDF file path\n",
        "f = open(r'hcachecklist.pdf', mode='rb')\n",
        "pdfdoc = PyPDF2.PdfReader(f)\n",
        "\n",
        "page_data = []\n",
        "for page in pdfdoc.pages:\n",
        "  text = page.extract_text()\n",
        "  page_data.append(text)\n",
        "\n",
        "print(\"Text extracted using PyPDF2:\\n\", page_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5EOl0TGp3jN",
        "outputId": "8e88b701-d69b-4f7a-aa37-d725d3964772"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extracted using PyPDF2:\n",
            " ['1 of 4 \\n \\n 888-388-HOPE (4673) \\nwww.hopecoalitionamerica.org\\n CHECKLIST OF IMPORTANT LEGAL DOCUMENTS \\nAND FINANCIAL STATEMENTS \\n \\nPlease review the list of important documents below  and check whether you have the document, whether \\nyou need to obtain the document or whether the doc ument does not apply to your household. Next, \\ncollect the documents you have and obtain the ones you still need. These documents, along with the \\ncompleted forms provided here, make up your  Emergency Financial First Aid Kit (EFFAK). \\nOnce you have all of these documents together, y ou should make a copy of your entire EFFAK. As \\nimportant information is often printed on the backs of  these documents, please be sure to copy both \\nsides. \\nBecause these documents contain such important and  personal information, we strongly recommend that \\nyou keep all original documents, photographs and co mputer backup disks in an off-site safety deposit \\nbox.  And be sure to keep the key to your safety deposit box in a safe place too! \\nIn addition, keep a copy of your EFFAK and copi es of your original documents at home in a \\nfireproof/waterproof metal box or safe. Because ATMs do  not work when electricity is out or they may not \\nbe restocked during an emergency, be sure to include some $10 and $20 bills in your storage box.  \\nIf you have an attorney, you may also want to provid e them with a copy of your EFFAK in a sealed \\nenvelope to be opened with your approval, or in the event you become incapacitated.  \\nIMPORTANT  LEGAL DOCUMENTS  THAT APPLY TO MY FAMILY \\n \\n1. Birth Certificate(s)/Adoption Papers \\n2. Marriage License  \\n3. Divorce Papers  \\n4. Social Security Card(s) \\n5. Passport/Green Card  \\n6. Naturalization Documents \\n7. Will \\n8. Power(s) of Attor ney (personal/property) \\n9. Mortgage or Real Estate Deeds of Trust \\n10. Vehicle Registration/Ownership Papers \\n11. ID/Driver’s License  \\n1. have _____need _____N/A _____  2. have _____need _____N/A _____  \\n3. have _____need _____N/A _____  \\n4. have _____need _____N/A _____  5. have _____need _____N/A _____  \\n6. have _____need _____N/A _____  \\n7. have _____need _____N/A _____  \\n8. have _____need _____N/A _____  \\n9. have _____need _____N/A _____  \\n10. have _____need _____N/A _____  \\n11. have _____need _____N/A _____ ', '2 of 4 \\n \\n 888-388-HOPE (4673) \\nwww.hopecoalitionamerica.org\\n TAX STATEMENTS  \\n12. Previous Year’s Tax Returns \\n13. Property Tax Statement 14. Personal Property Tax (i.e. Car Tax) \\n \\nF\\nINANCIAL  ACCOUNTS  \\n15. Bank/Credit Union Statements \\n16. Credit/Debit Card Statements \\n17. Retirement Accounts (401K, TSP, IRA) 18. Investment Accounts (S tocks, Bonds, Mutual \\nFunds) \\n19. Other      \\n \\n \\nSOURCES  OF INCOME /ASSETS  \\n20. Recent Pay Stubs for All Sources of Income \\n21.  Government Benefits (e.g. Social Security, \\nTemporary Assistance for Needy Families, Veterans’) \\n22. Alimony Income \\n23. Child Support Income \\n24. Professional Appraisals of Personal Property \\n25.  Rewards Accounts (e.g., Frequent Flyer Programs, \\nHotel Rewards) \\n26. Other       \\n \\nFINANCIAL  OBLIGATIONS  \\n27. Mortgage Statement \\n28. Lease \\n29. Utility Bills (Elect ric, Water, Gas) \\n30. Car Payment \\n31. Student Loan \\n32. Alimony Payments \\n33. Child Support Payments \\n34. Elder Care Facilities \\n35. Other Debt        \\n12. have _____need _____N/A _____  13. have _____need _____N/A _____  \\n14. have _____need _____N/A _____  \\n \\n \\n15. have _____need _____N/A _____  \\n16. have _____need _____N/A _____  \\n17. have _____need _____N/A _____  \\n18. have _____need _____N/A _____  \\n \\n19. have _____need _____N/A _____  \\n \\n \\n \\n20. have _____need _____N/A _____  \\n21. have _____need _____N/A _____ \\n \\n \\n22. have _____need _____N/A _____  \\n23. have _____need _____N/A _____  \\n24. have _____need _____N/A _____  \\n25. have _____need _____N/A _____  \\n \\n26. have _____need _____N/A _____  \\n \\n  \\n27. have _____need _____N/A _____ \\n28. have _____need _____N/A _____  \\n29. have _____need _____N/A _____ \\n30. have _____need _____N/A _____ \\n31. have _____need _____N/A _____ \\n32. have _____need _____N/A _____ \\n33. have _____need _____N/A _____ \\n34. have _____need _____N/A _____ \\n35. have _____need _____N/A _____', '3 of 4 \\n \\n 888-388-HOPE (4673) \\nwww.hopecoalitionamerica.org\\n TAX STATEMENTS  \\n36. Property Insurance \\n37. Rental Insurance 38. Auto Insurance \\n39. Life Insurance \\n40. Other      \\n \\n \\nMEDICAL  \\n41. Health Insurance ID Card (s) \\n42. Record of Immunizations/Allergies \\n43. List of Necessary Medications \\n44. Disabilities Documentation \\n45. Living Will \\n46. Dental Records / Child Identity Cards / DNA Swabs \\n47. Other       \\n \\nMILITARY  \\n48. Current Military ID \\n49. Military Discharge DD 214 \\n50. Other       \\n \\nOTHER FINANCIAL /LEGAL DOCUMENTATION  \\n5 1 .          \\n5 2 .          \\n5 3 .          \\n36. have _____need _____N/A _____  \\n37. have _____need _____N/A _____  \\n38. have _____need _____N/A _____  \\n39. have _____need _____N/A _____  \\n40. have _____need _____N/A _____  \\n \\n  \\n41. have _____need _____N/A _____  \\n42. have _____need _____N/A _____  \\n43. have _____need _____N/A _____  \\n44. have _____need _____N/A _____  \\n45. have _____need _____N/A _____  \\n46. have _____need _____N/A _____  \\n47. have _____need _____N/A _____  \\n \\n \\n48. have _____need _____N/A _____ \\n49. have _____need _____N/A _____  \\n50. have _____need _____N/A _____ \\n \\n  \\n51. have _____need _____N/A _____ \\n52. have _____need _____N/A _____ \\n53. have _____need _____N/A _____ \\n ', ' \\n4 of 4 \\n \\n 888-388-HOPE (4673) \\nwww.hopecoalitionamerica.org\\n  \\nWe suggest you include a date on each line as you collect and obtain your necessary documents.  This \\nwill help you track your progress  as you work toward  preparing your househ old finances for any \\nunanticipated emergency.  \\nIMPORTANT : If you are a small business owner, you should make sure that you safeguard your business \\nfinances as well: backup computer files routinely, keep original of critical document in an off-site safety \\ndeposit box and keep copies in a secure fireproof, waterproof container on site. \\n ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, this converts text data from each page into an element in the list `page_data`. We can now use this data for downstream processing."
      ],
      "metadata": {
        "id": "I_gidP_3u1xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise E1: Fetch all URLs mentioned in a few research papers.\n",
        "\n",
        "1. Download this conference proceeding and place it under Files. URL: https://aclanthology.org/2023.acl-long.0.pdf\n",
        "\n",
        "2. Process the PDF file using code similar to above.\n",
        "\n",
        "3. Extract all URLs and print (Hint: You can assume that any sentence, after splitting the text with newline symbol `\\n`, containing \"https\" qualifies as a URL"
      ],
      "metadata": {
        "id": "9PcLv4MKvMiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Navigating through existing Public Datasets\n",
        "\n",
        "Like we discussed in our last lecture, several datasets are availabe for public use and Huggingface (http://huggingface.co) is a hub that hosts a few datasets, processed and readily available for building NLP applicaitons. Let's check some datasets below.\n",
        "\n",
        "Datasets from Huggingface are made available through a library called `datasets`. We have to install it.\n",
        "\n"
      ],
      "metadata": {
        "id": "XVEU0RkOwjZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LusWaofbxP0r",
        "outputId": "00720c56-decd-4468-dc4a-9b4516595121"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load IMDb reviews dataset as an example. This is where the data is hosted: https://huggingface.co/datasets/imdb\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Get a sample text from the \"train\" split of the dataset\n",
        "sample_text = dataset[\"train\"][0]\n",
        "print(\"Sample Text:\\n\", sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZU8Ne0oxDaO",
        "outputId": "659071f4-24cd-4411-fa0d-e5c1fc9926cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Text:\n",
            " {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise E1: Print samples from the following datasets\n",
        "\n",
        "In separate code blocks\n",
        "1. Try to print a few sample sentences from the `spanish_billion_words` corpus,  the `simple_questions_v2` corpus.\n",
        "\n",
        "2. Be sure to visit huggingface website, type these corpora names in the search box, look at the data format and modify the code above to get the textual elements."
      ],
      "metadata": {
        "id": "CXInOvhlx-gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Basic Regular Expression Patterns\n",
        "\n",
        "Regular expressions, often abbreviated as regex or regexp, are sequences of characters that define a search pattern. They are powerful tools for string manipulation and matching within text. Regular expressions provide a concise and flexible means to:\n",
        "  \n",
        "- **Search Patterns:** Specify patterns of characters to search for in a text.\n",
        "- **Match Patterns:** Identify whether a string conforms to a given pattern.\n",
        "- **Extract Information:** Extract specific data from strings based on defined patterns.\n",
        "- **Replace Patterns:** Replace occurrences of a pattern with a specified replacement.\n",
        "\n",
        "For a detailed overview on Python RegEx, please follow: https://www.geeksforgeeks.org/python-regex/\n",
        "\n",
        "Let's say we want to extract important information from text such as dates, emails, and phone numbers. We can use regular expressions to extract and display these patterns from the text as follows:"
      ],
      "metadata": {
        "id": "VhAm39hCzH70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Sample text containing dates, emails, and phone numbers\n",
        "sample_text = \"Meeting on 2022-05-20, contact@example.com, and call me at 123-456-7890.\"\n",
        "\n",
        "# Regular expressions for extracting patterns\n",
        "dates = re.findall(r'\\d{4}-\\d{2}-\\d{2}', sample_text)\n",
        "emails = re.findall(r'[A-Za-z0-9]+@[A-Za-z0-9]+\\.[a-z]+', sample_text)\n",
        "phone_numbers = re.findall(r'\\d{3}-\\d{3}-\\d{4}', sample_text)\n",
        "\n",
        "print(\"Dates:\", dates)\n",
        "print(\"Emails:\", emails)\n",
        "print(\"Phone Numbers:\", phone_numbers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8vVyrQ8zFuw",
        "outputId": "d7090944-a99f-4fc7-87a0-3691d6bb3421"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dates: ['2022-05-20']\n",
            "Emails: ['contact@example.com']\n",
            "Phone Numbers: ['123-456-7890']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise E3:\n",
        "1. Following section 1, extract all text from `hcachecklist.pdf` (you can copy paste the code from #1). Now, list down all two word phrases that follow a pattern `a <word>`, `an <word>` and `the <word>`. Some example phrases: `a key`, `the document`.\n",
        "\n",
        "2. For this research paper here https://arxiv.org/abs/1706.03762 , programatically find out if there is any link to a codebase that can be used to replicate the paper (Hint: Load the text from the paper using PyPDF2, look for URLs, clean URLs and then filter out the ones that contain `github.com` in them. Print the final list of urls.\n",
        "\n",
        "3. **[Optional, not-graded]** Repeat 2 for all papers from EMNLP 2023 conference. You will have to scrape the website: https://aclanthology.org/events/emnlp-2023/#2023emnlp-main ,get URLs of all PDF papers, download the PDFs from within your program and repeate #2 for each of them. Dump the final list of GitHub URLs in a file."
      ],
      "metadata": {
        "id": "jQNP7rfm2kBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Introduction to NLTK and SpaCY: Two popular NLP libraries for text processing\n",
        "\n",
        "**NLTK (Natural Language Toolkit):**\n",
        "\n",
        "NLTK is a powerful library for working with human language data. It provides easy-to-use interfaces to linguistic resources and algorithms, making it an excellent tool for natural language processing, text analysis, and machine learning. NLTK includes functionalities for tokenization, stemming, tagging, parsing, and more.\n",
        "\n",
        "**spaCy:**\n",
        "\n",
        "spaCy is a modern natural language processing library that is designed for efficiency and production use. It excels in processing large volumes of text quickly and accurately. spaCy provides pre-trained models for various languages, covering tasks such as part-of-speech tagging, named entity recognition, and dependency parsing.\n",
        "\n",
        "**Part-of-Speech (PoS) Tagging Example:**\n",
        "\n",
        "Let's demonstrate a simple PoS tagging example using NLTK and spaCy:\n",
        "\n",
        "\n",
        "\n",
        "In this example:\n",
        "- NLTK is used for tokenization and PoS tagging.\n",
        "- spaCy is used for PoS tagging.\n",
        "\n",
        "Both libraries provide information about the part of speech of each word in the given sentence. Feel free to run this code to see the PoS tagging results."
      ],
      "metadata": {
        "id": "BIedJlTV313Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK PoS Tagging Example\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"NLTK is a powerful library for natural language processing.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print(\"NLTK PoS Tagging:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# spaCy PoS Tagging Example\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(sentence)\n",
        "\n",
        "spacy_pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "print(\"\\nspaCy PoS Tagging:\")\n",
        "print(spacy_pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XheUYjNZ4a20",
        "outputId": "980c3f14-1864-4088-b1ce-c110410190f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK PoS Tagging:\n",
            "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n",
            "\n",
            "spaCy PoS Tagging:\n",
            "[('NLTK', 'PROPN'), ('is', 'AUX'), ('a', 'DET'), ('powerful', 'ADJ'), ('library', 'NOUN'), ('for', 'ADP'), ('natural', 'ADJ'), ('language', 'NOUN'), ('processing', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use these libraries for text pre-processing and condcuting layer-wise processing that we discussed in week 1 and 2."
      ],
      "metadata": {
        "id": "INr9iTrn4iwu"
      }
    }
  ]
}