{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS7SPvSpoyFw"
      },
      "source": [
        "# Lab2: Loading and Navigating through Text Corpora and PDF files, Analyzing patterns using Regular Expressions, NLTK and SpaCY library introduction\n",
        "\n",
        "Note: this is a lab session **graded**. Complete all exercises and upload to Canvas under **Lab 2: PDF Files, Datasets, RegEx and more** (https://utexas.instructure.com/courses/1382133/assignments/6619547?module_item_id=13585840) by no later than **01/25/2024, 11:59 PM** (Labs are supposed to be completed inside class).\n",
        "\n",
        "\n",
        "**Also Note:** The first take home assignment - \"Assignment 1: Regular Expressions based Pattern Extraction on PDF data\" will be posted tonight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqVjwQv0rA4s"
      },
      "source": [
        "##1. Processing PDF files and extracting Textual Data\n",
        "\n",
        "By now, we are familiar with the basics of reading and writing simple text files (with a `.txt` extension). However, before delving deeper into advanced text processing techniques, it's crucial to understand how to automatically read and process various file formats. In this context, let's explore PDF file processing, complementing our knowledge of processing web pages using BeautifulSoup and related libraries (previously discussed in Lab1). PDFs and web pages stand out as significant sources of data, collectively presenting vast amounts of text data for text mining endeavors.\n",
        "\n",
        "Before you execute the example code below, please download the sample PDF file from **https://www.fdic.gov/news/events/affordable/hcachecklist.pdf** and place it under **Files**. You can, of course, first open the file locally and examine the content.\n",
        "\n",
        "For reading PDF content, we will need to use a specialized library *PyPDF2* which may not be installed in your environment.\n",
        "\n",
        "Let's launch the installation command fist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz8K8ZDMuZxR",
        "outputId": "f1c72c93-17c3-41ab-94bf-15961c838f34"
      },
      "outputs": [],
      "source": [
        "%pip install PyPDF2\n",
        "%pip install nltk\n",
        "%pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5EOl0TGp3jN",
        "outputId": "8e88b701-d69b-4f7a-aa37-d725d3964772"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Sample PDF file path\n",
        "f = open(r'hcachecklist.pdf', mode='rb')\n",
        "pdfdoc = PyPDF2.PdfReader(f)\n",
        "\n",
        "page_data = []\n",
        "for page in pdfdoc.pages:\n",
        "  text = page.extract_text()\n",
        "  page_data.append(text)\n",
        "\n",
        "print(\"Text extracted using PyPDF2: \\n\", page_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_gidP_3u1xn"
      },
      "source": [
        "Now, this converts text data from each page into an element in the list `page_data`. We can now use this data for downstream processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PcLv4MKvMiW"
      },
      "source": [
        "### Exercise E1: Fetch all URLs mentioned in a few research papers.\n",
        "\n",
        "1. Download this conference proceeding and place it under Files. URL: https://aclanthology.org/2023.acl-long.0.pdf\n",
        "\n",
        "2. Process the PDF file using code similar to above.\n",
        "\n",
        "3. Extract all URLs and print (Hint: You can assume that any sentence, after splitting the text with newline symbol `\\n`, containing \"https\" qualifies as a URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyPDF is imported\n",
        "\n",
        "# open pdf file\n",
        "de_file = open(r'2023.acl-long.0.pdf', mode='rb')\n",
        "dedoc = PyPDF2.PdfReader(de_file)\n",
        "\n",
        "la_data = []\n",
        "for page in dedoc.pages:\n",
        "  text = page.extract_text()\n",
        "  la_data.append(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1https://github.com/acl-org/acl-2023-materialsxl\n",
            "4https://2023.aclweb.org/committees/program/xli\n",
            "10https://aclrollingreview.org/reviewertutorialxlv\n",
            "11https://2023.aclweb.org/blog/reviewer-assignment/xlvi\n",
            "13https://www.sphinx-doc.org/\n",
            "14https://myst-parser.readthedocs.io/xlix\n",
            "40https://2023.aclweb.org/program/best_reviewerslxix\n",
            "41https://2023.aclweb.org/blog/visa-info/lxxiii\n"
          ]
        }
      ],
      "source": [
        "for line in la_data:\n",
        "  lis = line.split(\"\\n\")\n",
        "  for lane in lis:\n",
        "    if \"https\" in lane:\n",
        "      print(lane)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVEU0RkOwjZI"
      },
      "source": [
        "## 2. Navigating through existing Public Datasets\n",
        "\n",
        "Like we discussed in our last lecture, several datasets are availabe for public use and Huggingface (http://huggingface.co) is a hub that hosts a few datasets, processed and readily available for building NLP applicaitons. Let's check some datasets below.\n",
        "\n",
        "Datasets from Huggingface are made available through a library called `datasets`. We have to install it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LusWaofbxP0r",
        "outputId": "00720c56-decd-4468-dc4a-9b4516595121"
      },
      "outputs": [],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZU8Ne0oxDaO",
        "outputId": "659071f4-24cd-4411-fa0d-e5c1fc9926cd"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load IMDb reviews dataset as an example. This is where the data is hosted: https://huggingface.co/datasets/imdb\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Get a sample text from the \"train\" split of the dataset\n",
        "sample_text = dataset[\"train\"][0]\n",
        "print(\"Sample Text:\\n\", sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXInOvhlx-gr"
      },
      "source": [
        "## Exercise E2: Print samples from the following datasets\n",
        "\n",
        "In separate code blocks\n",
        "1. Try to print a few sample sentences from the `spanish_billion_words` corpus,  the `simple_questions_v2` corpus.\n",
        "\n",
        "2. Be sure to visit huggingface website, type these corpora names in the search box, look at the data format and modify the code above to get the textual elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the spanish_billion_words dataset\n",
        "dataset2 = load_dataset(\"spanish_billion_words\")\n",
        "sample_text2 = dataset2[\"train\"][0]\n",
        "print(\"Sample Text:\\n\", sample_text2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "My virtual workspace has no space and its too computationally expensive to run locally and colab says 5 hours to compile so... idk I tested the other one and it worked so ig this one works as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Text:\n",
            " {'id': '0', 'subject_entity': 'www.freebase.com/m/04whkz5', 'relationship': 'www.freebase.com/book/written_work/subjects', 'object_entity': 'www.freebase.com/m/01cj3p', 'question': 'what is the book e about\\n'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for simple_questions_v2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/simple_questions_v2\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the simple_questions_v2 dataset\n",
        "dataset3 = load_dataset(\"simple_questions_v2\")\n",
        "sample_text3 = dataset3[\"train\"][0]\n",
        "print(\"Sample Text:\\n\", sample_text3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhAm39hCzH70"
      },
      "source": [
        "##3. Basic Regular Expression Patterns\n",
        "\n",
        "Regular expressions, often abbreviated as regex or regexp, are sequences of characters that define a search pattern. They are powerful tools for string manipulation and matching within text. Regular expressions provide a concise and flexible means to:\n",
        "  \n",
        "- **Search Patterns:** Specify patterns of characters to search for in a text.\n",
        "- **Match Patterns:** Identify whether a string conforms to a given pattern.\n",
        "- **Extract Information:** Extract specific data from strings based on defined patterns.\n",
        "- **Replace Patterns:** Replace occurrences of a pattern with a specified replacement.\n",
        "\n",
        "For a detailed overview on Python RegEx, please follow: https://www.geeksforgeeks.org/python-regex/\n",
        "\n",
        "Let's say we want to extract important information from text such as dates, emails, and phone numbers. We can use regular expressions to extract and display these patterns from the text as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8vVyrQ8zFuw",
        "outputId": "d7090944-a99f-4fc7-87a0-3691d6bb3421"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Sample text containing dates, emails, and phone numbers\n",
        "sample_text = \"Meeting on 2022-05-20, contact@example.com, and call me at 123-456-7890.\"\n",
        "\n",
        "# Regular expressions for extracting patterns\n",
        "dates = re.findall(r'\\d{4}-\\d{2}-\\d{2}', sample_text)\n",
        "emails = re.findall(r'[A-Za-z0-9]+@[A-Za-z0-9]+\\.[a-z]+', sample_text)\n",
        "phone_numbers = re.findall(r'\\d{3}-\\d{3}-\\d{4}', sample_text)\n",
        "\n",
        "print(\"Dates:\", dates)\n",
        "print(\"Emails:\", emails)\n",
        "print(\"Phone Numbers:\", phone_numbers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQNP7rfm2kBs"
      },
      "source": [
        "### Exercise E3:\n",
        "1. Following section 1, extract all text from `hcachecklist.pdf` (you can copy paste the code from #1). Now, list down all two word phrases that follow a pattern `a <word>`, `an <word>` and `the <word>`. Some example phrases: `a key`, `the document`.\n",
        "\n",
        "2. For this research paper here https://arxiv.org/abs/1706.03762 , programatically find out if there is any link to a codebase that can be used to replicate the paper (Hint: Load the text from the paper using PyPDF2, look for URLs, clean URLs and then filter out the ones that contain `github.com` in them. Print the final list of urls.\n",
        "\n",
        "3. **[Optional, not-graded]** Repeat 2 for all papers from EMNLP 2023 conference. You will have to scrape the website: https://aclanthology.org/events/emnlp-2023/#2023emnlp-main ,get URLs of all PDF papers, download the PDFs from within your program and repeate #2 for each of them. Dump the final list of GitHub URLs in a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the list', 'the document', 'the document', 'the doc', 'the documents', 'the ones', 'the \\ncompleted', 'a copy', 'the backs', 'an off', 'the key', 'a safe', 'a copy', 'a \\nfireproof', 'an emergency', 'an attorney', 'a copy', 'a sealed', 'the event', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____3', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a _____', 'a date', 'a small', 'an off', 'a secure']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# question 1\n",
        "page_str = \"\"\n",
        "for page in pdfdoc.pages:\n",
        "  text = page.extract_text()\n",
        "  text = text.lower()\n",
        "  page_str += text\n",
        "\n",
        "#print(page_str)\n",
        "two_word = re.findall(r'\\b((?:a|an|the)\\s+\\w+)\\b', page_str)\n",
        "# oh my goodness this took too long and required much looking at the g2g regex page to figure out but I think this is it\n",
        "print(two_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# question 2\n",
        "# open pdf file\n",
        "paper = open(r'1706.03762.pdf', mode='rb')\n",
        "dedoc = PyPDF2.PdfReader(paper)\n",
        "\n",
        "la_data = []\n",
        "for page in dedoc.pages:\n",
        "  text = page.extract_text()\n",
        "  la_data.append(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['avaswani@google.comNoam Shazeer∗',\n",
              " 'noam@google.comNiki Parmar∗',\n",
              " 'nikip@google.comJakob Uszkoreit∗',\n",
              " 'usz@google.com',\n",
              " 'llion@google.comAidan N. Gomez∗ †',\n",
              " 'lukaszkaiser@google.com',\n",
              " 'illia.polosukhin@gmail.com',\n",
              " 'The code we used to train and evaluate our models is available at https://github.com/']"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "urls = []\n",
        "for line in la_data:\n",
        "  lis = line.split(\"\\n\")\n",
        "  for lane in lis:\n",
        "    if \".com\" in lane or \".gov\" in lane or \".org\" in lane:\n",
        "      urls.append(lane)\n",
        "\n",
        "urls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIedJlTV313Q"
      },
      "source": [
        "## 4. Introduction to NLTK and SpaCY: Two popular NLP libraries for text processing\n",
        "\n",
        "**NLTK (Natural Language Toolkit):**\n",
        "\n",
        "NLTK is a powerful library for working with human language data. It provides easy-to-use interfaces to linguistic resources and algorithms, making it an excellent tool for natural language processing, text analysis, and machine learning. NLTK includes functionalities for tokenization, stemming, tagging, parsing, and more.\n",
        "\n",
        "**spaCy:**\n",
        "\n",
        "spaCy is a modern natural language processing library that is designed for efficiency and production use. It excels in processing large volumes of text quickly and accurately. spaCy provides pre-trained models for various languages, covering tasks such as part-of-speech tagging, named entity recognition, and dependency parsing.\n",
        "\n",
        "**Part-of-Speech (PoS) Tagging Example:**\n",
        "\n",
        "Let's demonstrate a simple PoS tagging example using NLTK and spaCy:\n",
        "\n",
        "\n",
        "\n",
        "In this example:\n",
        "- NLTK is used for tokenization and PoS tagging.\n",
        "- spaCy is used for PoS tagging.\n",
        "\n",
        "Both libraries provide information about the part of speech of each word in the given sentence. Feel free to run this code to see the PoS tagging results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XheUYjNZ4a20",
        "outputId": "980c3f14-1864-4088-b1ce-c110410190f7"
      },
      "outputs": [],
      "source": [
        "# NLTK PoS Tagging Example\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"NLTK is a powerful library for natural language processing.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print(\"NLTK PoS Tagging:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# spaCy PoS Tagging Example\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(sentence)\n",
        "\n",
        "spacy_pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "print(\"\\nspaCy PoS Tagging:\")\n",
        "print(spacy_pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INr9iTrn4iwu"
      },
      "source": [
        "We will use these libraries for text pre-processing and condcuting layer-wise processing that we discussed in week 1 and 2."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
