{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS7SPvSpoyFw"
      },
      "source": [
        "# Lab2: Loading and Navigating through Text Corpora and PDF files, Analyzing patterns using Regular Expressions, NLTK and SpaCY library introduction\n",
        "\n",
        "Note: this is a lab session **graded**. Complete all exercises and upload to Canvas under **Lab 2: PDF Files, Datasets, RegEx and more** (https://utexas.instructure.com/courses/1382133/assignments/6619547?module_item_id=13585840) by no later than **01/25/2024, 11:59 PM** (Labs are supposed to be completed inside class).\n",
        "\n",
        "\n",
        "**Also Note:** The first take home assignment - \"Assignment 1: Regular Expressions based Pattern Extraction on PDF data\" will be posted tonight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqVjwQv0rA4s"
      },
      "source": [
        "##1. Processing PDF files and extracting Textual Data\n",
        "\n",
        "By now, we are familiar with the basics of reading and writing simple text files (with a `.txt` extension). However, before delving deeper into advanced text processing techniques, it's crucial to understand how to automatically read and process various file formats. In this context, let's explore PDF file processing, complementing our knowledge of processing web pages using BeautifulSoup and related libraries (previously discussed in Lab1). PDFs and web pages stand out as significant sources of data, collectively presenting vast amounts of text data for text mining endeavors.\n",
        "\n",
        "Before you execute the example code below, please download the sample PDF file from **https://www.fdic.gov/news/events/affordable/hcachecklist.pdf** and place it under **Files**. You can, of course, first open the file locally and examine the content.\n",
        "\n",
        "For reading PDF content, we will need to use a specialized library *PyPDF2* which may not be installed in your environment.\n",
        "\n",
        "Let's launch the installation command fist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz8K8ZDMuZxR",
        "outputId": "f1c72c93-17c3-41ab-94bf-15961c838f34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5EOl0TGp3jN",
        "outputId": "8e88b701-d69b-4f7a-aa37-d725d3964772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text extracted using PyPDF2: \n",
            " ['1 of 4 \\n \\n 888-388-HOPE (4673) \\nwww.hopecoalitionamerica.org\\n CHECKLIST OF IMPORTANT LEGAL DOCUMENTS \\nAND FINANCIAL STATEMENTS \\n \\nPlease review the list of important documents below  and check whether you have the document, whether \\nyou need to obtain the document or whether the doc ument does not apply to your household. Next, \\ncollect the documents you have and obtain the ones you still need. These documents, along with the \\ncompleted forms provided here, make up your  Emergency Financial First Aid Kit (EFFAK). \\nOnce you have all of these documents together, y ou should make a copy of your entire EFFAK. As \\nimportant information is often printed on the backs of  these documents, please be sure to copy both \\nsides. \\nBecause these documents contain such important and  personal information, we strongly recommend that \\nyou keep all original documents, photographs and co mputer backup disks in an off-site safety deposit \\nbox.  And be sure to keep the key to your safety deposit box in a safe place too! \\nIn addition, keep a copy of your EFFAK and copi es of your original documents at home in a \\nfireproof/waterproof metal box or safe. Because ATMs do  not work when electricity is out or they may not \\nbe restocked during an emergency, be sure to include some $10 and $20 bills in your storage box.  \\nIf you have an attorney, you may also want to provid e them with a copy of your EFFAK in a sealed \\nenvelope to be opened with your approval, or in the event you become incapacitated.  \\nIMPORTANT  LEGAL DOCUMENTS  THAT APPLY TO MY FAMILY \\n \\n1. Birth Certificate(s)/Adoption Papers \\n2. Marriage License  \\n3. Divorce Papers  \\n4. Social Security Card(s) \\n5. Passport/Green Card  \\n6. Naturalization Documents \\n7. Will \\n8. Power(s) of Attor ney (personal/property) \\n9. Mortgage or Real Estate Deeds of Trust \\n10. Vehicle Registration/Ownership Papers \\n11. ID/Driver’s License  \\n1. have _____need _____N/A _____  2. have _____need _____N/A _____  \\n3. have _____need _____N/A _____  \\n4. have _____need _____N/A _____  5. have _____need _____N/A _____  \\n6. have _____need _____N/A _____  \\n7. have _____need _____N/A _____  \\n8. have _____need _____N/A _____  \\n9. have _____need _____N/A _____  \\n10. have _____need _____N/A _____  \\n11. have _____need _____N/A _____ ', '2 of 4 \\n \\n 888-388-HOPE (4673) \\nwww.hopecoalitionamerica.org\\n TAX STATEMENTS  \\n12. Previous Year’s Tax Returns \\n13. Property Tax Statement 14. Personal Property Tax (i.e. Car Tax) \\n \\nF\\nINANCIAL  ACCOUNTS  \\n15. Bank/Credit Union Statements \\n16. Credit/Debit Card Statements \\n17. Retirement Accounts (401K, TSP, IRA) 18. Investment Accounts (S tocks, Bonds, Mutual \\nFunds) \\n19. Other      \\n \\n \\nSOURCES  OF INCOME /ASSETS  \\n20. Recent Pay Stubs for All Sources of Income \\n21.  Government Benefits (e.g. Social Security, \\nTemporary Assistance for Needy Families, Veterans’) \\n22. Alimony Income \\n23. Child Support Income \\n24. Professional Appraisals of Personal Property \\n25.  Rewards Accounts (e.g., Frequent Flyer Programs, \\nHotel Rewards) \\n26. Other       \\n \\nFINANCIAL  OBLIGATIONS  \\n27. Mortgage Statement \\n28. Lease \\n29. Utility Bills (Elect ric, Water, Gas) \\n30. Car Payment \\n31. Student Loan \\n32. Alimony Payments \\n33. Child Support Payments \\n34. Elder Care Facilities \\n35. Other Debt        \\n12. have _____need _____N/A _____  13. have _____need _____N/A _____  \\n14. have _____need _____N/A _____  \\n \\n \\n15. have _____need _____N/A _____  \\n16. have _____need _____N/A _____  \\n17. have _____need _____N/A _____  \\n18. have _____need _____N/A _____  \\n \\n19. have _____need _____N/A _____  \\n \\n \\n \\n20. have _____need _____N/A _____  \\n21. have _____need _____N/A _____ \\n \\n \\n22. have _____need _____N/A _____  \\n23. have _____need _____N/A _____  \\n24. have _____need _____N/A _____  \\n25. have _____need _____N/A _____  \\n \\n26. have _____need _____N/A _____  \\n \\n  \\n27. have _____need _____N/A _____ \\n28. have _____need _____N/A _____  \\n29. have _____need _____N/A _____ \\n30. have _____need _____N/A _____ \\n31. have _____need _____N/A _____ \\n32. have _____need _____N/A _____ \\n33. have _____need _____N/A _____ \\n34. have _____need _____N/A _____ \\n35. have _____need _____N/A _____', '3 of 4 \\n \\n 888-388-HOPE (4673) \\nwww.hopecoalitionamerica.org\\n TAX STATEMENTS  \\n36. Property Insurance \\n37. Rental Insurance 38. Auto Insurance \\n39. Life Insurance \\n40. Other      \\n \\n \\nMEDICAL  \\n41. Health Insurance ID Card (s) \\n42. Record of Immunizations/Allergies \\n43. List of Necessary Medications \\n44. Disabilities Documentation \\n45. Living Will \\n46. Dental Records / Child Identity Cards / DNA Swabs \\n47. Other       \\n \\nMILITARY  \\n48. Current Military ID \\n49. Military Discharge DD 214 \\n50. Other       \\n \\nOTHER FINANCIAL /LEGAL DOCUMENTATION  \\n5 1 .          \\n5 2 .          \\n5 3 .          \\n36. have _____need _____N/A _____  \\n37. have _____need _____N/A _____  \\n38. have _____need _____N/A _____  \\n39. have _____need _____N/A _____  \\n40. have _____need _____N/A _____  \\n \\n  \\n41. have _____need _____N/A _____  \\n42. have _____need _____N/A _____  \\n43. have _____need _____N/A _____  \\n44. have _____need _____N/A _____  \\n45. have _____need _____N/A _____  \\n46. have _____need _____N/A _____  \\n47. have _____need _____N/A _____  \\n \\n \\n48. have _____need _____N/A _____ \\n49. have _____need _____N/A _____  \\n50. have _____need _____N/A _____ \\n \\n  \\n51. have _____need _____N/A _____ \\n52. have _____need _____N/A _____ \\n53. have _____need _____N/A _____ \\n ', ' \\n4 of 4 \\n \\n 888-388-HOPE (4673) \\nwww.hopecoalitionamerica.org\\n  \\nWe suggest you include a date on each line as you collect and obtain your necessary documents.  This \\nwill help you track your progress  as you work toward  preparing your househ old finances for any \\nunanticipated emergency.  \\nIMPORTANT : If you are a small business owner, you should make sure that you safeguard your business \\nfinances as well: backup computer files routinely, keep original of critical document in an off-site safety \\ndeposit box and keep copies in a secure fireproof, waterproof container on site. \\n ']\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Sample PDF file path\n",
        "f = open(r'hcachecklist.pdf', mode='rb')\n",
        "pdfdoc = PyPDF2.PdfReader(f)\n",
        "\n",
        "page_data = []\n",
        "for page in pdfdoc.pages:\n",
        "  text = page.extract_text()\n",
        "  page_data.append(text)\n",
        "\n",
        "print(\"Text extracted using PyPDF2: \\n\", page_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_gidP_3u1xn"
      },
      "source": [
        "Now, this converts text data from each page into an element in the list `page_data`. We can now use this data for downstream processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PcLv4MKvMiW"
      },
      "source": [
        "### Exercise E1: Fetch all URLs mentioned in a few research papers.\n",
        "\n",
        "1. Download this conference proceeding and place it under Files. URL: https://aclanthology.org/2023.acl-long.0.pdf\n",
        "\n",
        "2. Process the PDF file using code similar to above.\n",
        "\n",
        "3. Extract all URLs and print (Hint: You can assume that any sentence, after splitting the text with newline symbol `\\n`, containing \"https\" qualifies as a URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyPDF is imported\n",
        "\n",
        "# open pdf file\n",
        "de_file = open(r'2023.acl-long.0.pdf', mode='rb')\n",
        "dedoc = PyPDF2.PdfReader(de_file)\n",
        "\n",
        "la_data = []\n",
        "for page in dedoc.pages:\n",
        "  text = page.extract_text()\n",
        "  la_data.append(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1https://github.com/acl-org/acl-2023-materialsxl\n",
            "4https://2023.aclweb.org/committees/program/xli\n",
            "10https://aclrollingreview.org/reviewertutorialxlv\n",
            "11https://2023.aclweb.org/blog/reviewer-assignment/xlvi\n",
            "13https://www.sphinx-doc.org/\n",
            "14https://myst-parser.readthedocs.io/xlix\n",
            "40https://2023.aclweb.org/program/best_reviewerslxix\n",
            "41https://2023.aclweb.org/blog/visa-info/lxxiii\n"
          ]
        }
      ],
      "source": [
        "for line in la_data:\n",
        "  lis = line.split(\"\\n\")\n",
        "  for lane in lis:\n",
        "    if \"https\" in lane:\n",
        "      print(lane)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVEU0RkOwjZI"
      },
      "source": [
        "## 2. Navigating through existing Public Datasets\n",
        "\n",
        "Like we discussed in our last lecture, several datasets are availabe for public use and Huggingface (http://huggingface.co) is a hub that hosts a few datasets, processed and readily available for building NLP applicaitons. Let's check some datasets below.\n",
        "\n",
        "Datasets from Huggingface are made available through a library called `datasets`. We have to install it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LusWaofbxP0r",
        "outputId": "00720c56-decd-4468-dc4a-9b4516595121"
      },
      "outputs": [],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZU8Ne0oxDaO",
        "outputId": "659071f4-24cd-4411-fa0d-e5c1fc9926cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading readme: 100%|██████████| 7.81k/7.81k [00:00<00:00, 17.1MB/s]\n",
            "Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 27.0MB/s]\n",
            "Downloading data: 100%|██████████| 20.5M/20.5M [00:00<00:00, 33.6MB/s]\n",
            "Downloading data: 100%|██████████| 42.0M/42.0M [00:00<00:00, 43.0MB/s]\n",
            "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 97931.13 examples/s] \n",
            "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 171365.91 examples/s]\n",
            "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 223235.93 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Text:\n",
            " {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load IMDb reviews dataset as an example. This is where the data is hosted: https://huggingface.co/datasets/imdb\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Get a sample text from the \"train\" split of the dataset\n",
        "sample_text = dataset[\"train\"][0]\n",
        "print(\"Sample Text:\\n\", sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXInOvhlx-gr"
      },
      "source": [
        "## Exercise E2: Print samples from the following datasets\n",
        "\n",
        "In separate code blocks\n",
        "1. Try to print a few sample sentences from the `spanish_billion_words` corpus,  the `simple_questions_v2` corpus.\n",
        "\n",
        "2. Be sure to visit huggingface website, type these corpora names in the search box, look at the data format and modify the code above to get the textual elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for spanish_billion_words contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/spanish_billion_words\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Generating train split:  10%|█         | 4709479/46925295 [03:41<33:02, 21296.62 examples/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load IMDb reviews dataset as an example. This is where the data is hosted: https://huggingface.co/datasets/imdb\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset2 \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspanish_billion_words\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dataset3 \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple_questions_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded :)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/load.py:2549\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2546\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2549\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2559\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2560\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2561\u001b[0m )\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1005\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1767\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1767\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1100\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1107\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1605\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1603\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1606\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1607\u001b[0m     ):\n\u001b[1;32m   1608\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1609\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1744\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     writer \u001b[38;5;241m=\u001b[39m writer_class(\n\u001b[1;32m   1735\u001b[0m         features\u001b[38;5;241m=\u001b[39mwriter\u001b[38;5;241m.\u001b[39m_features,\n\u001b[1;32m   1736\u001b[0m         path\u001b[38;5;241m=\u001b[39mfpath\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSSSS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJJJJJ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         embed_local_files\u001b[38;5;241m=\u001b[39membed_local_files,\n\u001b[1;32m   1742\u001b[0m     )\n\u001b[1;32m   1743\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mencode_example(record) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m record\n\u001b[0;32m-> 1744\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/arrow_writer.py:476\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[0;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# Utilize the keys and duplicate checking when `self._check_duplicates` is passed True\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_duplicates:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Create unique hash from key and store as (key, example) pairs\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28mhash\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hasher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples\u001b[38;5;241m.\u001b[39mappend((example, \u001b[38;5;28mhash\u001b[39m))\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# Maintain record of keys and their respective hashes for checking duplicates\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/keyhash.py:101\u001b[0m, in \u001b[0;36mKeyHasher.hash\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns 128-bits unique hash of input key\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mkey: the input key to be hashed (should be str, int or bytes)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mReturns: 128-bit int hash key\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m md5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_md5\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 101\u001b[0m byte_key \u001b[38;5;241m=\u001b[39m \u001b[43m_as_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m md5\u001b[38;5;241m.\u001b[39mupdate(byte_key)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Convert to integer with hexadecimal conversion\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/keyhash.py:53\u001b[0m, in \u001b[0;36m_as_bytes\u001b[0;34m(hash_data)\u001b[0m\n\u001b[1;32m     51\u001b[0m     hash_data \u001b[38;5;241m=\u001b[39m hash_data\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hash_data, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 53\u001b[0m     hash_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhash_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# If data is not of the required type, raise error\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidKeyError(hash_data)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load IMDb reviews dataset as an example. This is where the data is hosted: https://huggingface.co/datasets/imdb\n",
        "dataset2 = load_dataset(\"spanish_billion_words\")\n",
        "dataset3 = load_dataset(\"simple_questions_v2\")\n",
        "print(\"loaded :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhAm39hCzH70"
      },
      "source": [
        "##3. Basic Regular Expression Patterns\n",
        "\n",
        "Regular expressions, often abbreviated as regex or regexp, are sequences of characters that define a search pattern. They are powerful tools for string manipulation and matching within text. Regular expressions provide a concise and flexible means to:\n",
        "  \n",
        "- **Search Patterns:** Specify patterns of characters to search for in a text.\n",
        "- **Match Patterns:** Identify whether a string conforms to a given pattern.\n",
        "- **Extract Information:** Extract specific data from strings based on defined patterns.\n",
        "- **Replace Patterns:** Replace occurrences of a pattern with a specified replacement.\n",
        "\n",
        "For a detailed overview on Python RegEx, please follow: https://www.geeksforgeeks.org/python-regex/\n",
        "\n",
        "Let's say we want to extract important information from text such as dates, emails, and phone numbers. We can use regular expressions to extract and display these patterns from the text as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8vVyrQ8zFuw",
        "outputId": "d7090944-a99f-4fc7-87a0-3691d6bb3421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dates: ['2022-05-20']\n",
            "Emails: ['contact@example.com']\n",
            "Phone Numbers: ['123-456-7890']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Sample text containing dates, emails, and phone numbers\n",
        "sample_text = \"Meeting on 2022-05-20, contact@example.com, and call me at 123-456-7890.\"\n",
        "\n",
        "# Regular expressions for extracting patterns\n",
        "dates = re.findall(r'\\d{4}-\\d{2}-\\d{2}', sample_text)\n",
        "emails = re.findall(r'[A-Za-z0-9]+@[A-Za-z0-9]+\\.[a-z]+', sample_text)\n",
        "phone_numbers = re.findall(r'\\d{3}-\\d{3}-\\d{4}', sample_text)\n",
        "\n",
        "print(\"Dates:\", dates)\n",
        "print(\"Emails:\", emails)\n",
        "print(\"Phone Numbers:\", phone_numbers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQNP7rfm2kBs"
      },
      "source": [
        "### Exercise E3:\n",
        "1. Following section 1, extract all text from `hcachecklist.pdf` (you can copy paste the code from #1). Now, list down all two word phrases that follow a pattern `a <word>`, `an <word>` and `the <word>`. Some example phrases: `a key`, `the document`.\n",
        "\n",
        "2. For this research paper here https://arxiv.org/abs/1706.03762 , programatically find out if there is any link to a codebase that can be used to replicate the paper (Hint: Load the text from the paper using PyPDF2, look for URLs, clean URLs and then filter out the ones that contain `github.com` in them. Print the final list of urls.\n",
        "\n",
        "3. **[Optional, not-graded]** Repeat 2 for all papers from EMNLP 2023 conference. You will have to scrape the website: https://aclanthology.org/events/emnlp-2023/#2023emnlp-main ,get URLs of all PDF papers, download the PDFs from within your program and repeate #2 for each of them. Dump the final list of GitHub URLs in a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIedJlTV313Q"
      },
      "source": [
        "## 4. Introduction to NLTK and SpaCY: Two popular NLP libraries for text processing\n",
        "\n",
        "**NLTK (Natural Language Toolkit):**\n",
        "\n",
        "NLTK is a powerful library for working with human language data. It provides easy-to-use interfaces to linguistic resources and algorithms, making it an excellent tool for natural language processing, text analysis, and machine learning. NLTK includes functionalities for tokenization, stemming, tagging, parsing, and more.\n",
        "\n",
        "**spaCy:**\n",
        "\n",
        "spaCy is a modern natural language processing library that is designed for efficiency and production use. It excels in processing large volumes of text quickly and accurately. spaCy provides pre-trained models for various languages, covering tasks such as part-of-speech tagging, named entity recognition, and dependency parsing.\n",
        "\n",
        "**Part-of-Speech (PoS) Tagging Example:**\n",
        "\n",
        "Let's demonstrate a simple PoS tagging example using NLTK and spaCy:\n",
        "\n",
        "\n",
        "\n",
        "In this example:\n",
        "- NLTK is used for tokenization and PoS tagging.\n",
        "- spaCy is used for PoS tagging.\n",
        "\n",
        "Both libraries provide information about the part of speech of each word in the given sentence. Feel free to run this code to see the PoS tagging results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XheUYjNZ4a20",
        "outputId": "980c3f14-1864-4088-b1ce-c110410190f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK PoS Tagging:\n",
            "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# spaCy PoS Tagging Example\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(sentence)\n\u001b[1;32m     20\u001b[0m spacy_pos_tags \u001b[38;5;241m=\u001b[39m [(token\u001b[38;5;241m.\u001b[39mtext, token\u001b[38;5;241m.\u001b[39mpos_) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "# NLTK PoS Tagging Example\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"NLTK is a powerful library for natural language processing.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print(\"NLTK PoS Tagging:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# spaCy PoS Tagging Example\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(sentence)\n",
        "\n",
        "spacy_pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "print(\"\\nspaCy PoS Tagging:\")\n",
        "print(spacy_pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INr9iTrn4iwu"
      },
      "source": [
        "We will use these libraries for text pre-processing and condcuting layer-wise processing that we discussed in week 1 and 2."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
