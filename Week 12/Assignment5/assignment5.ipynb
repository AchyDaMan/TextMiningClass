{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achintya Yedavalli\n",
    "\n",
    "# Assignment 5: Text Classification\n",
    "\n",
    "We are going work on the task of fake news detection from Tweets, i.e.,given a Tweet related Covid19, classify whether the tweet is fake or not. You are given two data files: `FakeNews_train.csv` and `FakeNews_test.csv`. These files contain two columns, \"tweet\" and \"label,\" where \"label\" indicates whether a tweet is \"real\" or \"fake\". Your task is to explore machine learning-based models to select the best model for classifying fake news tweets.\n",
    "\n",
    "## 1. Data Preprocessing\n",
    "\n",
    "### Load the raw training & test datasets from the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>• Businesses and offices can reopen for staff ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @CDCDirector: We do not know yet if the ant...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an image of a suspected coronavirus va...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We can’t forget that in the middle of a global...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#IndiaFightsCorona Focused and effective effor...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet label\n",
       "0  • Businesses and offices can reopen for staff ...  real\n",
       "1  RT @CDCDirector: We do not know yet if the ant...  real\n",
       "2  This is an image of a suspected coronavirus va...  fake\n",
       "3  We can’t forget that in the middle of a global...  fake\n",
       "4  #IndiaFightsCorona Focused and effective effor...  real"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load de data\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"FakeNews_train.csv\")\n",
    "test = pd.read_csv(\"FakeNews_test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text data by removing URLs, hashtags, and mentions.\n",
    "\n",
    "If the file exists though we are skipping the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning time!\n",
    "# we use regex for this\n",
    "import re\n",
    "# credit to https://www.geeksforgeeks.org/remove-urls-from-string-in-python/\n",
    "def remove_non_english(text):\n",
    "    # Define a regex pattern to find\n",
    "    pattern = re.compile(r\"https?://\\S+|(?<=\\s)[@#]|^[@#]|[^a-zA-Z0-9\\s]\")\n",
    "\n",
    "    # Use the sub() method to replace\n",
    "    text_without_noneg = pattern.sub(\"\", text)\n",
    "\n",
    "    return text_without_noneg\n",
    "\n",
    "train_clean = []\n",
    "test_clean = []\n",
    "for line in train[\"tweet\"]:\n",
    "  train_clean.append(remove_non_english(line))\n",
    "\n",
    "for line in test[\"tweet\"]:\n",
    "  test_clean.append(remove_non_english(line))\n",
    "\n",
    "# make my life easier by putting back clean into the original dfs\n",
    "train[\"tweet\"] = train_clean\n",
    "test[\"tweet\"] = test_clean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text data.\n",
    "\n",
    "### Remove stop words and lemmatize the text data.\n",
    "\n",
    "(doing c and d together because it's easier)\n",
    "\n",
    "(also if file exists we are skipping preprocessing altogether)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#tokenize data\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nlp_pipeline = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# get a list of stopwords from NLTK\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def pre_process_a_single_sentence(sentence: str):\n",
    "    # Lower case text\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    processed_sentence = []\n",
    "\n",
    "    # Tokenize, and lemmatize the text\n",
    "    doc = nlp_pipeline(sentence)\n",
    "\n",
    "    for token in doc:\n",
    "    # here token is an object that contains various information about each token\n",
    "    # information such as lemma, pos, parse labels are available\n",
    "    # we will check here if tokens are present in stopwords; if not, we will retain their lemma\n",
    "        if token not in stops:\n",
    "            lemmatized_token = token.lemma_\n",
    "            processed_sentence.append(lemmatized_token)\n",
    "        processed_sentence = \" \".join (processed_sentence)\n",
    "        return processed_sentence\n",
    "\n",
    "\n",
    "train_clean = []\n",
    "test_clean = []\n",
    "\n",
    "# run the pre-processing\n",
    "for tweet in train[\"tweet\"]:\n",
    "    train_clean.append(pre_process_a_single_sentence(tweet))\n",
    "\n",
    "for tweet in test[\"tweet\"]:\n",
    "    test_clean.append(pre_process_a_single_sentence(tweet))\n",
    "\n",
    "train[\"tweet\"] = train_clean\n",
    "test[\"tweet\"] = test_clean\n",
    "\n",
    "train_clean = None\n",
    "test_clean = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the processed data into 2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"FakeNews_train_preprocessed.csv\")\n",
    "test.to_csv(\"FakeNews_test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining ML models\n",
    "\n",
    "### define three ML models: \n",
    "\n",
    "(a) LogisticRegression \n",
    "\n",
    "(b) SVC \n",
    "\n",
    "(c) MLPClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## 2. Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "## 3. Feed forward neural network or multi-layered perceptron (MLPClassifier)\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring  Basic Text Features\n",
    "\n",
    "### For each example, extract TF-IDF features with max_features set to 5000.\n",
    "\n",
    "### Train and evaluate all thee ML models you defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression = 71.26168224299066%\n",
      "Accuracy of Support Vector Classification = 71.26168224299066%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLP Classification = 71.02803738317756%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# compute \"goodness\" of classification through accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract text and labels\n",
    "X_train = train['tweet']\n",
    "y_train = train['label']\n",
    "X_test = test['tweet']\n",
    "y_test = test['label']\n",
    "\n",
    "# generic training function\n",
    "def train_and_evaluate_classifier(classifier, X_train, y_actual, X_test, y_test_actual):\n",
    "  classifier.fit(X_train, y_actual)\n",
    "  y_pred = classifier.predict(X_test)\n",
    "  accuracy = accuracy_score(y_test_actual, y_pred)\n",
    "  return accuracy\n",
    "\n",
    "\n",
    "# Create a CountVectorizer for unigrams (bag of words)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression\n",
    "classifier = LogisticRegression()\n",
    "accuracy = train_and_evaluate_classifier(classifier, X_train_vec, y_train, X_test_vec, y_test)\n",
    "print (f\"Accuracy of Logistic Regression = {accuracy*100}%\")\n",
    "\n",
    "# Train SVC\n",
    "classifier = SVC(kernel=\"linear\")\n",
    "accuracy = train_and_evaluate_classifier(classifier, X_train_vec, y_train, X_test_vec, y_test)\n",
    "print (f\"Accuracy of Support Vector Classification = {accuracy*100}%\")\n",
    "\n",
    "# Train MLPClassifier\n",
    "classifier = MLPClassifier(random_state=1)\n",
    "accuracy = train_and_evaluate_classifier(classifier, X_train_vec, y_train, X_test_vec, y_test)\n",
    "print (f\"Accuracy of MLP Classification = {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which model performs the best? For the best model, print 10 examples from the test data, the actual and predicted labels. \n",
    "\n",
    "The best-performing model is the Support Vector Classification model which is at ~92.835% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16285/1704556462.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_section['pred_label'] = y_pred.tolist()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coronavirusupdates</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>fake</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>real</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>there</td>\n",
       "      <td>fake</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>asian</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>there</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>madrids</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>both</td>\n",
       "      <td>real</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>there</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ryanair</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet label pred_label\n",
       "5   coronavirusupdates  real       real\n",
       "7                  the  fake       real\n",
       "9                   in  real       fake\n",
       "11               there  fake       real\n",
       "13               asian  fake       fake\n",
       "15               there  real       real\n",
       "17             madrids  fake       fake\n",
       "19                both  real       fake\n",
       "21               there  real       real\n",
       "23             ryanair  fake       fake"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print 10 examples from best fit model\n",
    "classifier = SVC(kernel=\"linear\")\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "sample_vec = X_test_vec[5:25:2]\n",
    "\n",
    "y_pred = classifier.predict(sample_vec)\n",
    "# append onto new dataframe\n",
    "df_section = test[5:25:2]\n",
    "df_section['pred_label'] = y_pred.tolist()\n",
    "\n",
    "df_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share observations and insights\n",
    "\n",
    "I think that the SVC method is really the best one for this type of work; real/fake classification is not something that needs a complicated Feed Forward Neural Network, and logistic regression is really breaking at 5000 features, so SVC is the clear winner for this round.\n",
    "\n",
    "## 4. Exploring Averaged Word Embeddings as Features (2 points)\n",
    "\n",
    "### For each example, extract average word embeddings using pertained word2vec model. \n",
    "\n",
    "Follow Week 11's tutorial (Lab11_Word_and_sentence_embeddings.ipynb) for extracting word embeddings and averaging them to form sentence embeddings for each example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract average word embeddings\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained Word2Vec model, or Download a pre-trained word2vec (trained on Google News data)\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec features shape: (5136, 300)\n"
     ]
    }
   ],
   "source": [
    "# Function to extract sentence vector from word vectors by averaging word embeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "p = 0\n",
    "def create_word2vec_embeddings(dataframe):\n",
    "    sentences = [text.split() for text in dataframe['tweet']]\n",
    "\n",
    "    # Average Word Vectors for each text (I gave up and asked chatgpt how to fix the code)\n",
    "    def document_vector(doc):\n",
    "        vectors = [w2v_model[w] for w in doc if w in w2v_model]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            # Return a zero vector if no valid word vectors found\n",
    "            return np.zeros_like(w2v_model['example'])\n",
    "\n",
    "\n",
    "    X_w2v = np.array([document_vector(text) for text in sentences if document_vector(text).shape != ()])\n",
    "    return X_w2v\n",
    "\n",
    "X_train_vec = create_word2vec_embeddings(train)\n",
    "X_test_vec = create_word2vec_embeddings(test)\n",
    "print(\"Word2Vec features shape:\", X_train_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Evaluate all the ML Models defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test Logistic Regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression = 68.22429906542055%\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter=1000)\n",
    "accuracy = train_and_evaluate_classifier(classifier, X_train_vec, y_train, X_test_vec, y_test)\n",
    "print (f\"Accuracy of Logistic Regression = {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Classification = 68.45794392523365%\n"
     ]
    }
   ],
   "source": [
    "classifier = SVC(kernel=\"linear\")\n",
    "accuracy = train_and_evaluate_classifier(classifier, X_train_vec, y_train, X_test_vec, y_test)\n",
    "print (f\"Accuracy of Support Vector Classification = {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLP Classification = 69.47040498442367%\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(random_state=1, max_iter=300)\n",
    "accuracy = train_and_evaluate_classifier(classifier, X_train_vec, y_train, X_test_vec, y_test)\n",
    "print (f\"Accuracy of MLP Classification = {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which model is the best? Print 10 results\n",
    "\n",
    "According to the accuracy reports of the three models, the feed forward network (MLPClassifier) is the most accurate model tested at 89.096%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16285/573733212.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_section['pred_label'] = y_pred.tolist()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>real</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>as</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>there</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>marvinbrite</td>\n",
       "      <td>fake</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>there</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>a</td>\n",
       "      <td>fake</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lesotho</td>\n",
       "      <td>fake</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>for</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>just</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tweet label pred_label\n",
       "9            in  real       fake\n",
       "12           as  real       real\n",
       "15        there  real       real\n",
       "18  marvinbrite  fake       real\n",
       "21        there  real       real\n",
       "24            a  fake       real\n",
       "27      lesotho  fake       real\n",
       "30               real       real\n",
       "33          for  real       real\n",
       "36         just  real       real"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print 10 examples from best fit model\n",
    "# MLP is already the classifier\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "sample_vec = X_test_vec[9:39:3]\n",
    "\n",
    "y_pred = classifier.predict(sample_vec)\n",
    "# append onto new dataframe\n",
    "df_section = test[9:39:3]\n",
    "df_section['pred_label'] = y_pred.tolist()\n",
    "\n",
    "df_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Sentence Transformer Embeddings as Features\n",
    "\n",
    "### For each example, extract sentence embeddings directly using sentence transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load a pre-trained BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') # changed to the regular name for a BERT model because Hugging Face crapped out lots of times\n",
    "\n",
    "train_embed = model.encode(train[\"tweet\"])\n",
    "test_embed = model.encode(test[\"tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test Logistic Regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression = 73.28660436137072%\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter=1000)\n",
    "accuracy = train_and_evaluate_classifier(classifier, train_embed, y_train, test_embed, y_test)\n",
    "print (f\"Accuracy of Logistic Regression = {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Classification = 73.75389408099689%\n"
     ]
    }
   ],
   "source": [
    "classifier = SVC(kernel=\"linear\")\n",
    "accuracy = train_and_evaluate_classifier(classifier, train_embed, y_train, test_embed, y_test)\n",
    "print (f\"Accuracy of Support Vector Classification = {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLP Classification = 73.20872274143302%\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(random_state=1, max_iter=300)\n",
    "accuracy = train_and_evaluate_classifier(classifier, train_embed, y_train, test_embed, y_test)\n",
    "print (f\"Accuracy of MLP Classification = {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which model is the best? Print 10 results\n",
    "\n",
    "According to the accuracy reports of the three models, the feed forward network (MLPClassifier) is the most accurate model tested at 90.81%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16285/1845448623.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_section['pred_label'] = y_pred.tolist()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>as</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>news</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>a</td>\n",
       "      <td>fake</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>the</td>\n",
       "      <td>fake</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>president</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>just</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>trump</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>rt</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>some</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet label pred_label\n",
       "12         as  real       real\n",
       "16       this  real       real\n",
       "20       news  fake       fake\n",
       "24          a  fake       real\n",
       "28        the  fake       real\n",
       "32  president  fake       fake\n",
       "36       just  real       real\n",
       "40      trump  fake       fake\n",
       "44         rt  real       real\n",
       "48       some  real       real"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print 10 examples from best fit model\n",
    "# MLP is already the classifier\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "sample_vec = X_test_vec[12:52:4]\n",
    "\n",
    "y_pred = classifier.predict(sample_vec)\n",
    "# append onto new dataframe\n",
    "df_section = test[12:52:4]\n",
    "df_section['pred_label'] = y_pred.tolist()\n",
    "\n",
    "df_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Across all testing, the combination of embedding and model is the SVC model with TF-IDF features. It may only be ~2% better than the BERT or the W2V embeddings, but it comes out as the highest in the end. This was unexpected because you normally assume that the higher complexity you go the more accurate you are, but I think that it mostly doesn't apply for a smaller task like this, where the data is only around 5000 rows or so. In the future a larger dataset might be more appreciated to work with, even if it might kill my computer lol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
