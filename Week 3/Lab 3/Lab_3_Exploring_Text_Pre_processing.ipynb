{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLDXIZvECEfP"
      },
      "source": [
        "# Lab 3: Exploring Text Pre-processing with NLTK\n",
        "\n",
        "Note: This lab is **NOT** graded. However, it is highly recommended that you solve the exercises which will help you in solving Assignment 2 (to be posted tonight).\n",
        "\n",
        "In the context of the tutorial, \"search\" refers to the process of querying and retrieving relevant information from a collection of textual documents. The goal is to demonstrate how different text pre-processing techniques can enhance the effectiveness of search queries. The term \"search\" is used in the context of searching for specific information or patterns within a set of documents, and the pre-processing steps aim to improve the accuracy and relevance of the search results.\n",
        "\n",
        "For pre-processing, we will use (a) basic Python funcitons for regular expressions (b) Natural Language Toolkit (NLTK).\n",
        "\n",
        "We start by installing NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsrJAM3jDb0_",
        "outputId": "5589689c-54b2-4de4-a780-5198363d87dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: spacy in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy) (1.26.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "%pip install spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DKXB6xHErbG"
      },
      "source": [
        "Let's create sample documents and queries. You can always replace this with a process that can ingest real documents (say from PDF files) and convert documents in to lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qy961pOpEl7o"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"Text pre-processing is essential for natural language processing.\",\n",
        "    \"NLTK and SpaCy are popular libraries for text analysis.\",\n",
        "    \"Clean and normalized text improves search accuracy.\",\n",
        "    \"Tokenization breaks text into words or phrases.\",\n",
        "    \"Stemming reduces words to their root form.\",\n",
        "    \"Lemmatization provides the base or dictionary form of a word.\",\n",
        "    \"Stop words removal eliminates common words.\",\n",
        "    \"Hunpos helps correct spelling errors.\",\n",
        "    \"Python is widely used in natural language processing.\",\n",
        "    \"SpaCy's lemmatization is more advanced than NLTK's.\"\n",
        "]\n",
        "\n",
        "search_queries = [\n",
        "    \"text analyis\",\n",
        "    \"tokenization\",\n",
        "    \"lemmatization in Python\",\n",
        "    \"natural languge processing tools\",\n",
        "    \"reducing word\",\n",
        "    \"nltk and spacy\",\n",
        "    \"breaking words\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8r2IOYFGAq-"
      },
      "source": [
        "# 1. Implementing a basic search algorithm\n",
        "\n",
        "Let's implement a function that receives a list of tokens as input and matches that with all the documents (a list of list of tokens) and returns a ranked list of documents as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3f8dKEbmIbAc"
      },
      "outputs": [],
      "source": [
        "def return_ranked_results(search_query_tokens, all_documents):\n",
        "    \"\"\"\n",
        "    Rank lists based on maximum overlap with the input list.\n",
        "\n",
        "    Parameters:\n",
        "    - search_query_tokens: The input list for comparison.\n",
        "    - all_documents: List of lists to be ranked.\n",
        "\n",
        "    Returns:\n",
        "    - List: Ranked list_of_lists based on maximum overlap.\n",
        "    \"\"\"\n",
        "    match_score = {}\n",
        "\n",
        "    for i, document in enumerate(all_documents):\n",
        "      score = 0\n",
        "      document_index = i\n",
        "      for token in search_query_tokens:\n",
        "        if token in document:\n",
        "          score += 1\n",
        "      match_score[document_index] = score\n",
        "\n",
        "    ranked_documents = sorted(match_score.items(),key = lambda x: x[1] ,reverse = True)\n",
        "    return list(ranked_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1YsjWPpJQbE"
      },
      "source": [
        "Let's conduct search without pre-processing the documents and search queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqBvGDWHJgqw",
        "outputId": "7d845bac-4280-4b87-987f-99f26abd3eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------\n",
            "Results for query: text analyis\n",
            "--------------------------\n",
            "NLTK and SpaCy are popular libraries for text analysis. 1\n",
            "Clean and normalized text improves search accuracy. 1\n",
            "Tokenization breaks text into words or phrases. 1\n",
            "--------------------------\n",
            "Results for query: tokenization\n",
            "--------------------------\n",
            "--------------------------\n",
            "Results for query: lemmatization in Python\n",
            "--------------------------\n",
            "Python is widely used in natural language processing. 2\n",
            "SpaCy's lemmatization is more advanced than NLTK's. 1\n",
            "--------------------------\n",
            "Results for query: natural languge processing tools\n",
            "--------------------------\n",
            "Text pre-processing is essential for natural language processing. 1\n",
            "Python is widely used in natural language processing. 1\n",
            "--------------------------\n",
            "Results for query: reducing word\n",
            "--------------------------\n",
            "--------------------------\n",
            "Results for query: nltk and spacy\n",
            "--------------------------\n",
            "NLTK and SpaCy are popular libraries for text analysis. 1\n",
            "Clean and normalized text improves search accuracy. 1\n",
            "--------------------------\n",
            "Results for query: breaking words\n",
            "--------------------------\n",
            "Tokenization breaks text into words or phrases. 1\n",
            "Stemming reduces words to their root form. 1\n",
            "Stop words removal eliminates common words. 1\n"
          ]
        }
      ],
      "source": [
        "# convert documents into list of list of tokens using the split() method. No sophisticated techniques used\n",
        "\n",
        "\n",
        "def perform_search_and_show_results(documents, search_queries):\n",
        "  documents_tokens = []\n",
        "\n",
        "  for document in documents:\n",
        "    documents_tokens.append(document.split())\n",
        "\n",
        "  for query in search_queries:\n",
        "    search_tokens = query.split()\n",
        "    results = return_ranked_results(search_tokens, documents_tokens)\n",
        "    print (f\"--------------------------\")\n",
        "    print (f\"Results for query: {query}\")\n",
        "    print (f\"--------------------------\")\n",
        "    for result in results:\n",
        "      document_id = result[0]\n",
        "      score = result[1]\n",
        "      if score !=0:\n",
        "        print (documents[document_id], score)\n",
        "\n",
        "perform_search_and_show_results(documents, search_queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6mf9wXGFp6X"
      },
      "source": [
        "# Let's apply some text pre-processing techniques and see if the search improves.\n",
        "\n",
        "We are going to apply the following text-preprocessing\n",
        "\n",
        "1. Cleaning\n",
        "2. Tokenization and stopword removal\n",
        "3. Stemming\n",
        "4. Lemmatization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0EF6dc4ZLSA",
        "outputId": "74162765-0ea0-459c-ad31-83940ba20863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------\n",
            "Results for query: text analyis\n",
            "--------------------------\n",
            "text pre-processing is essential for natural language processing. 1\n",
            "nltk and spacy are popular libraries for text analysis. 1\n",
            "clean and normalized text improves search accuracy. 1\n",
            "tokenization breaks text into words or phrases. 1\n",
            "--------------------------\n",
            "Results for query: tokenization\n",
            "--------------------------\n",
            "tokenization breaks text into words or phrases. 1\n",
            "--------------------------\n",
            "Results for query: lemmatization in python\n",
            "--------------------------\n",
            "python is widely used in natural language processing. 2\n",
            "lemmatization provides the base or dictionary form of a word. 1\n",
            "spacy's lemmatization is more advanced than nltk's. 1\n",
            "--------------------------\n",
            "Results for query: natural languge processing tools\n",
            "--------------------------\n",
            "text pre-processing is essential for natural language processing. 1\n",
            "python is widely used in natural language processing. 1\n",
            "--------------------------\n",
            "Results for query: reducing word\n",
            "--------------------------\n",
            "--------------------------\n",
            "Results for query: nltk and spacy\n",
            "--------------------------\n",
            "nltk and spacy are popular libraries for text analysis. 3\n",
            "clean and normalized text improves search accuracy. 1\n",
            "--------------------------\n",
            "Results for query: breaking words\n",
            "--------------------------\n",
            "tokenization breaks text into words or phrases. 1\n",
            "stemming reduces words to their root form. 1\n",
            "stop words removal eliminates common words. 1\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def clean_and_lowercase_text(text):\n",
        "    # implement cleaning logic here\n",
        "    # lower case\n",
        "    text = text.lower()\n",
        "    return text # Convert to lowercase\n",
        "\n",
        "\n",
        "cleaned_documents = [clean_and_lowercase_text(doc) for doc in documents]\n",
        "cleaned_search_queries = [clean_and_lowercase_text(query) for query in search_queries]\n",
        "perform_search_and_show_results(cleaned_documents, cleaned_search_queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeuojuenbRkv"
      },
      "source": [
        "We see slightly improved results. Let's go over other steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD28xOstbZTK",
        "outputId": "12e6008d-e88b-43dc-8d4d-edfd443a36f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# these datasets that NLTK needs to tokenize should be downloaded once.\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZqczeVJbsmG",
        "outputId": "2436b8b7-1393-4ae2-d112-cd48460f8441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------\n",
            "Results for query: text analyis\n",
            "--------------------------\n",
            "text pre-processing essential natural language processing . 1\n",
            "nltk spacy popular libraries text analysis . 1\n",
            "clean normalized text improves search accuracy . 1\n",
            "tokenization breaks text words phrases . 1\n",
            "--------------------------\n",
            "Results for query: tokenization\n",
            "--------------------------\n",
            "tokenization breaks text words phrases . 1\n",
            "--------------------------\n",
            "Results for query: lemmatization python\n",
            "--------------------------\n",
            "lemmatization provides base dictionary form word . 1\n",
            "python widely used natural language processing . 1\n",
            "spacy 's lemmatization advanced nltk 's . 1\n",
            "--------------------------\n",
            "Results for query: natural languge processing tools\n",
            "--------------------------\n",
            "text pre-processing essential natural language processing . 2\n",
            "python widely used natural language processing . 2\n",
            "--------------------------\n",
            "Results for query: reducing word\n",
            "--------------------------\n",
            "lemmatization provides base dictionary form word . 1\n",
            "--------------------------\n",
            "Results for query: nltk spacy\n",
            "--------------------------\n",
            "nltk spacy popular libraries text analysis . 2\n",
            "spacy 's lemmatization advanced nltk 's . 2\n",
            "--------------------------\n",
            "Results for query: breaking words\n",
            "--------------------------\n",
            "tokenization breaks text words phrases . 1\n",
            "stemming reduces words root form . 1\n",
            "stop words removal eliminates common words . 1\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "  tokenized_text = word_tokenize(text)\n",
        "  filtered_text = [token for token in tokenized_text if token not in stop_words]\n",
        "  return \" \".join(filtered_text)\n",
        "\n",
        "tokenized_cleaned_documents = [tokenize_and_remove_stopwords(doc) for doc in cleaned_documents]\n",
        "tokenized_cleaned_search_queries = [tokenize_and_remove_stopwords(query) for query in cleaned_search_queries]\n",
        "perform_search_and_show_results(tokenized_cleaned_documents, tokenized_cleaned_search_queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ML8juofvUg"
      },
      "source": [
        "Again, a more improved search result obtained. Let's apply stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7gL-uWDfusE",
        "outputId": "304e531e-1ea5-4f17-a5a0-d14474ab1621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------\n",
            "Results for query: text analyi\n",
            "--------------------------\n",
            "text pre-process essenti natur languag process . 1\n",
            "nltk spaci popular librari text analysi . 1\n",
            "clean normal text improv search accuraci . 1\n",
            "token break text word phrase . 1\n",
            "--------------------------\n",
            "Results for query: token\n",
            "--------------------------\n",
            "token break text word phrase . 1\n",
            "--------------------------\n",
            "Results for query: lemmat python\n",
            "--------------------------\n",
            "lemmat provid base dictionari form word . 1\n",
            "python wide use natur languag process . 1\n",
            "spaci 's lemmat advanc nltk 's . 1\n",
            "--------------------------\n",
            "Results for query: natur langug process tool\n",
            "--------------------------\n",
            "text pre-process essenti natur languag process . 2\n",
            "python wide use natur languag process . 2\n",
            "--------------------------\n",
            "Results for query: reduc word\n",
            "--------------------------\n",
            "stem reduc word root form . 2\n",
            "token break text word phrase . 1\n",
            "lemmat provid base dictionari form word . 1\n",
            "stop word remov elimin common word . 1\n",
            "--------------------------\n",
            "Results for query: nltk spaci\n",
            "--------------------------\n",
            "nltk spaci popular librari text analysi . 2\n",
            "spaci 's lemmat advanc nltk 's . 2\n",
            "--------------------------\n",
            "Results for query: break word\n",
            "--------------------------\n",
            "token break text word phrase . 2\n",
            "stem reduc word root form . 1\n",
            "lemmat provid base dictionari form word . 1\n",
            "stop word remov elimin common word . 1\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "  text = tokenize_and_remove_stopwords(text)\n",
        "  tokens = text.split()\n",
        "  stemmed_words = [ps.stem(token) for token in tokens]\n",
        "  return \" \".join(stemmed_words)\n",
        "\n",
        "stemmed_documents = [stem_text(doc) for doc in cleaned_documents]\n",
        "stemmed_queries = [stem_text(query) for query in cleaned_search_queries]\n",
        "perform_search_and_show_results(stemmed_documents, stemmed_queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foa_elTAhQYQ"
      },
      "source": [
        "Now, let's try lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R9RF5jlf-R2",
        "outputId": "482c0027-d873-46d0-9efb-57f5ac86833c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------\n",
            "Results for query: text analyis\n",
            "--------------------------\n",
            "text pre-processing essential natural language processing . 1\n",
            "nltk spacy popular library text analysis . 1\n",
            "clean normalized text improves search accuracy . 1\n",
            "tokenization break text word phrase . 1\n",
            "--------------------------\n",
            "Results for query: tokenization\n",
            "--------------------------\n",
            "tokenization break text word phrase . 1\n",
            "--------------------------\n",
            "Results for query: lemmatization python\n",
            "--------------------------\n",
            "lemmatization provides base dictionary form word . 1\n",
            "python widely used natural language processing . 1\n",
            "spacy 's lemmatization advanced nltk 's . 1\n",
            "--------------------------\n",
            "Results for query: natural languge processing tool\n",
            "--------------------------\n",
            "text pre-processing essential natural language processing . 2\n",
            "python widely used natural language processing . 2\n",
            "--------------------------\n",
            "Results for query: reducing word\n",
            "--------------------------\n",
            "tokenization break text word phrase . 1\n",
            "stemming reduces word root form . 1\n",
            "lemmatization provides base dictionary form word . 1\n",
            "stop word removal eliminates common word . 1\n",
            "--------------------------\n",
            "Results for query: nltk spacy\n",
            "--------------------------\n",
            "nltk spacy popular library text analysis . 2\n",
            "spacy 's lemmatization advanced nltk 's . 2\n",
            "--------------------------\n",
            "Results for query: breaking word\n",
            "--------------------------\n",
            "tokenization break text word phrase . 1\n",
            "stemming reduces word root form . 1\n",
            "lemmatization provides base dictionary form word . 1\n",
            "stop word removal eliminates common word . 1\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  text = tokenize_and_remove_stopwords(text)\n",
        "  tokens = text.split()\n",
        "  lemmas = [wnl.lemmatize(token) for token in tokens]\n",
        "  return \" \".join(lemmas)\n",
        "\n",
        "lemmatized_documents = [lemmatize_text(doc) for doc in cleaned_documents]\n",
        "lemmatized_queries = [lemmatize_text(query) for query in cleaned_search_queries]\n",
        "perform_search_and_show_results(lemmatized_documents, lemmatized_queries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo4H0gXMgStD"
      },
      "source": [
        "Exercise E1. Search terms within a PDF document\n",
        "\n",
        "For this research paper here https://arxiv.org/abs/1706.03762\n",
        "1. Extract all text. Convert them into sentences using NLTK's Sentence Tokenizer\n",
        "\n",
        "Example code:\n",
        "\n",
        "```\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"God is Great! I won a lottery.\"\n",
        "print(sent_tokenize(text))\n",
        "\n",
        "Output: ['God is Great!', 'I won a lottery ']\n",
        "```\n",
        "\n",
        "2. Define a list of search queries on your own.\n",
        "\n",
        "3. Reimplement the above techniques and compare search results before and after applying text-pre-processing"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
