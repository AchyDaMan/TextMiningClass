{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lab10. Deep Learning for NLP -I - Text Classification using Feed Forward Networks\n",
        "\n",
        "In this tutorial, let's repeat what we did in Week 7 i.e., build a sentiment classification model. But this time we will build it using a deep feed forward neural architecture.\n",
        "\n",
        "**This tutorial is graded**. complete the exercises and turn in under week 10.\n",
        "\n",
        "### 1. What is Sentiment Analysis\n",
        "Sentiment analysis (SA), formally known as opinion mining, is a natural language processing (NLP) task that involves determining and quantifying the emotional tone or sentiment expressed within a piece of text, typically written or spoken language. In simple terms, sentiment analysis aims to classify text into predefined categories that represent the sentiment or emotional polarity conveyed by the text. These categories are typically binary, classifying text as either *positive* or *negative*, but they can also be more fine-grained, such as *positive*, *negative*, or *neutral*.\n",
        "\n",
        "We will use the IMDB Movie Review dataset again, which contains reviews and binary sentiment values\n",
        "\n",
        "**Note:** I have already extracted and provided the training and test data in the form of CSV files.\n",
        "- `train.csv` - Contains 80% of the IMDB data to be used for training classifiers.\n",
        "\n",
        "- `test.csv` - Contains 20% of the IMDB data to be used for training classifiers.\n",
        "\n",
        "Each CSV file has two columns\n",
        "\n",
        "- **text** : containing the movie review\n",
        "- **sentiment** : containing the original sentiment -- 0 representing negaitve and 1 representing positive\n",
        "\n",
        "Let's load the data in dataframes and vectorize the sentences.\n"
      ],
      "metadata": {
        "id": "b4fZ-ZaFhJWS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RA-9j38BhCbS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Load training and testing data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Extract input texts and labels\n",
        "train_texts = train_data['text']\n",
        "train_labels = train_data['label']\n",
        "test_texts = test_data['text']\n",
        "test_labels = test_data['label']\n",
        "\n",
        "# Create CountVectorizer to convert text into count vectors. We will restrict the vocabulary size to 1000.\n",
        "\n",
        "vectorizer = CountVectorizer(max_features = 1000)\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)\n",
        "\n",
        "# Vocabulary size. This should come out to be 1000\n",
        "vocab_size = len(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Design a Feed Forward Network using the Tensorflow\n",
        "\n",
        "Let's design a neural network with the following configuration\n",
        "\n",
        "1. Input Layer:\n",
        "   - The input layer of the neural network is implicitly defined by the shape of the input data, which is the count vectors obtained from the text data using `CountVectorizer`.\n",
        "   - Each input sample is represented as a count vector, where each element corresponds to the frequency of a particular word in the vocabulary.\n",
        "   - Therefore, the input layer has `vocab_size` neurons, where `vocab_size` is the size of the vocabulary (i.e., the number of unique words in the text corpus).\n",
        "\n",
        "2. Hidden Layer:\n",
        "   - The hidden layer consists of 16 neurons (units) and uses the ReLU (Rectified Linear Unit) activation function.\n",
        "   - Each neuron in the hidden layer takes the input from all `vocab_size` neurons of the input layer.\n",
        "   - The output of each neuron in the hidden layer is computed by taking a weighted sum of the inputs followed by the ReLU activation function.\n",
        "   - The ReLU activation function introduces non-linearity to the network, allowing it to learn complex patterns in the data.\n",
        "\n",
        "3. Output Layer:\n",
        "   - The output layer consists of a single neuron, which serves as the binary classifier's output.\n",
        "   - It uses the sigmoid activation function, which squashes the output into the range [0, 1], effectively representing the probability of the input belonging to the positive class (in this case, class 1).\n",
        "   - The output value closer to 1 indicates a higher probability of belonging to the positive class, while a value closer to 0 indicates a higher probability of belonging to the negative class (class 0).\n",
        "\n",
        "In summary, the neural network architecture can be described as follows:\n",
        "- Input layer: `vocab_size` neurons (input features)\n",
        "- Hidden layer: 16 neurons with ReLU activation\n",
        "- Output layer: Single neuron with sigmoid activation\n",
        "\n",
        "The network learns to map the input count vectors to the binary labels (0 or 1) by adjusting the weights and biases of the connections between neurons during the training process, using the binary cross-entropy loss function and the Adam optimizer."
      ],
      "metadata": {
        "id": "Zm1B1Wdojn1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(16, input_shape=(vocab_size,), activation='relu'))  # 16 units in the hidden layer\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EvmZeojk4RQ",
        "outputId": "9a526984-091c-4d27-aab3-5673ede27caf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 16)                16016     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16161 (63.13 KB)\n",
            "Trainable params: 16161 (63.13 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluating the model using backpropagation\n"
      ],
      "metadata": {
        "id": "BQsbN_S7lFSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train.toarray(), train_labels, epochs=50, batch_size=16, verbose=2)\n",
        "\n",
        "# Evaluate the model on testing data\n",
        "loss, accuracy = model.evaluate(X_test.toarray(), test_labels, verbose=0)\n",
        "print(f'Accuracy on testing data: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq9cThIDlEqy",
        "outputId": "3696c78d-1fbd-41ed-84ef-9368efba23c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "100/100 - 1s - loss: 0.6385 - accuracy: 0.6475 - 1s/epoch - 13ms/step\n",
            "Epoch 2/50\n",
            "100/100 - 0s - loss: 0.4329 - accuracy: 0.8200 - 275ms/epoch - 3ms/step\n",
            "Epoch 3/50\n",
            "100/100 - 0s - loss: 0.2951 - accuracy: 0.8856 - 293ms/epoch - 3ms/step\n",
            "Epoch 4/50\n",
            "100/100 - 0s - loss: 0.2188 - accuracy: 0.9250 - 318ms/epoch - 3ms/step\n",
            "Epoch 5/50\n",
            "100/100 - 0s - loss: 0.1791 - accuracy: 0.9456 - 283ms/epoch - 3ms/step\n",
            "Epoch 6/50\n",
            "100/100 - 0s - loss: 0.1355 - accuracy: 0.9644 - 316ms/epoch - 3ms/step\n",
            "Epoch 7/50\n",
            "100/100 - 0s - loss: 0.0940 - accuracy: 0.9794 - 265ms/epoch - 3ms/step\n",
            "Epoch 8/50\n",
            "100/100 - 0s - loss: 0.0779 - accuracy: 0.9819 - 180ms/epoch - 2ms/step\n",
            "Epoch 9/50\n",
            "100/100 - 0s - loss: 0.0527 - accuracy: 0.9912 - 182ms/epoch - 2ms/step\n",
            "Epoch 10/50\n",
            "100/100 - 0s - loss: 0.0355 - accuracy: 0.9962 - 171ms/epoch - 2ms/step\n",
            "Epoch 11/50\n",
            "100/100 - 0s - loss: 0.0270 - accuracy: 0.9981 - 209ms/epoch - 2ms/step\n",
            "Epoch 12/50\n",
            "100/100 - 0s - loss: 0.0206 - accuracy: 0.9987 - 177ms/epoch - 2ms/step\n",
            "Epoch 13/50\n",
            "100/100 - 0s - loss: 0.0141 - accuracy: 0.9994 - 174ms/epoch - 2ms/step\n",
            "Epoch 14/50\n",
            "100/100 - 0s - loss: 0.0106 - accuracy: 1.0000 - 180ms/epoch - 2ms/step\n",
            "Epoch 15/50\n",
            "100/100 - 0s - loss: 0.0081 - accuracy: 1.0000 - 175ms/epoch - 2ms/step\n",
            "Epoch 16/50\n",
            "100/100 - 0s - loss: 0.0065 - accuracy: 1.0000 - 190ms/epoch - 2ms/step\n",
            "Epoch 17/50\n",
            "100/100 - 0s - loss: 0.0053 - accuracy: 1.0000 - 204ms/epoch - 2ms/step\n",
            "Epoch 18/50\n",
            "100/100 - 0s - loss: 0.0044 - accuracy: 1.0000 - 194ms/epoch - 2ms/step\n",
            "Epoch 19/50\n",
            "100/100 - 0s - loss: 0.0038 - accuracy: 1.0000 - 186ms/epoch - 2ms/step\n",
            "Epoch 20/50\n",
            "100/100 - 0s - loss: 0.0032 - accuracy: 1.0000 - 188ms/epoch - 2ms/step\n",
            "Epoch 21/50\n",
            "100/100 - 0s - loss: 0.0028 - accuracy: 1.0000 - 223ms/epoch - 2ms/step\n",
            "Epoch 22/50\n",
            "100/100 - 0s - loss: 0.0024 - accuracy: 1.0000 - 213ms/epoch - 2ms/step\n",
            "Epoch 23/50\n",
            "100/100 - 0s - loss: 0.0021 - accuracy: 1.0000 - 199ms/epoch - 2ms/step\n",
            "Epoch 24/50\n",
            "100/100 - 0s - loss: 0.0018 - accuracy: 1.0000 - 200ms/epoch - 2ms/step\n",
            "Epoch 25/50\n",
            "100/100 - 0s - loss: 0.0016 - accuracy: 1.0000 - 188ms/epoch - 2ms/step\n",
            "Epoch 26/50\n",
            "100/100 - 0s - loss: 0.0015 - accuracy: 1.0000 - 192ms/epoch - 2ms/step\n",
            "Epoch 27/50\n",
            "100/100 - 0s - loss: 0.0013 - accuracy: 1.0000 - 206ms/epoch - 2ms/step\n",
            "Epoch 28/50\n",
            "100/100 - 0s - loss: 0.0012 - accuracy: 1.0000 - 170ms/epoch - 2ms/step\n",
            "Epoch 29/50\n",
            "100/100 - 0s - loss: 0.0011 - accuracy: 1.0000 - 205ms/epoch - 2ms/step\n",
            "Epoch 30/50\n",
            "100/100 - 0s - loss: 9.7674e-04 - accuracy: 1.0000 - 189ms/epoch - 2ms/step\n",
            "Epoch 31/50\n",
            "100/100 - 0s - loss: 8.9029e-04 - accuracy: 1.0000 - 224ms/epoch - 2ms/step\n",
            "Epoch 32/50\n",
            "100/100 - 0s - loss: 8.1521e-04 - accuracy: 1.0000 - 314ms/epoch - 3ms/step\n",
            "Epoch 33/50\n",
            "100/100 - 0s - loss: 7.4480e-04 - accuracy: 1.0000 - 203ms/epoch - 2ms/step\n",
            "Epoch 34/50\n",
            "100/100 - 0s - loss: 6.8034e-04 - accuracy: 1.0000 - 182ms/epoch - 2ms/step\n",
            "Epoch 35/50\n",
            "100/100 - 0s - loss: 6.3209e-04 - accuracy: 1.0000 - 195ms/epoch - 2ms/step\n",
            "Epoch 36/50\n",
            "100/100 - 0s - loss: 5.8408e-04 - accuracy: 1.0000 - 214ms/epoch - 2ms/step\n",
            "Epoch 37/50\n",
            "100/100 - 0s - loss: 5.3349e-04 - accuracy: 1.0000 - 198ms/epoch - 2ms/step\n",
            "Epoch 38/50\n",
            "100/100 - 0s - loss: 4.9395e-04 - accuracy: 1.0000 - 230ms/epoch - 2ms/step\n",
            "Epoch 39/50\n",
            "100/100 - 0s - loss: 4.5793e-04 - accuracy: 1.0000 - 240ms/epoch - 2ms/step\n",
            "Epoch 40/50\n",
            "100/100 - 0s - loss: 4.2308e-04 - accuracy: 1.0000 - 220ms/epoch - 2ms/step\n",
            "Epoch 41/50\n",
            "100/100 - 0s - loss: 3.9208e-04 - accuracy: 1.0000 - 215ms/epoch - 2ms/step\n",
            "Epoch 42/50\n",
            "100/100 - 0s - loss: 3.6295e-04 - accuracy: 1.0000 - 202ms/epoch - 2ms/step\n",
            "Epoch 43/50\n",
            "100/100 - 0s - loss: 3.3642e-04 - accuracy: 1.0000 - 204ms/epoch - 2ms/step\n",
            "Epoch 44/50\n",
            "100/100 - 0s - loss: 3.1304e-04 - accuracy: 1.0000 - 200ms/epoch - 2ms/step\n",
            "Epoch 45/50\n",
            "100/100 - 0s - loss: 2.9131e-04 - accuracy: 1.0000 - 183ms/epoch - 2ms/step\n",
            "Epoch 46/50\n",
            "100/100 - 0s - loss: 2.7050e-04 - accuracy: 1.0000 - 185ms/epoch - 2ms/step\n",
            "Epoch 47/50\n",
            "100/100 - 0s - loss: 2.5338e-04 - accuracy: 1.0000 - 227ms/epoch - 2ms/step\n",
            "Epoch 48/50\n",
            "100/100 - 0s - loss: 2.3443e-04 - accuracy: 1.0000 - 291ms/epoch - 3ms/step\n",
            "Epoch 49/50\n",
            "100/100 - 0s - loss: 2.1953e-04 - accuracy: 1.0000 - 267ms/epoch - 3ms/step\n",
            "Epoch 50/50\n",
            "100/100 - 0s - loss: 2.0655e-04 - accuracy: 1.0000 - 481ms/epoch - 5ms/step\n",
            "Accuracy on testing data: 80.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise E1. Design a different neural network and compara the tesat accuracy with the network given in the example.\n",
        "\n",
        "Design a Feed Forward Neural Network (FFNN) for text classification with the following architecture:\n",
        "\n",
        "- Input Layer: vocab_size neurons (input features)\n",
        "- Hidden Layer 1: 32 neurons with ReLU activation\n",
        "- Hidden Layer 2: 16 neurons with ReLU activation\n",
        "- Output Layer: Single neuron with sigmoid activation\n",
        "\n",
        "Also change the batch size to 32 and train the system in simmilar manner as above. Write down your observations."
      ],
      "metadata": {
        "id": "AAPn7yxilQok"
      }
    }
  ]
}