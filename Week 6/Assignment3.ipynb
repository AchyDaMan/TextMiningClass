{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wg4-r1-u7uO"
      },
      "source": [
        "# Assignment 3: Hashtag based Tweet search\n",
        "\n",
        "We will extend Assignment 2 and work on building a vector based search for hashtag based search of tweets.\n",
        "\n",
        "Overview:\n",
        "Welcome to TweetMiner, the leading organization in Twitter data analysis! As an NLP scientist in our team, you're entrusted with the task of extracting the most relevant tweets based on input hashtags. For instance, if the hashtag is \"#abortion,\" we expect you to extract the top N (let's say N=10) tweets that truly discuss the topic of \"abortion.\" Similarly, for a hashtag like \"#politicaladvertising,\" your algorithm should identify and extract the top N (again, let's use N=10) tweets about \"political advertising\".\n",
        "For this assignment your tasks are the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWyn0NixzDUo"
      },
      "source": [
        "## Task 1: Use CountVectorizer (binary = true) vectorization technique and perform search\n",
        "\n",
        "### Processing Tweets:\n",
        "\n",
        "1. Pre-process tweets using applicable pre-processing techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEDcivGEu5bQ",
        "outputId": "e121353c-2efc-42e1-d6e3-0645f595e8f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Exists, skipping\n"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "if not os.path.isfile(\"preprocessed_tweets.txt\"):\n",
        "\n",
        "  # first load file! same as Assignment 2\n",
        "  with open(\"australian_election_2019_tweets.txt\") as f:\n",
        "      list_tweets = f.read().splitlines()\n",
        "\n",
        "  # pre-processing from Lab 4\n",
        "  import spacy\n",
        "  from nltk.corpus import stopwords\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "\n",
        "  # get a list of stopwords from NLTK\n",
        "  stops = set(stopwords.words('english'))\n",
        "\n",
        "  # Load SpaCy English language model\n",
        "  # this is a pipeline capable of applying morphological, lexical and syntax analysis on text\n",
        "\n",
        "  nlp_pipeline = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "  def pre_process_a_single_sentence(sentence: str):\n",
        "    # Lower case text\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    processed_sentence = []\n",
        "\n",
        "    # Tokenize, and lemmatize the text\n",
        "    doc = nlp_pipeline(sentence)\n",
        "\n",
        "    for token in doc:\n",
        "      # here token is an object that contains various information about each token\n",
        "      # information such as lemma, pos, parse labels are available\n",
        "      # we will check here if tokens are present in stopwords; if not, we will retain their lemma\n",
        "      if token not in stops:\n",
        "        lemmatized_token = token.lemma_\n",
        "        processed_sentence.append(lemmatized_token)\n",
        "    processed_sentence = \" \".join (processed_sentence)\n",
        "    return processed_sentence\n",
        "\n",
        "  # remove duplicates first\n",
        "  l_t = list(set(list_tweets))\n",
        "\n",
        "  # we use regex for removing URLs, non-english text\n",
        "  import re\n",
        "  # credit to https://www.geeksforgeeks.org/remove-urls-from-string-in-python/\n",
        "  def remove_non_english(text):\n",
        "      # Define a regex pattern to find\n",
        "      pattern = re.compile(r\"https?://\\S+|(?<=\\s)[@#]|^[@#]|[^a-zA-Z0-9\\s]\")\n",
        "\n",
        "      # Use the sub() method to replace\n",
        "      text_without_noneg = pattern.sub(\"\", text)\n",
        "\n",
        "      return text_without_noneg\n",
        "\n",
        "  ltrdru = []\n",
        "\n",
        "  for line in l_t:\n",
        "    ltrdru.append(remove_non_english(line))\n",
        "\n",
        "  # preprocess text actual\n",
        "  prepro_tweets = [pre_process_a_single_sentence(sentence) for sentence in ltrdru]\n",
        "  print(prepro_tweets[:10])\n",
        "\n",
        "\n",
        "  # save lines\n",
        "  with open('preprocessed_tweets.txt', 'w') as f:\n",
        "      for line in prepro_tweets:\n",
        "          f.write('%s\\n' %line)\n",
        "\n",
        "else:\n",
        "  print(\"File Exists, skipping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I am skipping if I have the file saved because it takes 20 MINUTES to pre-process the file. But now I can simply load it to save time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"preprocessed_tweets.txt\") as f:\n",
        "    prepro_tweets = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-JPpE0AzNBS"
      },
      "source": [
        "2. Vectorize pre-processed tweets with CountVectorizer (binary = true) . This will create sparse vectors of tweets based on its vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000\n",
            "['000' '001' '003' ... 'zombie' 'zone' 'zubspike']\n"
          ]
        }
      ],
      "source": [
        "# also taken from lab 4\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Define the N for N-grams\n",
        "N = 1\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False, binary = True, max_features = 10000)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(prepro_tweets)\n",
        "\n",
        "# Check a few items in the vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "# sanity check: check the list of vocabulary\n",
        "print(vocab[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# making a transformation of the text\n",
        "bow_transformed_corpus = []\n",
        "\n",
        "for sentence in prepro_tweets:\n",
        "  transformed_vector = vectorizer.transform([sentence])\n",
        "  bow_transformed_corpus.append(transformed_vector.toarray()[0])\n",
        "\n",
        "# sanity check : print a few items from the bow_transformed_corpus and bow_transformed_query\n",
        "print (\"Transformed Corpus Samples\", bow_transformed_corpus[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Processing hashtags and conduct search:\n",
        "\n",
        "1. Manually define a list of 10 hashtags, initiating each with the \"#\" symbol. Ensure the list consists of 5 single-word hashtags and 5 multiword hashtags. For multiword hashtags, capitalize the first letter of each word (e.g., #PoliticalAdvertising). \n",
        "\n",
        "Hashtags used: '#RenewableEnergy', '#TaxLaws', '#ParliamentaryMajority', '#coalition', '#Labor' '#Liberal', '#Auspol', '#DemocracySausage', '#ausvotes', and '#AusVotes22'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# list of 10 hashtags\n",
        "hashtags = ['#RenewableEnergy', '#TaxLaws', '#ParliamentaryMajority', '#Coalition', '#Labor', '#Liberal', '#Auspol', '#DemocracySausage', '#Ausvotes', '#AusVotes22']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Remove the \"#\" symbol from all hashtags. If the hashtag is multiword, split it into individual words using regular expressions. Refer to the code snippet available at https://stackoverflow.com/questions/68448243/efficient-way-to-split-multi-word-hashtag-in-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Renewable Energy\n",
            "Tax Laws\n",
            "Parliamentary Majority\n",
            "Coalition\n",
            "Labor\n",
            "Liberal\n",
            "Auspol\n",
            "Democracy Sausage\n",
            "Ausvotes\n",
            "Aus Votes22\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "x = 0\n",
        "for tag in hashtags:\n",
        "    hashtags[x] = re.sub(r'#(\\w*[A-Z]\\w*)', \n",
        "                         lambda m: ' '.join(re.findall('[A-Z][^A-Z]*', m.group())), tag)\n",
        "    print(hashtags[x])\n",
        "    x += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. For each hashtag,\n",
        "\n",
        "a. Vectorize the hashtags USING THE SAME Vectorizer that you built under \"Processing Tweets\". Let's call it \"queryVector\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n",
            "['Aus' 'Auspol' 'Ausvotes' 'Coalition' 'Democracy' 'Energy' 'Labor' 'Laws'\n",
            " 'Liberal' 'Majority' 'Parliamentary' 'Renewable' 'Sausage' 'Tax'\n",
            " 'Votes22']\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. Compute the pairwise similarity between the \"queryVector\" and  each tweet vector using inverse of Euclidean Distance (you can copy the implementation from ALTERNATIVE_Lab4 notebook). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPj6Ccw9Pq/vRcnXnqi8vu9",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
