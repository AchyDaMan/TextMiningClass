{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wg4-r1-u7uO"
      },
      "source": [
        "# Assignment 3: Hashtag based Tweet search\n",
        "\n",
        "We will extend Assignment 2 and work on building a vector based search for hashtag based search of tweets.\n",
        "\n",
        "Overview:\n",
        "Welcome to TweetMiner, the leading organization in Twitter data analysis! As an NLP scientist in our team, you're entrusted with the task of extracting the most relevant tweets based on input hashtags. For instance, if the hashtag is \"#abortion,\" we expect you to extract the top N (let's say N=10) tweets that truly discuss the topic of \"abortion.\" Similarly, for a hashtag like \"#politicaladvertising,\" your algorithm should identify and extract the top N (again, let's use N=10) tweets about \"political advertising\".\n",
        "For this assignment your tasks are the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWyn0NixzDUo"
      },
      "source": [
        "## Task 1: Use CountVectorizer (binary = true) vectorization technique and perform search\n",
        "\n",
        "### Processing Tweets:\n",
        "\n",
        "1. Pre-process tweets using applicable pre-processing techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEDcivGEu5bQ",
        "outputId": "e121353c-2efc-42e1-d6e3-0645f595e8f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Exists, skipping\n"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "if not os.path.isfile(\"preprocessed_tweets.txt\"):\n",
        "\n",
        "  # first load file! same as Assignment 2\n",
        "  with open(\"australian_election_2019_tweets.txt\") as f:\n",
        "      list_tweets = f.read().splitlines()\n",
        "\n",
        "  # pre-processing from Lab 4\n",
        "  import spacy\n",
        "  from nltk.corpus import stopwords\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "\n",
        "  # get a list of stopwords from NLTK\n",
        "  stops = set(stopwords.words('english'))\n",
        "\n",
        "  # Load SpaCy English language model\n",
        "  # this is a pipeline capable of applying morphological, lexical and syntax analysis on text\n",
        "\n",
        "  nlp_pipeline = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "  def pre_process_a_single_sentence(sentence: str):\n",
        "    # Lower case text\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    processed_sentence = []\n",
        "\n",
        "    # Tokenize, and lemmatize the text\n",
        "    doc = nlp_pipeline(sentence)\n",
        "\n",
        "    for token in doc:\n",
        "      # here token is an object that contains various information about each token\n",
        "      # information such as lemma, pos, parse labels are available\n",
        "      # we will check here if tokens are present in stopwords; if not, we will retain their lemma\n",
        "      if token not in stops:\n",
        "        lemmatized_token = token.lemma_\n",
        "        processed_sentence.append(lemmatized_token)\n",
        "    processed_sentence = \" \".join (processed_sentence)\n",
        "    return processed_sentence\n",
        "\n",
        "  # remove duplicates first\n",
        "  l_t = list(set(list_tweets))\n",
        "\n",
        "  # we use regex for removing URLs, non-english text\n",
        "  import re\n",
        "  # credit to https://www.geeksforgeeks.org/remove-urls-from-string-in-python/\n",
        "  def remove_non_english(text):\n",
        "      # Define a regex pattern to find\n",
        "      pattern = re.compile(r\"https?://\\S+|(?<=\\s)[@#]|^[@#]|[^a-zA-Z0-9\\s]\")\n",
        "\n",
        "      # Use the sub() method to replace\n",
        "      text_without_noneg = pattern.sub(\"\", text)\n",
        "\n",
        "      return text_without_noneg\n",
        "\n",
        "  ltrdru = []\n",
        "\n",
        "  for line in l_t:\n",
        "    ltrdru.append(remove_non_english(line))\n",
        "\n",
        "  # preprocess text actual\n",
        "  prepro_tweets = [pre_process_a_single_sentence(sentence) for sentence in ltrdru]\n",
        "\n",
        "\n",
        "  # save lines\n",
        "  with open('preprocessed_tweets.txt', 'w') as f:\n",
        "      for line in prepro_tweets:\n",
        "          f.write('%s\\n' %line)\n",
        "\n",
        "else:\n",
        "  print(\"File Exists, skipping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I am skipping if I have the file saved because it takes 20 MINUTES to pre-process the file. But now I can simply load it to save time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "opened\n"
          ]
        }
      ],
      "source": [
        "prepro_tweets = []\n",
        "with open(\"preprocessed_tweets.txt\") as f:\n",
        "    for line in f.readlines():\n",
        "        # see if there is a loose blank line and skip it if so\n",
        "        if (len(line.strip()) == 0):\n",
        "            continue\n",
        "        if line:\n",
        "            prepro_tweets.append(line.strip())\n",
        "print(\"opened\")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-JPpE0AzNBS"
      },
      "source": [
        "2. Vectorize pre-processed tweets with CountVectorizer (binary = true) . This will create sparse vectors of tweets based on its vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['01' '02' '05' '07' '10' '100' '1000' '10000' '100000' '101']\n"
          ]
        }
      ],
      "source": [
        "# also taken from lab 4\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Define the N for N-grams\n",
        "N = 1\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False, binary = True, max_features = 5000)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(prepro_tweets)\n",
        "\n",
        "# Check a few items in the vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "# sanity check: check the list of vocabulary\n",
        "print(vocab[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed Corpus Samples [array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0])]\n"
          ]
        }
      ],
      "source": [
        "# making a transformation of the text\n",
        "tweet_vectors = []\n",
        "\n",
        "for sentence in prepro_tweets:\n",
        "  transformed_vector = vectorizer.transform([sentence])\n",
        "  tweet_vectors.append(transformed_vector.toarray()[0])\n",
        "\n",
        "print (\"Transformed Corpus Samples\", tweet_vectors[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Processing hashtags and conduct search:\n",
        "\n",
        "1. Manually define a list of 10 hashtags, initiating each with the \"#\" symbol. Ensure the list consists of 5 single-word hashtags and 5 multiword hashtags. For multiword hashtags, capitalize the first letter of each word (e.g., #PoliticalAdvertising). \n",
        "\n",
        "Hashtags used: '#RenewableEnergy', '#TaxLaws', '#ParliamentaryMajority', '#coalition', '#Labor' '#Liberal', '#Auspol', '#DemocracySausage', '#ausvotes', and '#AusVotes22'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# list of 10 hashtags\n",
        "hashtags = ['#RenewableEnergy', '#TaxLaws', '#ParliamentaryMajority', '#Coalition', '#Labor', '#Liberal', '#Auspol', '#DemocracySausage', '#Ausvotes', '#AusVotes22']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Remove the \"#\" symbol from all hashtags. If the hashtag is multiword, split it into individual words using regular expressions. Refer to the code snippet available at https://stackoverflow.com/questions/68448243/efficient-way-to-split-multi-word-hashtag-in-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Renewable Energy\n",
            "Tax Laws\n",
            "Parliamentary Majority\n",
            "Coalition\n",
            "Labor\n",
            "Liberal\n",
            "Auspol\n",
            "Democracy Sausage\n",
            "Ausvotes\n",
            "Aus Votes22\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "x = 0\n",
        "for tag in hashtags:\n",
        "    hashtags[x] = re.sub(r'#(\\w*[A-Z]\\w*)', \n",
        "                         lambda m: ' '.join(re.findall('[A-Z][^A-Z]*', m.group())), tag)\n",
        "    print(hashtags[x])\n",
        "    x += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. For each hashtag,\n",
        "\n",
        "a. Vectorize the hashtags USING THE SAME Vectorizer that you built under \"Processing Tweets\". Let's call it \"queryVector\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "query_vector = vectorizer.transform([hashtags_1]).toarray()[0]\n",
        "query_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. Compute the pairwise similarity between the \"queryVector\" and  each tweet vector using inverse of Euclidean Distance (you can copy the implementation from ALTERNATIVE_Lab4 notebook). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1442/912260388.py:16: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  return 1 / (np.linalg.norm(np.array(vector1) - np.array(vector2)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Renewable Energy Tax Laws Parliamentary Majority Coalition Labor Liberal Auspol Democracy Sausage Ausvotes Aus Votes22\n",
            "Document: yummmm, Score: inf\n",
            "Document: imbeingverysarcasticrightnow, Score: inf\n",
            "Document: kot5aside, Score: inf\n",
            "Document: 6 405, Score: inf\n",
            "Document: helenhaine, Score: inf\n"
          ]
        }
      ],
      "source": [
        "# let's define a similarity function.\n",
        "# we take 1 / euclidean_distance between two vectors as the similarity\n",
        "\n",
        "import numpy as np\n",
        "def euclidean_distance_based_similarity (vector1, vector2):\n",
        "    \"\"\"\n",
        "    Compute the Euclidean distance between two vectors.\n",
        "\n",
        "    Parameters:\n",
        "        vector1 (array-like): First vector.\n",
        "        vector2 (array-like): Second vector.\n",
        "\n",
        "    Returns:\n",
        "        float: Euclidean distance between the two vectors.\n",
        "    \"\"\"\n",
        "    return 1 / (np.linalg.norm(np.array(vector1) - np.array(vector2)))\n",
        "\n",
        "\n",
        "similarity_scores = {}\n",
        "\n",
        "for i, document_vector in enumerate(tweet_vectors):\n",
        "  sim = euclidean_distance_based_similarity(document_vector, query_vector)\n",
        "  similarity_scores[i] = sim\n",
        "\n",
        "ranked_documents = sorted(similarity_scores.items(),key = lambda x: x[1] ,reverse = True)\n",
        "\n",
        "# Let's print the top 5 documents based on ranked score\n",
        "\n",
        "print (f\"Query: {hashtags_1}\")\n",
        "for document_idx, score in ranked_documents[:5]:\n",
        "    print (f\"Document: {prepro_tweets[document_idx]}, Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(67, inf),\n",
              " (75, inf),\n",
              " (182, inf),\n",
              " (214, inf),\n",
              " (259, inf),\n",
              " (260, inf),\n",
              " (327, inf),\n",
              " (358, inf),\n",
              " (498, inf),\n",
              " (546, inf),\n",
              " (605, inf),\n",
              " (677, inf),\n",
              " (693, inf),\n",
              " (709, inf),\n",
              " (714, inf)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. Rank tweets based on the similarity score in ascending order. Print the top 10 most similar tweets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "d. Repeat (a) (b) (c) for all 10 hashtags that you manually created."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPj6Ccw9Pq/vRcnXnqi8vu9",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
