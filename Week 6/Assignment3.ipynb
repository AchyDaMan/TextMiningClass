{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wg4-r1-u7uO"
      },
      "source": [
        "# Assignment 3: Hashtag based Tweet search\n",
        "\n",
        "We will extend Assignment 2 and work on building a vector based search for hashtag based search of tweets.\n",
        "\n",
        "Overview:\n",
        "Welcome to TweetMiner, the leading organization in Twitter data analysis! As an NLP scientist in our team, you're entrusted with the task of extracting the most relevant tweets based on input hashtags. For instance, if the hashtag is \"#abortion,\" we expect you to extract the top N (let's say N=10) tweets that truly discuss the topic of \"abortion.\" Similarly, for a hashtag like \"#politicaladvertising,\" your algorithm should identify and extract the top N (again, let's use N=10) tweets about \"political advertising\".\n",
        "For this assignment your tasks are the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWyn0NixzDUo"
      },
      "source": [
        "## Task 1: Use CountVectorizer (binary = true) vectorization technique and perform search\n",
        "\n",
        "### Processing Tweets:\n",
        "\n",
        "1. Pre-process tweets using applicable pre-processing techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEDcivGEu5bQ",
        "outputId": "e121353c-2efc-42e1-d6e3-0645f595e8f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# first load file! same as Assignment 2\n",
        "with open(\"australian_election_2019_tweets.txt\") as f:\n",
        "    list_tweets = f.read().splitlines()\n",
        "\n",
        "# pre-processing from Lab 4\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# get a list of stopwords from NLTK\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "# Load SpaCy English language model\n",
        "# this is a pipeline capable of applying morphological, lexical and syntax analysis on text\n",
        "\n",
        "nlp_pipeline = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pre_process_a_single_sentence(sentence: str):\n",
        "  # Lower case text\n",
        "  sentence = sentence.lower()\n",
        "\n",
        "  processed_sentence = []\n",
        "\n",
        "  # Tokenize, and lemmatize the text\n",
        "  doc = nlp_pipeline(sentence)\n",
        "\n",
        "  for token in doc:\n",
        "    # here token is an object that contains various information about each token\n",
        "    # information such as lemma, pos, parse labels are available\n",
        "    # we will check here if tokens are present in stopwords; if not, we will retain their lemma\n",
        "    if token not in stops:\n",
        "      lemmatized_token = token.lemma_\n",
        "      processed_sentence.append(lemmatized_token)\n",
        "  processed_sentence = \" \".join (processed_sentence)\n",
        "  return processed_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove duplicates first\n",
        "l_t = list(set(list_tweets))\n",
        "\n",
        "# we use regex for removing URLs, non-english text\n",
        "import re\n",
        "# credit to https://www.geeksforgeeks.org/remove-urls-from-string-in-python/\n",
        "def remove_non_english(text):\n",
        "    # Define a regex pattern to find\n",
        "    pattern = re.compile(r\"https?://\\S+|(?<=\\s)[@#]|^[@#]|[^a-zA-Z0-9\\s]\")\n",
        "\n",
        "    # Use the sub() method to replace\n",
        "    text_without_noneg = pattern.sub(\"\", text)\n",
        "\n",
        "    return text_without_noneg\n",
        "\n",
        "ltrdru = []\n",
        "\n",
        "for line in l_t:\n",
        "  ltrdru.append(remove_non_english(line))\n",
        "\n",
        "# preprocess text actual\n",
        "prepro_tweets = [pre_process_a_single_sentence(sentence) for sentence in ltrdru]\n",
        "print(prepro_tweets[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I am going to save this preprocessed text to a file for future use because it takes >10 MINUTES to run, and I can't do that every time I need to re-run this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save lines\n",
        "with open('preprocessed_tweets.txt', 'w') as f:\n",
        "    for line in prepro_tweets:\n",
        "        f.write('%s\\n' %line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then I can load it to save time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"preprocessed_tweets.txt\") as f:\n",
        "    prepro_tweets = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-JPpE0AzNBS"
      },
      "source": [
        "2. Vectorize pre-processed tweets with CountVectorizer (binary = true) . This will create sparse vectors of tweets based on its vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# also taken from lab 4\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Define the N for N-grams\n",
        "N = 1\n",
        "# Initialize the CountVectorizer with N-gram range\n",
        "vectorizer = CountVectorizer(ngram_range=(N, N), lowercase = False, binary = True)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "vectorizer.fit(prepro_tweets)\n",
        "\n",
        "# Check a few items in the vocabulary\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "# check the vocabulary size\n",
        "print(len(vocab))\n",
        "\n",
        "# sanity check: check the list of vocabulary\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Processing hashtags and conduct search:\n",
        "\n",
        "1. Manually define a list of 10 hashtags, initiating each with the \"#\" symbol. Ensure the list consists of 5 single-word hashtags and 5 multiword hashtags. For multiword hashtags, capitalize the first letter of each word (e.g., #PoliticalAdvertising). "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPj6Ccw9Pq/vRcnXnqi8vu9",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
