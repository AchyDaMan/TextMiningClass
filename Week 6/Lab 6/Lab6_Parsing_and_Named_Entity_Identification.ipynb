{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lab6_Parsing_and_Named_Entity_Identification\n",
        "\n",
        "In this practicum, we'll delve into text parsing, specifically dependency parsing and named entity identification.\n",
        "\n",
        "Complete all exercises and submit under \"Lab 6: Parsing_and_Named_Entity_Identification\" : https://utexas.instructure.com/courses/1382133/assignments/6627276\n",
        "\n",
        "# 1. Dependency Parsing Example using SpaCy\n"
      ],
      "metadata": {
        "id": "YEMeiYNuwzrT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A93n1Us_vV5H",
        "outputId": "416864d4-7744-415c-c416-9de11189c7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency --(compound)--> parsing\n",
            "parsing --(nsubj)--> helps\n",
            "helps --(ROOT)--> helps\n",
            "in --(prep)--> helps\n",
            "understanding --(pcomp)--> in\n",
            "sentence --(compound)--> structure\n",
            "structure --(dobj)--> understanding\n",
            ". --(punct)--> helps\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Dependency parsing helps in understanding sentence structure.\"\n",
        "\n",
        "# Process the sentence\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print the dependency parse tree\n",
        "for token in doc:\n",
        "  print(f\"{token.text} --({token.dep_})--> {token.head.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Explaining Dependency Parse Labels\n"
      ],
      "metadata": {
        "id": "ccu0XLyK040D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to print dependency relation definitions\n",
        "def print_dependency_definitions():\n",
        "  # Iterate over all dependency labels and print their definitions\n",
        "  for label in sorted(nlp.get_pipe(\"parser\").labels):\n",
        "    print(f\"{label}: {spacy.explain(label)}\")\n",
        "\n",
        "# Print dependency relation definitions\n",
        "print_dependency_definitions()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xTxF5cU02MN",
        "outputId": "826c9b33-f978-44d9-da5a-7a00291d1687"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROOT: root\n",
            "acl: clausal modifier of noun (adjectival clause)\n",
            "acomp: adjectival complement\n",
            "advcl: adverbial clause modifier\n",
            "advmod: adverbial modifier\n",
            "agent: agent\n",
            "amod: adjectival modifier\n",
            "appos: appositional modifier\n",
            "attr: attribute\n",
            "aux: auxiliary\n",
            "auxpass: auxiliary (passive)\n",
            "case: case marking\n",
            "cc: coordinating conjunction\n",
            "ccomp: clausal complement\n",
            "compound: compound\n",
            "conj: conjunct\n",
            "csubj: clausal subject\n",
            "csubjpass: clausal subject (passive)\n",
            "dative: dative\n",
            "dep: unclassified dependent\n",
            "det: determiner\n",
            "dobj: direct object\n",
            "expl: expletive\n",
            "intj: interjection\n",
            "mark: marker\n",
            "meta: meta modifier\n",
            "neg: negation modifier\n",
            "nmod: modifier of nominal\n",
            "npadvmod: noun phrase as adverbial modifier\n",
            "nsubj: nominal subject\n",
            "nsubjpass: nominal subject (passive)\n",
            "nummod: numeric modifier\n",
            "oprd: object predicate\n",
            "parataxis: parataxis\n",
            "pcomp: complement of preposition\n",
            "pobj: object of preposition\n",
            "poss: possession modifier\n",
            "preconj: pre-correlative conjunction\n",
            "predet: None\n",
            "prep: prepositional modifier\n",
            "prt: particle\n",
            "punct: punctuation\n",
            "quantmod: modifier of quantifier\n",
            "relcl: relative clause modifier\n",
            "xcomp: open clausal complement\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/glossary.py:20: UserWarning: [W118] Term 'predet' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Named Entity Recognition\n",
        "\n",
        "Named Entity Recognition (NER) is a natural language processing (NLP) technique that involves identifying and classifying named entities (such as names of people, organizations, locations, dates, and more).\n",
        "\n",
        "A common named entity tag set used in Named Entity Recognition (NER) includes the following categories:\n",
        "\n",
        "1. PERSON: Names of individuals, including first and last names.\n",
        "2. ORGANIZATION: Names of companies, institutions, and organizations.\n",
        "3. LOCATION: Names of places, such as cities, countries, and geographical locations.\n",
        "4. DATE: Expressions of date and time, including specific dates, months, years, and time references.\n",
        "5. TIME: Specific times and time-related expressions.\n",
        "6. PERCENT: Percentage values.\n",
        "7. MONEY: Monetary values and currency references.\n",
        "8. QUANTITY: Measurements or quantities, such as distances, weights, and numbers.\n",
        "9. ORDINAL: Ordinal numbers (e.g., first, second, third).\n",
        "10. CARDINAL: Cardinal numbers (e.g., one, two, three).\n",
        "11. EVENT: Names of events, meetings, and occurrences.\n",
        "12. ARTIFACT: Names of products, inventions, and works of art.\n",
        "13. WORK_OF_ART: Names of artistic works, such as books, movies, and paintings.\n",
        "14. LANGUAGE: Names of languages.\n",
        "15. NORP: Nationalities, religious and political groups.\n",
        "\n",
        "This is just a standard set of named entity categories, and in practice, NER systems can be customized to include additional categories or adapt to specific domains or languages as needed."
      ],
      "metadata": {
        "id": "yP3_NlGt2TNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple Inc. is headquartered in Cupertino, California.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTv5WlBM2Sln",
        "outputId": "c94f2568-7bf0-4061-b294-b9b3f84b0da9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple Inc. ORG\n",
            "Cupertino GPE\n",
            "California GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Putting Everything we have Discussed Together to Extract Knowledge from Raw Text.\n",
        "\n",
        "Knowledge extraction from text refers to the process of automatically identifying and extracting structured information or knowledge from unstructured text sources such as articles, documents, or web pages. This extracted knowledge can then be organized and represented in a structured format that is suitable for analysis, storage, and further processing.\n",
        "\n",
        "One common way to represent knowledge extracted from text is through triples. Triples are basic units of structured information consisting of three parts: a subject, a predicate, and an object. This representation is often referred to as a \"subject-predicate-object\" or \"entity-relationship-entity\" structure.\n",
        "\n",
        "Here's how triples work:\n",
        "\n",
        "**Subject:** The entity or concept being described or referenced.\n",
        "\n",
        "**Predicate:** The relationship or attribute that connects the subject to the object.\n",
        "\n",
        "**Object:** The value or entity that is related to the subject by the predicate.\n",
        "\n",
        "For example, consider the following sentence:\n",
        "\n",
        "`John teaches at UT Austin`\n",
        "\n",
        "In this sentence, we can extract the following triple:\n",
        "\n",
        "Subject: John\n",
        "Predicate: teach at\n",
        "Object: UT Austin\n",
        "\n",
        "This can be simply written as `<John, teach at, UT Austin>`\n",
        "\n",
        "Triples allow for the representation of various types of knowledge, including relationships between entities, attributes of entities, events, facts, and more. They provide a structured way to organize and store information extracted from text, enabling easier analysis, inference, and integration with other data sources. Triple-based representations are commonly used in knowledge graphs, semantic web technologies, and natural language processing applications for tasks such as information retrieval, question answering, and knowledge base construction.\n",
        "\n",
        "\n",
        "Below are some functions to extract triples using Dependency Parsing, PoS tagging and Named Entity Recognition.\n",
        "\n"
      ],
      "metadata": {
        "id": "dzM1d6Pp2wnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_triples(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  triples = []\n",
        "  for token in doc:\n",
        "    # We will try to identify the SUBJECT and ROOT (head node of subject in the graph)\n",
        "    # if the ROOT has a coarse grain POS category of VERB, we consider the ROOT as PRECICATE\n",
        "    # all children of the note \"direct object (dobj) are considered as OBJECT\"\n",
        "    #print (token.text, token.dep_)\n",
        "    if token.dep_.startswith(\"nsubj\") and token.head.pos_ == \"VERB\":\n",
        "      subject = get_entity_or_noun_phrase(token)\n",
        "      predicate = token.head.lemma_\n",
        "      obj = None\n",
        "      for child in token.head.children:\n",
        "        if \"obj\" in child.dep_:\n",
        "          obj = get_entity_or_noun_phrase(child)\n",
        "          break\n",
        "        elif \"prep\" in child.dep_:\n",
        "          # we merge the preposition to the predicate\n",
        "          predicate = predicate+\" \" +child.text\n",
        "          obj = \" \".join([c.text for c in child.subtree if c.text != child.text])\n",
        "          break\n",
        "      #lowercase everything\n",
        "      if subject is not None:\n",
        "        subject = subject.lower()\n",
        "      else:\n",
        "        subject = \"None\"\n",
        "      if predicate is not None:\n",
        "        predicate = predicate.lower()\n",
        "      else:\n",
        "        predicate = \"None\"\n",
        "      if obj is not None:\n",
        "        obj = obj.lower()\n",
        "      else:\n",
        "        obj = \"None\"\n",
        "      triples.append((subject, predicate, obj))\n",
        "  return triples\n",
        "\n",
        "# Function to get entity or noun phrase\n",
        "def get_entity_or_noun_phrase(token):\n",
        "  if token.ent_type_:\n",
        "    return token.text\n",
        "  else:\n",
        "    return \" \".join([child.text for child in token.subtree])"
      ],
      "metadata": {
        "id": "tOeipZka50cl"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1. Processing Documents and Extracting Tuples.\n",
        "\n",
        "For processing documents, we need to extract sentences from documents. Let's work on a hypothetical docuemnt:\n",
        "\n",
        "```\n",
        "document = \"Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
        "```"
      ],
      "metadata": {
        "id": "89M85zwdF2hY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "document = \"Barack Obama was born in Hawaii. Obama served as the 44th president of the United States.\"\n",
        "\n",
        "sentences = sent_tokenize(document)\n",
        "all_triples = []\n",
        "for sentence in sentences:\n",
        "  triples = extract_triples(sentence)\n",
        "  all_triples.extend(triples)\n",
        "\n",
        "print (all_triples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T5dvW5WGbEJ",
        "outputId": "bdd15a96-200b-4a96-bf95-d45eab584abb"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('obama', 'bear in', 'hawaii'), ('obama', 'serve as', 'the 44th president of the united states')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Using the knowlegde extracted to answer questions\n",
        "\n",
        "We can parse questions in the same way as we parsed sentences in the document. Assuming that questions represent incomplete knowlegde tuples, we can take incomplete tuples and search within our knowledge base to find the missing portions.\n"
      ],
      "metadata": {
        "id": "4zcB9g2tSybx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_answer_from_triples(question_triple, knowledge_triples):\n",
        "  answer = None\n",
        "  for knowledge_triple in knowledge_triples:\n",
        "    if question_triple[0] in knowledge_triple[0] and question_triple[1] in knowledge_triple[1]:\n",
        "      answer = knowledge_triple[2]\n",
        "    elif question_triple[0] in knowledge_triple[0] and question_triple[2] in knowledge_triple[2]:\n",
        "      answer = knowledge_triple[1]\n",
        "    elif question_triple[1] in knowledge_triple[1] and question_triple[2] in knowledge_triple[2]:\n",
        "      answer = knowledge_triple[0]\n",
        "  return answer\n",
        "\n",
        "def answer_question(question, knowledge_triples):\n",
        "  question_triple = extract_triples(question)[0] # only one triple from one question\n",
        "  print (\"Question triples\", question_triple)\n",
        "  answer = find_answer_from_triples(question_triple, knowledge_triples)\n",
        "  if answer is None:\n",
        "     answer = \"Sorry, I don't have an answer to that question.\"\n",
        "  print (answer)"
      ],
      "metadata": {
        "id": "p6-XSsAUTJv5"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try to answer some questions."
      ],
      "metadata": {
        "id": "xUNVZGQwXiTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Where was Barack Obama born?\",\n",
        "    \"Who served as the president of the United States?\",\n",
        "    \"What did Barack Obama do?\",\n",
        "    \"Who was born in Hawaii?\",\n",
        "]\n",
        "for question in questions:\n",
        "  answer_question(question,all_triples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ_u4drdXlsN",
        "outputId": "41c99faa-38e6-4cbe-d370-0e7819070c6d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question triples ('obama', 'bear', 'None')\n",
            "hawaii\n",
            "Question triples ('who', 'serve as', 'the president of the united states')\n",
            "Sorry, I don't have an answer to that question.\n",
            "Question triples ('obama', 'do', 'what')\n",
            "Sorry, I don't have an answer to that question.\n",
            "Question triples ('who', 'bear in', 'hawaii')\n",
            "obama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, we would probably need some fuzzy matching techniques to answer all questions perfectly."
      ],
      "metadata": {
        "id": "dJk8V3Btb7AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E1. Exercise: Identifying Entities from News\n",
        "\n",
        "- Visit a news website of your choice. Copy paste a few sentences from an article (preferrably containing a lot of names) into a variable `document`.\n",
        "\n",
        "- Process the text (clean up a bit if required) and put all unique sentences in a list.\n",
        "\n",
        "- Perform NER on the sentences using SpaCY and print the unique entities (person, location, organizations etc).\n",
        "\n",
        "- Try repeating the same exercise on a French news website https://www.lefigaro.fr/. You will have to download the and `fr_core_news_sm` .\n",
        "\n",
        "- Write down your observations.\n",
        "\n",
        "**Optional Exercise 2 (not-graded) **\n",
        "Can you extract some knowledge tuples by using NER, POS and/or Dependency Parsing from the same English news website following our example code. What kind of ammendments did you need to make to the existing rules to extract better tuples.\n",
        "\n",
        "**Optional Exercise 3 (not-graded) **\n",
        "Can you build a question answering system, following the above examples? Feel free to formulate variery of questions and check the accuracy of your system (qualitatively).\n",
        "\n"
      ],
      "metadata": {
        "id": "2mWPZ17vcFdk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdLw7LUSbiLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}