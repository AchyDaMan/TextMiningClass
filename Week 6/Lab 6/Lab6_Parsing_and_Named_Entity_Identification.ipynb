{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEMeiYNuwzrT"
      },
      "source": [
        "## Lab6_Parsing_and_Named_Entity_Identification\n",
        "\n",
        "In this practicum, we'll delve into text parsing, specifically dependency parsing and named entity identification.\n",
        "\n",
        "Complete all exercises and submit under \"Lab 6: Parsing_and_Named_Entity_Identification\" : https://utexas.instructure.com/courses/1382133/assignments/6627276\n",
        "\n",
        "# 1. Dependency Parsing Example using SpaCy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A93n1Us_vV5H",
        "outputId": "416864d4-7744-415c-c416-9de11189c7d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dependency --(compound)--> parsing\n",
            "parsing --(nsubj)--> helps\n",
            "helps --(ROOT)--> helps\n",
            "in --(prep)--> helps\n",
            "understanding --(pcomp)--> in\n",
            "sentence --(compound)--> structure\n",
            "structure --(dobj)--> understanding\n",
            ". --(punct)--> helps\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Dependency parsing helps in understanding sentence structure.\"\n",
        "\n",
        "# Process the sentence\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print the dependency parse tree\n",
        "for token in doc:\n",
        "  print(f\"{token.text} --({token.dep_})--> {token.head.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccu0XLyK040D"
      },
      "source": [
        "### 1.1. Explaining Dependency Parse Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xTxF5cU02MN",
        "outputId": "826c9b33-f978-44d9-da5a-7a00291d1687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROOT: root\n",
            "acl: clausal modifier of noun (adjectival clause)\n",
            "acomp: adjectival complement\n",
            "advcl: adverbial clause modifier\n",
            "advmod: adverbial modifier\n",
            "agent: agent\n",
            "amod: adjectival modifier\n",
            "appos: appositional modifier\n",
            "attr: attribute\n",
            "aux: auxiliary\n",
            "auxpass: auxiliary (passive)\n",
            "case: case marking\n",
            "cc: coordinating conjunction\n",
            "ccomp: clausal complement\n",
            "compound: compound\n",
            "conj: conjunct\n",
            "csubj: clausal subject\n",
            "csubjpass: clausal subject (passive)\n",
            "dative: dative\n",
            "dep: unclassified dependent\n",
            "det: determiner\n",
            "dobj: direct object\n",
            "expl: expletive\n",
            "intj: interjection\n",
            "mark: marker\n",
            "meta: meta modifier\n",
            "neg: negation modifier\n",
            "nmod: modifier of nominal\n",
            "npadvmod: noun phrase as adverbial modifier\n",
            "nsubj: nominal subject\n",
            "nsubjpass: nominal subject (passive)\n",
            "nummod: numeric modifier\n",
            "oprd: object predicate\n",
            "parataxis: parataxis\n",
            "pcomp: complement of preposition\n",
            "pobj: object of preposition\n",
            "poss: possession modifier\n",
            "preconj: pre-correlative conjunction\n",
            "predet: None\n",
            "prep: prepositional modifier\n",
            "prt: particle\n",
            "punct: punctuation\n",
            "quantmod: modifier of quantifier\n",
            "relcl: relative clause modifier\n",
            "xcomp: open clausal complement\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.10/site-packages/spacy/glossary.py:20: UserWarning: [W118] Term 'predet' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to print dependency relation definitions\n",
        "def print_dependency_definitions():\n",
        "  # Iterate over all dependency labels and print their definitions\n",
        "  for label in sorted(nlp.get_pipe(\"parser\").labels):\n",
        "    print(f\"{label}: {spacy.explain(label)}\")\n",
        "\n",
        "# Print dependency relation definitions\n",
        "print_dependency_definitions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP3_NlGt2TNo"
      },
      "source": [
        "# 2. Named Entity Recognition\n",
        "\n",
        "Named Entity Recognition (NER) is a natural language processing (NLP) technique that involves identifying and classifying named entities (such as names of people, organizations, locations, dates, and more).\n",
        "\n",
        "A common named entity tag set used in Named Entity Recognition (NER) includes the following categories:\n",
        "\n",
        "1. PERSON: Names of individuals, including first and last names.\n",
        "2. ORGANIZATION: Names of companies, institutions, and organizations.\n",
        "3. LOCATION: Names of places, such as cities, countries, and geographical locations.\n",
        "4. DATE: Expressions of date and time, including specific dates, months, years, and time references.\n",
        "5. TIME: Specific times and time-related expressions.\n",
        "6. PERCENT: Percentage values.\n",
        "7. MONEY: Monetary values and currency references.\n",
        "8. QUANTITY: Measurements or quantities, such as distances, weights, and numbers.\n",
        "9. ORDINAL: Ordinal numbers (e.g., first, second, third).\n",
        "10. CARDINAL: Cardinal numbers (e.g., one, two, three).\n",
        "11. EVENT: Names of events, meetings, and occurrences.\n",
        "12. ARTIFACT: Names of products, inventions, and works of art.\n",
        "13. WORK_OF_ART: Names of artistic works, such as books, movies, and paintings.\n",
        "14. LANGUAGE: Names of languages.\n",
        "15. NORP: Nationalities, religious and political groups.\n",
        "\n",
        "This is just a standard set of named entity categories, and in practice, NER systems can be customized to include additional categories or adapt to specific domains or languages as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTv5WlBM2Sln",
        "outputId": "c94f2568-7bf0-4061-b294-b9b3f84b0da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apple Inc. ORG\n",
            "Cupertino GPE\n",
            "California GPE\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple Inc. is headquartered in Cupertino, California.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzM1d6Pp2wnt"
      },
      "source": [
        "# 3. Putting Everything we have Discussed Together to Extract Knowledge from Raw Text.\n",
        "\n",
        "Knowledge extraction from text refers to the process of automatically identifying and extracting structured information or knowledge from unstructured text sources such as articles, documents, or web pages. This extracted knowledge can then be organized and represented in a structured format that is suitable for analysis, storage, and further processing.\n",
        "\n",
        "One common way to represent knowledge extracted from text is through triples. Triples are basic units of structured information consisting of three parts: a subject, a predicate, and an object. This representation is often referred to as a \"subject-predicate-object\" or \"entity-relationship-entity\" structure.\n",
        "\n",
        "Here's how triples work:\n",
        "\n",
        "**Subject:** The entity or concept being described or referenced.\n",
        "\n",
        "**Predicate:** The relationship or attribute that connects the subject to the object.\n",
        "\n",
        "**Object:** The value or entity that is related to the subject by the predicate.\n",
        "\n",
        "For example, consider the following sentence:\n",
        "\n",
        "`John teaches at UT Austin`\n",
        "\n",
        "In this sentence, we can extract the following triple:\n",
        "\n",
        "Subject: John\n",
        "Predicate: teach at\n",
        "Object: UT Austin\n",
        "\n",
        "This can be simply written as `<John, teach at, UT Austin>`\n",
        "\n",
        "Triples allow for the representation of various types of knowledge, including relationships between entities, attributes of entities, events, facts, and more. They provide a structured way to organize and store information extracted from text, enabling easier analysis, inference, and integration with other data sources. Triple-based representations are commonly used in knowledge graphs, semantic web technologies, and natural language processing applications for tasks such as information retrieval, question answering, and knowledge base construction.\n",
        "\n",
        "\n",
        "Below are some functions to extract triples using Dependency Parsing, PoS tagging and Named Entity Recognition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tOeipZka50cl"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_triples(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  triples = []\n",
        "  for token in doc:\n",
        "    # We will try to identify the SUBJECT and ROOT (head node of subject in the graph)\n",
        "    # if the ROOT has a coarse grain POS category of VERB, we consider the ROOT as PRECICATE\n",
        "    # all children of the note \"direct object (dobj) are considered as OBJECT\"\n",
        "    #print (token.text, token.dep_)\n",
        "    if token.dep_.startswith(\"nsubj\") and token.head.pos_ == \"VERB\":\n",
        "      subject = get_entity_or_noun_phrase(token)\n",
        "      predicate = token.head.lemma_\n",
        "      obj = None\n",
        "      for child in token.head.children:\n",
        "        if \"obj\" in child.dep_:\n",
        "          obj = get_entity_or_noun_phrase(child)\n",
        "          break\n",
        "        elif \"prep\" in child.dep_:\n",
        "          # we merge the preposition to the predicate\n",
        "          predicate = predicate+\" \" +child.text\n",
        "          obj = \" \".join([c.text for c in child.subtree if c.text != child.text])\n",
        "          break\n",
        "      #lowercase everything\n",
        "      if subject is not None:\n",
        "        subject = subject.lower()\n",
        "      else:\n",
        "        subject = \"None\"\n",
        "      if predicate is not None:\n",
        "        predicate = predicate.lower()\n",
        "      else:\n",
        "        predicate = \"None\"\n",
        "      if obj is not None:\n",
        "        obj = obj.lower()\n",
        "      else:\n",
        "        obj = \"None\"\n",
        "      triples.append((subject, predicate, obj))\n",
        "  return triples\n",
        "\n",
        "# Function to get entity or noun phrase\n",
        "def get_entity_or_noun_phrase(token):\n",
        "  if token.ent_type_:\n",
        "    return token.text\n",
        "  else:\n",
        "    return \" \".join([child.text for child in token.subtree])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89M85zwdF2hY"
      },
      "source": [
        "3.1. Processing Documents and Extracting Tuples.\n",
        "\n",
        "For processing documents, we need to extract sentences from documents. Let's work on a hypothetical docuemnt:\n",
        "\n",
        "```\n",
        "document = \"Barack Obama was born in Hawaii. He served as the 44th president of the United States.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T5dvW5WGbEJ",
        "outputId": "bdd15a96-200b-4a96-bf95-d45eab584abb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('obama', 'bear in', 'hawaii'), ('obama', 'serve as', 'the 44th president of the united states')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "document = \"Barack Obama was born in Hawaii. Obama served as the 44th president of the United States.\"\n",
        "\n",
        "sentences = sent_tokenize(document)\n",
        "all_triples = []\n",
        "for sentence in sentences:\n",
        "  triples = extract_triples(sentence)\n",
        "  all_triples.extend(triples)\n",
        "\n",
        "print (all_triples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zcB9g2tSybx"
      },
      "source": [
        "## 4. Using the knowlegde extracted to answer questions\n",
        "\n",
        "We can parse questions in the same way as we parsed sentences in the document. Assuming that questions represent incomplete knowlegde tuples, we can take incomplete tuples and search within our knowledge base to find the missing portions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p6-XSsAUTJv5"
      },
      "outputs": [],
      "source": [
        "def find_answer_from_triples(question_triple, knowledge_triples):\n",
        "  answer = None\n",
        "  for knowledge_triple in knowledge_triples:\n",
        "    if question_triple[0] in knowledge_triple[0] and question_triple[1] in knowledge_triple[1]:\n",
        "      answer = knowledge_triple[2]\n",
        "    elif question_triple[0] in knowledge_triple[0] and question_triple[2] in knowledge_triple[2]:\n",
        "      answer = knowledge_triple[1]\n",
        "    elif question_triple[1] in knowledge_triple[1] and question_triple[2] in knowledge_triple[2]:\n",
        "      answer = knowledge_triple[0]\n",
        "  return answer\n",
        "\n",
        "def answer_question(question, knowledge_triples):\n",
        "  question_triple = extract_triples(question)[0] # only one triple from one question\n",
        "  print (\"Question triples\", question_triple)\n",
        "  answer = find_answer_from_triples(question_triple, knowledge_triples)\n",
        "  if answer is None:\n",
        "     answer = \"Sorry, I don't have an answer to that question.\"\n",
        "  print (answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUNVZGQwXiTA"
      },
      "source": [
        "Now let's try to answer some questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ_u4drdXlsN",
        "outputId": "41c99faa-38e6-4cbe-d370-0e7819070c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question triples ('obama', 'bear', 'None')\n",
            "hawaii\n",
            "Question triples ('who', 'serve as', 'the president of the united states')\n",
            "Sorry, I don't have an answer to that question.\n",
            "Question triples ('obama', 'do', 'what')\n",
            "Sorry, I don't have an answer to that question.\n",
            "Question triples ('who', 'bear in', 'hawaii')\n",
            "obama\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"Where was Barack Obama born?\",\n",
        "    \"Who served as the president of the United States?\",\n",
        "    \"What did Barack Obama do?\",\n",
        "    \"Who was born in Hawaii?\",\n",
        "]\n",
        "for question in questions:\n",
        "  answer_question(question,all_triples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJk8V3Btb7AN"
      },
      "source": [
        "As we can see, we would probably need some fuzzy matching techniques to answer all questions perfectly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mWPZ17vcFdk"
      },
      "source": [
        "## E1. Exercise: Identifying Entities from News\n",
        "\n",
        "- Visit a news website of your choice. Copy paste a few sentences from an article (preferrably containing a lot of names) into a variable `document`.\n",
        "\n",
        "- Process the text (clean up a bit if required) and put all unique sentences in a list.\n",
        "\n",
        "- Perform NER on the sentences using SpaCY and print the unique entities (person, location, organizations etc).\n",
        "\n",
        "- Try repeating the same exercise on a French news website https://www.lefigaro.fr/. You will have to download the and `fr_core_news_sm` .\n",
        "\n",
        "- Write down your observations.\n",
        "\n",
        "**Optional Exercise 2 (not-graded) **\n",
        "Can you extract some knowledge tuples by using NER, POS and/or Dependency Parsing from the same English news website following our example code. What kind of ammendments did you need to make to the existing rules to extract better tuples.\n",
        "\n",
        "**Optional Exercise 3 (not-graded) **\n",
        "Can you build a question answering system, following the above examples? Feel free to formulate variery of questions and check the accuracy of your system (qualitatively).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JdLw7LUSbiLI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google Chrome is getting a new AI writing generator today. At its core, this Gemin-powered tool is essentially the existing \"Help me write\" feature from Gmail, but extended to the entire web and powered by one of Google's latest Gemini AI models. The company first announced this new tool in January and it remains in its 'experimental' phase, meaning you must explicitly enable it. To get started, head to the Chrome settings menu and look for the 'Experimental AI' page. From there, you can easily enable the new writing feature, as well as Google's new automatic tab organizer (which I haven't found particularly useful or smart so far) and the new Chrome theme manager. For now, the AI writer is only available in English on Windows, Mac and Linux. After that, right-click on any text field and select 'Help me write.' You can use this to write something completely now Gemini can also rewrite existing text.\n"
          ]
        }
      ],
      "source": [
        "document = \"Google Chrome is getting a new AI writing generator today. At its core, this Gemin-powered tool is essentially the existing \\\"Help me write\\\" feature from Gmail, but extended to the entire web and powered by one of Google's latest Gemini AI models. The company first announced this new tool in January and it remains in its 'experimental' phase, meaning you must explicitly enable it. To get started, head to the Chrome settings menu and look for the 'Experimental AI' page. From there, you can easily enable the new writing feature, as well as Google's new automatic tab organizer (which I haven't found particularly useful or smart so far) and the new Chrome theme manager. For now, the AI writer is only available in English on Windows, Mac and Linux. After that, right-click on any text field and select 'Help me write.' You can use this to write something completely now Gemini can also rewrite existing text.\"\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['google chrome is getting a new ai writing generator today',\n",
              " ' at its core, this gemin-powered tool is essentially the existing \"help me write\" feature from gmail, but extended to the entire web and powered by one of google\\'s latest gemini ai models',\n",
              " \" the company first announced this new tool in january and it remains in its 'experimental' phase, meaning you must explicitly enable it\",\n",
              " \" to get started, head to the chrome settings menu and look for the 'experimental ai' page\",\n",
              " \" from there, you can easily enable the new writing feature, as well as google's new automatic tab organizer (which i haven't found particularly useful or smart so far) and the new chrome theme manager\",\n",
              " ' for now, the ai writer is only available in english on windows, mac and linux',\n",
              " \" after that, right-click on any text field and select 'help me write\",\n",
              " \"' you can use this to write something completely now gemini can also rewrite existing text\",\n",
              " '']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# process text\n",
        "document = document.lower()\n",
        "\n",
        "list_sentences = document.split(\".\")\n",
        "list_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "google ORG\n",
            "today DATE\n",
            "one CARDINAL\n",
            "google ORG\n",
            "january DATE\n",
            "google ORG\n",
            "english LANGUAGE\n",
            "mac PERSON\n",
            "linux PERSON\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "for sentence in list_sentences:\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from fr-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rennes LOC\n",
            "l’ LOC\n",
            "JO MISC\n",
            "ministre des Sports ORG\n",
            "l’ LOC\n",
            "PSG ORG\n",
            "RMC ORG\n",
            "Amélie Oudéa-Castéra PER\n",
            "Kylian Mbappé MISC\n",
            "PSG ORG\n",
            "l’Education ORG\n",
            "ministère des Sports ORG\n",
            "Rennes LOC\n",
            "l’ ORG\n",
            "Jeux olympiques MISC\n"
          ]
        }
      ],
      "source": [
        "# do it but in french\n",
        "french_document = \"En déplacement à Rennes à l’usine de production des peluches des JO, la ministre des Sports a rendu hommage à l’attaquant du PSG, un «héros des dernières années». «Une page est en train de se tourner», a déclaré jeudi au micro de RMC Amélie Oudéa-Castéra, à propos du départ de Kylian Mbappé du PSG. Après un bref mais mouvementé passage au ministère de l’Education nationale, «AOC» est de retour au ministère des Sports à temps plein et se trouvait à Rennes pour visiter l’usine de production des peluches des Jeux olympiques.\"\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "list_fr_sentences = french_document.split(\".\")\n",
        "for sentence in list_fr_sentences:\n",
        "  doc = nlp(sentence)\n",
        "  for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There's a lot more repetition in this set of words, as well as more misc. objects than the english sets."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
