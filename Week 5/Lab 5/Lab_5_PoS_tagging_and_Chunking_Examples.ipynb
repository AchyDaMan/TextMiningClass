{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIwZDzNLfcni"
      },
      "source": [
        "## Lab 5. Exploring Shallow Parsers of Text\n",
        "\n",
        "In this practicum, we'll delve into shallow text parsing, specifically part-of-speech tagging and parsing. Today, we'll mainly use various taggers and chunkers.\n",
        "\n",
        "Complete all exercises and submit under \"Lab 5: PoS tagging and Chunking Examples\" : https://utexas.instructure.com/courses/1382133/assignments/6627267?module_item_id=13596161"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHXZp9perGZX"
      },
      "source": [
        "## 1. Part of Speech Tagging\n",
        "\n",
        "Part of speech (POS) tagging in natural language processing (NLP) is the process of assigning grammatical categories or labels (parts of speech) to words in a text corpus. These labels classify words based on their syntactic and grammatical roles within a sentence.\n",
        "\n",
        "POS tagging is a fundamental task in NLP and serves various purposes in language processing and understanding. Some applications of POS taging are:\n",
        "\n",
        "1. **Text Analysis**: POS tagging helps break down a text into its grammatical components, facilitating further analysis.\n",
        "\n",
        "2. **Information Retrieval**: It aids in retrieving documents or sentences containing specific parts of speech.\n",
        "\n",
        "3. **Sentiment Analysis**: Identifying adjectives and adverbs helps determine the sentiment or tone of a text.\n",
        "\n",
        "4. **Machine Translation**: POS tags can guide the translation of words and phrases in different languages.\n",
        "\n",
        "5. **Speech Recognition**: It assists in converting spoken language into written text by identifying parts of speech.\n",
        "\n",
        "6. **Search Engine Optimization (SEO)**: Knowing the parts of speech in web content can help optimize it for search engines.\n",
        "\n",
        "7. **Grammar Checking**: POS tagging can be used in grammar-checking tools to highlight errors or suggest improvements.\n",
        "\n",
        "8. **Named Entity Recognition (NER)**: It can help identify proper nouns and entities within a text.\n",
        "\n",
        "9. **Syntax Parsing**: POS tags are crucial for building parse trees and understanding the syntactic structure of sentences.\n",
        "\n",
        "10. **Question Answering**: Identifying nouns and verbs in a question can help find relevant answers in a text corpus.\n",
        "\n",
        "11. **Text Summarization**: POS tagging assists in summarizing texts by identifying important content words.\n",
        "\n",
        "... and many more\n",
        "\n",
        "Let's explore some exisitng POS taggers. (Note, you may need to install libraries to use certain taggers.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WAXUDVlrYxp"
      },
      "source": [
        "### 1.1. PoS Tagging with Natural Language Toolkit Library (NLTK)\n",
        "\n",
        "NLTK provides a list of taggers and documentations for them. Please refer to https://www.nltk.org/book/ch05.html for more details.\n",
        "\n",
        "**Always remember to tokenize before applying tagging / parsing**. Stemming and lemmatizations are not required (why?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ghElqGWrYG2",
        "outputId": "e6fe6d0e-2f4b-4a5b-9be9-690b7bb435bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded punkt\n",
            "Downloaded averaged_perceptron_tagger\n",
            "The conference, which was held in New York, had over 500 attendees (including students).\n",
            "['The', 'conference', ',', 'which', 'was', 'held', 'in', 'New', 'York', ',', 'had', 'over', '500', 'attendees', '(', 'including', 'students', ')', '.']\n",
            "[('The', 'DT'), ('conference', 'NN'), (',', ','), ('which', 'WDT'), ('was', 'VBD'), ('held', 'VBN'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP'), (',', ','), ('had', 'VBD'), ('over', 'IN'), ('500', 'CD'), ('attendees', 'NNS'), ('(', '('), ('including', 'VBG'), ('students', 'NNS'), (')', ')'), ('.', '.')]\n",
            "She said, 'I'll meet you at the park... if it doesn't rain.'\n",
            "['She', 'said', ',', \"'\", 'I', \"'ll\", 'meet', 'you', 'at', 'the', 'park', '...', 'if', 'it', 'does', \"n't\", 'rain', '.', \"'\"]\n",
            "[('She', 'PRP'), ('said', 'VBD'), (',', ','), (\"'\", \"''\"), ('I', 'PRP'), (\"'ll\", 'MD'), ('meet', 'VB'), ('you', 'PRP'), ('at', 'IN'), ('the', 'DT'), ('park', 'NN'), ('...', ':'), ('if', 'IN'), ('it', 'PRP'), ('does', 'VBZ'), (\"n't\", 'RB'), ('rain', 'VB'), ('.', '.'), (\"'\", \"''\")]\n",
            "The URL for the website is https://www.example.com, and you can reach me at john.doe@email.com.\n",
            "['The', 'URL', 'for', 'the', 'website', 'is', 'https', ':', '//www.example.com', ',', 'and', 'you', 'can', 'reach', 'me', 'at', 'john.doe', '@', 'email.com', '.']\n",
            "[('The', 'DT'), ('URL', 'NNP'), ('for', 'IN'), ('the', 'DT'), ('website', 'NN'), ('is', 'VBZ'), ('https', 'JJ'), (':', ':'), ('//www.example.com', 'NN'), (',', ','), ('and', 'CC'), ('you', 'PRP'), ('can', 'MD'), ('reach', 'VB'), ('me', 'PRP'), ('at', 'IN'), ('john.doe', 'NN'), ('@', 'NNP'), ('email.com', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def download_nltk_dataset(dataset_name):\n",
        "  # Let's do this one time implementation  for downloading an NLTK dataset\n",
        "  # ONLY IF it does not exist\n",
        "  try:\n",
        "      nltk.data.find(dataset_name)\n",
        "  except LookupError:\n",
        "      nltk.download(dataset_name)\n",
        "      print(f\"Downloaded {dataset_name}\")\n",
        "  else:\n",
        "        print(f\"{dataset_name} is already downloaded\")\n",
        "\n",
        "# Usage example:\n",
        "download_nltk_dataset(\"punkt\")  # Replace with the name of the dataset you want to download\n",
        "download_nltk_dataset(\"averaged_perceptron_tagger\")\n",
        "\n",
        "some_sentences =[\n",
        "    \"The conference, which was held in New York, had over 500 attendees (including students).\",\n",
        "    \"She said, 'I'll meet you at the park... if it doesn't rain.'\",\n",
        "    \"The URL for the website is https://www.example.com, and you can reach me at john.doe@email.com.\"\n",
        "    ]\n",
        "\n",
        "for sentence in some_sentences:\n",
        "  print (sentence)\n",
        "  words = word_tokenize(sentence)\n",
        "  print (words)\n",
        "  tags = nltk.pos_tag(words)\n",
        "  print(tags)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKehVHwNrzvB"
      },
      "source": [
        "### 1.2. Print the default POS tagset: The PENN Treebank Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uGrnmcpkskx",
        "outputId": "dbab5284-0bd1-4c86-ac53-32a7ce3a6b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Penn Treebank POS Tags:\n",
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download the Penn Treebank POS tagset (if not already downloaded)\n",
        "nltk.download('tagsets')\n",
        "\n",
        "# Print the list of Penn Treebank POS tags\n",
        "print(\"Penn Treebank POS Tags:\")\n",
        "nltk.help.upenn_tagset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oybXv29UsCDU"
      },
      "source": [
        "### 1.3. Using Spacy Library's POS taggers\n",
        "\n",
        "SpaCy's default models are based on deep learning techniques, specifically convolutional neural networks (CNN) and long short-term memory networks (LSTM).\n",
        "\n",
        "SpaCy can provide coarse (high-level) and fine (Upenn style) tagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM0qzkQCrF8F",
        "outputId": "83f016de-94d9-45ea-c24a-14e738eb2cc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "****************************************\n",
            "The conference, which was held in New York, had over 500 attendees (including students).\n",
            "Tokens:\n",
            "['The', 'conference', ',', 'which', 'was', 'held', 'in', 'New', 'York', ',', 'had', 'over', '500', 'attendees', '(', 'including', 'students', ')', '.']\n",
            "Coarse tags:\n",
            "['DET', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'VERB', 'ADP', 'PROPN', 'PROPN', 'PUNCT', 'VERB', 'ADP', 'NUM', 'NOUN', 'PUNCT', 'VERB', 'NOUN', 'PUNCT', 'PUNCT']\n",
            "Fine tags:\n",
            "['DT', 'NN', ',', 'WDT', 'VBD', 'VBN', 'IN', 'NNP', 'NNP', ',', 'VBD', 'IN', 'CD', 'NNS', '-LRB-', 'VBG', 'NNS', '-RRB-', '.']\n",
            "****************************************\n",
            "She said, 'I'll meet you at the park... if it doesn't rain.'\n",
            "Tokens:\n",
            "['She', 'said', ',', \"'\", 'I', \"'ll\", 'meet', 'you', 'at', 'the', 'park', '...', 'if', 'it', 'does', \"n't\", 'rain', '.', \"'\"]\n",
            "Coarse tags:\n",
            "['PRON', 'VERB', 'PUNCT', 'PUNCT', 'PRON', 'AUX', 'VERB', 'PRON', 'ADP', 'DET', 'NOUN', 'PUNCT', 'SCONJ', 'PRON', 'AUX', 'PART', 'VERB', 'PUNCT', 'PUNCT']\n",
            "Fine tags:\n",
            "['PRP', 'VBD', ',', '``', 'PRP', 'MD', 'VB', 'PRP', 'IN', 'DT', 'NN', ':', 'IN', 'PRP', 'VBZ', 'RB', 'VB', '.', \"''\"]\n",
            "****************************************\n",
            "The URL for the website is https://www.example.com, and you can reach me at john.doe@email.com.\n",
            "Tokens:\n",
            "['The', 'URL', 'for', 'the', 'website', 'is', 'https://www.example.com', ',', 'and', 'you', 'can', 'reach', 'me', 'at', 'john.doe@email.com', '.']\n",
            "Coarse tags:\n",
            "['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'AUX', 'NUM', 'PUNCT', 'CCONJ', 'PRON', 'AUX', 'VERB', 'PRON', 'ADP', 'PROPN', 'PUNCT']\n",
            "Fine tags:\n",
            "['DT', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'CD', ',', 'CC', 'PRP', 'MD', 'VB', 'PRP', 'IN', 'NNP', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# download a set of pre-trained models from spacy. Note: This is a model pipeline\n",
        "# This will do tokenization, tagging and parsing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "some_sentences =[\n",
        "    \"The conference, which was held in New York, had over 500 attendees (including students).\",\n",
        "    \"She said, 'I'll meet you at the park... if it doesn't rain.'\",\n",
        "    \"The URL for the website is https://www.example.com, and you can reach me at john.doe@email.com.\"\n",
        "    ]\n",
        "\n",
        "for sent in some_sentences:\n",
        "  print (\"****************************************\")\n",
        "  print (sent)\n",
        "  processed_sent = nlp(sent)\n",
        "  # See tokenization results\n",
        "  print (f\"Tokens:\")\n",
        "  tokens = [token.text for token in processed_sent]\n",
        "  print (tokens)\n",
        "\n",
        "  # Coarse tagging\n",
        "  print (f\"Coarse tags:\")\n",
        "  tags = [token.pos_ for token in processed_sent]\n",
        "  print (tags)\n",
        "\n",
        "  # Fine tagging\n",
        "  print (f\"Fine tags:\")\n",
        "  tags = [token.tag_ for token in processed_sent]\n",
        "  print (tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRh14983sb6W"
      },
      "source": [
        "### 1.4. Multilingual PoS Tagging using SpaCY\n",
        "Let's download the multilingual model first. Use command line interface and command `python -m spacy download xx_ent_wiki_sm` for this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm9aIIVssiqx",
        "outputId": "595bdf0e-3028-467c-c1f2-5fe86c9eff6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5679vj12soqF"
      },
      "source": [
        "Let's tag some GERMAN text now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9in0ipWssJi",
        "outputId": "a08a1891-9d00-4125-917a-6c8bfd3e84db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Dies', 'PDS'), ('ist', 'VAFIN'), ('ein', 'ART'), ('Beispiel', 'NN'), ('für', 'APPR'), ('PoS-Tagging', 'NE'), ('in', 'APPR'), ('mehreren', 'PIAT'), ('Sprachen', 'NN'), ('.', '$.')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load a multilingual model\n",
        "nlp = spacy.load(\"de_core_news_sm\")\n",
        "\n",
        "# Define a sentence in a different language (e.g., Spanish)\n",
        "sentence = \"Dies ist ein Beispiel für PoS-Tagging in mehreren Sprachen.\"\n",
        "\n",
        "# Process the sentence with the multilingual model\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Extract PoS tags\n",
        "tags = [(token.text, token.tag_) for token in doc]\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-nY7FNMtU3C"
      },
      "source": [
        "## Exercise E1. Analysis Spanish Wikipedia Data\n",
        "\n",
        "1. Copy paste the first paragraph of https://es.wikipedia.org/wiki/Pen%C3%A9lope_Cruz (Spanish wikipedia page for popular acress Penelope Cruz)\n",
        "2. Perform Part of Speech tagging using Spacy's PoS tagger.\n",
        "[Hint: you will have to use SpaCy's `es_core_news_sm` model that has been developed for Spanish.]\n",
        "3. Calculate the frequency or percentage of each POS tag category in the text and print the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from es-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "****************************************\n",
            "Penélope Cruz Sánchez (Alcobendas, 28 de abril de 1974) les una actriz y modelo española.\n",
            "Tokens:\n",
            "['Penélope', 'Cruz', 'Sánchez', '(', 'Alcobendas', ',', '28', 'de', 'abril', 'de', '1974', ')', 'les', 'una', 'actriz', 'y', 'modelo', 'española', '.']\n",
            "Coarse tags:\n",
            "['PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'NUM', 'ADP', 'NOUN', 'ADP', 'NUM', 'PUNCT', 'PRON', 'DET', 'NOUN', 'CCONJ', 'NOUN', 'ADJ', 'PUNCT']\n",
            "Fine tags:\n",
            "['PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'NUM', 'ADP', 'NOUN', 'ADP', 'NUM', 'PUNCT', 'PRON', 'DET', 'NOUN', 'CCONJ', 'NOUN', 'ADJ', 'PUNCT']\n",
            "****************************************\n",
            "En 2006 fue la primera actriz española candidata a los Premios Óscar y a los Globos de Oro en la categoría de mejor actriz protagonista, por su papel en la película española Volver, dirigida por el cineasta español Pedro Almodóvar;\n",
            "Tokens:\n",
            "['En', '2006', 'fue', 'la', 'primera', 'actriz', 'española', 'candidata', 'a', 'los', 'Premios', 'Óscar', 'y', 'a', 'los', 'Globos', 'de', 'Oro', 'en', 'la', 'categoría', 'de', 'mejor', 'actriz', 'protagonista', ',', 'por', 'su', 'papel', 'en', 'la', 'película', 'española', 'Volver', ',', 'dirigida', 'por', 'el', 'cineasta', 'español', 'Pedro', 'Almodóvar', ';']\n",
            "Coarse tags:\n",
            "['ADP', 'NOUN', 'AUX', 'DET', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'ADP', 'DET', 'PROPN', 'PROPN', 'CCONJ', 'ADP', 'DET', 'PROPN', 'ADP', 'PROPN', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADJ', 'PUNCT', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADJ', 'PROPN', 'PUNCT', 'ADJ', 'ADP', 'DET', 'NOUN', 'ADJ', 'PROPN', 'PROPN', 'PUNCT']\n",
            "Fine tags:\n",
            "['ADP', 'NOUN', 'AUX', 'DET', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'ADP', 'DET', 'PROPN', 'PROPN', 'CCONJ', 'ADP', 'DET', 'PROPN', 'ADP', 'PROPN', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADJ', 'PUNCT', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADJ', 'PROPN', 'PUNCT', 'ADJ', 'ADP', 'DET', 'NOUN', 'ADJ', 'PROPN', 'PROPN', 'PUNCT']\n",
            "****************************************\n",
            "en esa ocasión no obtuvo el Óscar, pero en 2008 se convirtió en la primera actriz española en conseguir el Óscar como mejor actriz de reparto gracias a la película Vicky Cristina Barcelona dirigida por Woody Allen.\n",
            "Tokens:\n",
            "['en', 'esa', 'ocasión', 'no', 'obtuvo', 'el', 'Óscar', ',', 'pero', 'en', '2008', 'se', 'convirtió', 'en', 'la', 'primera', 'actriz', 'española', 'en', 'conseguir', 'el', 'Óscar', 'como', 'mejor', 'actriz', 'de', 'reparto', 'gracias', 'a', 'la', 'película', 'Vicky', 'Cristina', 'Barcelona', 'dirigida', 'por', 'Woody', 'Allen', '.']\n",
            "Coarse tags:\n",
            "['ADP', 'DET', 'NOUN', 'ADV', 'VERB', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'ADP', 'NOUN', 'PRON', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADJ', 'ADP', 'VERB', 'DET', 'NUM', 'SCONJ', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'ADP', 'DET', 'NOUN', 'PROPN', 'PROPN', 'PROPN', 'ADJ', 'ADP', 'PROPN', 'PROPN', 'PUNCT']\n",
            "Fine tags:\n",
            "['ADP', 'DET', 'NOUN', 'ADV', 'VERB', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'ADP', 'NOUN', 'PRON', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADJ', 'ADP', 'VERB', 'DET', 'NUM', 'SCONJ', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'ADP', 'DET', 'NOUN', 'PROPN', 'PROPN', 'PROPN', 'ADJ', 'ADP', 'PROPN', 'PROPN', 'PUNCT']\n"
          ]
        }
      ],
      "source": [
        "span_sentences = [\"Penélope Cruz Sánchez (Alcobendas, 28 de abril de 1974) les una actriz y modelo española.\", \n",
        "                  \"En 2006 fue la primera actriz española candidata a los Premios Óscar y a los Globos de Oro en la categoría de mejor actriz protagonista, por su papel en la película española Volver, dirigida por el cineasta español Pedro Almodóvar;\",\n",
        "                  \"en esa ocasión no obtuvo el Óscar, pero en 2008 se convirtió en la primera actriz española en conseguir el Óscar como mejor actriz de reparto gracias a la película Vicky Cristina Barcelona dirigida por Woody Allen.\"\n",
        "                  ]\n",
        "\n",
        "s_nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "for sent in span_sentences:\n",
        "  print (\"****************************************\")\n",
        "  print (sent)\n",
        "  processed_sent = s_nlp(sent)\n",
        "  # See tokenization results\n",
        "  print (f\"Tokens:\")\n",
        "  tokens = [token.text for token in processed_sent]\n",
        "  print (tokens)\n",
        "\n",
        "  # Coarse tagging\n",
        "  print (f\"Coarse tags:\")\n",
        "  tags = [token.pos_ for token in processed_sent]\n",
        "  print (tags)\n",
        "\n",
        "  # Fine tagging\n",
        "  print (f\"Fine tags:\")\n",
        "  tags = [token.tag_ for token in processed_sent]\n",
        "  print (tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aBNVohmux4E"
      },
      "source": [
        "## 2. Shallow Parsing : Chunking\n",
        "\n",
        "In NLP, chunking is the process of extracting meaningful phrases (chunks) from a sentence or text. These phrases are typically noun phrases (NP), verb phrases (VP), or other grammatical units that convey useful information about the text.\n",
        "\n",
        "Various applications of chunking include (but not limited to):\n",
        "\n",
        "1. **Information Extraction:** Chunking is often used in information extraction tasks to identify and extract relevant information from unstructured text data. For example, in news articles, chunking can be used to extract names of people, organizations, locations, and other key entities.\n",
        "\n",
        "2. **Keyphrase Extraction:** Keyphrase extraction via chunking can be used to identify and extract the most important phrases or sentences from a document, aiding in the creation of concise and informative document summaries.\n",
        "\n",
        "3. **Search Engine Optimization (SEO):** For SEO purposes, chunking can help identify and extract important keywords and phrases from web content. These chunks can be used for keyword analysis and optimization.\n",
        "Content Extraction: In web scraping and content extraction applications, chunking can be used to locate and extract specific pieces of information from HTML documents or other structured text formats.\n",
        "\n",
        "4. **Academic Research and Literature Analysis:**\n",
        "In academic research, keyphrase extraction can help researchers quickly identify the main topics and contributions of research papers.\n",
        "\n",
        "5. **Legal Document Analysis:**\n",
        "In legal documents, chunking and keyphrase extraction can help identify relevant sections or clauses of interest.\n",
        "\n",
        "6. **Patent Analysis and Search:**\n",
        "Keyphrase extraction is commonly used in patent analysis to identify the essential elements of a patent application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h92w9Uou5lP"
      },
      "source": [
        "## 2.1. Rule Based Chunking with the help of POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWC4IViOu1kZ",
        "outputId": "b5ac2bbc-07aa-460b-8c31-ee87d62caffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural language', 'processing', 'a subfield', 'artificial intelligence']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.chunk import RegexpParser\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural language processing (NLP) is a subfield of artificial intelligence (AI).\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Part-of-speech tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Define a chunking grammar for noun phrases (NP)\n",
        "grammar = r\"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "# Create a chunk parser with the grammar\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "\n",
        "# Apply chunking to the part-of-speech tagged text\n",
        "tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Extract noun phrases\n",
        "noun_phrases = [\" \".join(leaf[0] for leaf in subtree.leaves()) for subtree in tree.subtrees() if subtree.label() == 'NP']\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_noun_phrases = [phrase for phrase in noun_phrases if phrase.lower() not in stop_words]\n",
        "\n",
        "print(filtered_noun_phrases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz2c53-pvMH4"
      },
      "source": [
        "### 2. Chunking using pre-trained models via spaCy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo5oo6IrvQBW",
        "outputId": "06577e7d-3896-4460-8a6c-b20dce045b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "John\n",
            "Mary\n",
            "pizza\n",
            "the restaurant\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"John and Mary are eating pizza in the restaurant\"\n",
        "\n",
        "# Process the sentence using spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Iterate over the parsed tokens and extract chunks\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnaHSKMjvZ1W"
      },
      "source": [
        "## Exercise E2: Verb phrase extraction\n",
        "A verb phrase is a syntactic unit that consists of one or more verbs and any accompanying elements, such as objects, complements, adverbs, or prepositional phrases.\n",
        "\n",
        "Here's the general structure of a verb phrase:\n",
        "\n",
        "`Verb Phrase = Verb + (Object/Complement/Adverb/Prepositional Phrase)`\n",
        "\n",
        "Examples:\n",
        "\n",
        "1. \"I ate rice\" -> \"ate rice\"\n",
        "2. \"I ate rice with fork\" -> \"ate rice with fork\"\n",
        "3. \"I ate rice with salt and paper \" -> \"ate rice with salt and paper\"\n",
        "4. \"I ate rice and slept \" -> \"ate rice\"\n",
        "\n",
        "For the list of sentences below, identify the verb phrases following section 2.1.\n",
        "\n",
        "```\n",
        "list_of_sents = [\n",
        "  \"John lives in New York.\",\n",
        "  \"J.K. Rowling wrote the book Harry Potter.\",\n",
        "  \" Tom Hanks acted in Forrest Gump.\"\n",
        "  ]\n",
        "```\n",
        "\n",
        "Expected outputs:\n",
        "1. `lives in New York.`.\n",
        "2. `wrote the book Harry Potter.`\n",
        "3. `acted in Forrest Gump.`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['lives in New York']\n",
            "['wrote the book Harry Potter']\n",
            "['acted in Forrest Gump']\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2\n",
        "\n",
        "list_of_sents = [\n",
        "  \"John lives in New York.\",\n",
        "  \"J.K. Rowling wrote the book Harry Potter.\",\n",
        "  \" Tom Hanks acted in Forrest Gump.\"\n",
        "  ]\n",
        "\n",
        "def e2(sentence):\n",
        "  # Tokenize the text\n",
        "  tokens = nltk.word_tokenize(sentence)\n",
        "  # Part-of-speech tagging\n",
        "  pos_tags = nltk.pos_tag(tokens)\n",
        "  # Define a chunking grammar for noun phrases (NP)\n",
        "  grammar = r\"\"\"\n",
        "    VP: {<VB.*><DT>?<JJ>*<NN.*>+}\n",
        "        {<VB.*><IN><DT>?<JJ>*<NN.*>+}\n",
        "\"\"\"\n",
        "  # Create a chunk parser with the grammar\n",
        "  chunk_parser = RegexpParser(grammar)\n",
        "  # Apply chunking to the part-of-speech tagged text\n",
        "  tree = chunk_parser.parse(pos_tags)\n",
        "  # Extract verb phrases\n",
        "  verb_phrases = [\" \".join(leaf[0] for leaf in subtree.leaves()) for subtree in tree.subtrees() if subtree.label() == 'VP']\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  filtered_verb_phrases = [phrase for phrase in verb_phrases if phrase.lower() not in stop_words]\n",
        "  print(filtered_verb_phrases)\n",
        "  # Process the sentence using spaCy\n",
        "  doc = nlp(sentence)\n",
        "  # Iterate over the parsed tokens and extract chunks\n",
        "  for chunk in doc.noun_chunks:\n",
        "    if not isinstance(chunk, spacy.tokens.span.Span):\n",
        "      print(chunk[0])\n",
        "\n",
        "for sent in list_of_sents:\n",
        "   e2(sent)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
